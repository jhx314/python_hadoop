
Apache > Hadoop > hadoop-yarn > Apache Hadoop 2.7.2
Wiki | git | Apache Hadoop  | Last Published: 2016-01-26  | Version: 2.7.2
General

    Overview
    Single Node Setup
    Cluster Setup
    Hadoop Commands Reference
    FileSystem Shell
    Hadoop Compatibility
    Interface Classification
    FileSystem Specification

Common

    CLI Mini Cluster
    Native Libraries
    Proxy User
    Rack Awareness
    Secure Mode
    Service Level Authorization
    HTTP Authentication
    Hadoop KMS
    Tracing

HDFS

    HDFS User Guide
    HDFS Commands Reference
    High Availability With QJM
    High Availability With NFS
    Federation
    ViewFs Guide
    HDFS Snapshots
    HDFS Architecture
    Edits Viewer
    Image Viewer
    Permissions and HDFS
    Quotas and HDFS
    HFTP
    C API libhdfs
    WebHDFS REST API
    HttpFS Gateway
    Short Circuit Local Reads
    Centralized Cache Management
    HDFS NFS Gateway
    HDFS Rolling Upgrade
    Extended Attributes
    Transparent Encryption
    HDFS Support for Multihoming
    Archival Storage, SSD & Memory
    Memory Storage Support

MapReduce

    MapReduce Tutorial
    MapReduce Commands Reference
    Compatibilty between Hadoop 1.x and Hadoop 2.x
    Encrypted Shuffle
    Pluggable Shuffle/Sort
    Distributed Cache Deploy

MapReduce REST APIs

    MR Application Master
    MR History Server

YARN

    Overview
    YARN Architecture
    Capacity Scheduler
    Fair Scheduler
    ResourceManager Restart
    ResourceManager HA
    Node Labels
    Web Application Proxy
    YARN Timeline Server
    Writing YARN Applications
    YARN Commands
    NodeManager Restart
    DockerContainerExecutor
    Using CGroups
    Secure Containers
    Registry

YARN REST APIs

    Introduction
    Resource Manager
    Node Manager
    Timeline Server

Hadoop Compatible File Systems

    Amazon S3
    Azure Blob Storage
    OpenStack Swift

Auth

    Overview
    Examples
    Configuration
    Building

Tools

    Hadoop Streaming
    Hadoop Archives
    DistCp
    GridMix
    Rumen
    Scheduler Load Simulator

Reference

    Release Notes
    API docs
    Common CHANGES.txt
    HDFS CHANGES.txt
    MapReduce CHANGES.txt
    YARN CHANGES.txt
    Metrics

Configuration

    core-default.xml
    hdfs-default.xml
    mapred-default.xml
    yarn-default.xml
    Deprecated Properties

Built by Maven
Hadoop YARN - Introduction to the web services REST API’s

    Overview
    URI’s
    HTTP Requests
        Summary of HTTP operations
        Security
        Headers Supported
    HTTP Responses
        Compression
        Response Formats
        Response Errors
        Response Examples
    Sample Usage

Overview

The Hadoop YARN web service REST APIs are a set of URI resources that give access to the cluster, nodes, applications, and application historical information. The URI resources are grouped into APIs based on the type of information returned. Some URI resources return collections while others return singletons.
URI’s

The URIs for the REST-based Web services have the following syntax:

  http://{http address of service}/ws/{version}/{resourcepath}

The elements in this syntax are as follows:

  {http address of service} - The http address of the service to get information about. 
                              Currently supported are the ResourceManager, NodeManager, 
                              MapReduce application master, and history server.
  {version} - The version of the APIs. In this release, the version is v1.
  {resourcepath} - A path that defines a singleton resource or a collection of resources. 

HTTP Requests

To invoke a REST API, your application calls an HTTP operation on the URI associated with a resource.
Summary of HTTP operations

Currently only GET is supported. It retrieves information about the resource specified.
Security

The web service REST API’s go through the same security as the web UI. If your cluster adminstrators have filters enabled you must authenticate via the mechanism they specified.
Headers Supported

  * Accept 
  * Accept-Encoding

Currently the only fields used in the header is Accept and Accept-Encoding. Accept currently supports XML and JSON for the response type you accept. Accept-Encoding currently supports only gzip format and will return gzip compressed output if this is specified, otherwise output is uncompressed. All other header fields are ignored.
HTTP Responses

The next few sections describe some of the syntax and other details of the HTTP Responses of the web service REST APIs.
Compression

This release supports gzip compression if you specify gzip in the Accept-Encoding header of the HTTP request (Accept-Encoding: gzip).
Response Formats

This release of the web service REST APIs supports responses in JSON and XML formats. JSON is the default. To set the response format, you can specify the format in the Accept header of the HTTP request.

As specified in HTTP Response Codes, the response body can contain the data that represents the resource or an error message. In the case of success, the response body is in the selected format, either JSON or XML. In the case of error, the resonse body is in either JSON or XML based on the format requested. The Content-Type header of the response contains the format requested. If the application requests an unsupported format, the response status code is 500. Note that the order of the fields within response body is not specified and might change. Also, additional fields might be added to a response body. Therefore, your applications should use parsing routines that can extract data from a response body in any order.
Response Errors

After calling an HTTP request, an application should check the response status code to verify success or detect an error. If the response status code indicates an error, the response body contains an error message. The first field is the exception type, currently only RemoteException is returned. The following table lists the items within the RemoteException error message:
Item 	Data Type 	Description
exception 	String 	Exception type
javaClassName 	String 	Java class name of exception
message 	String 	Detailed message of exception
Response Examples
JSON response with single resource

HTTP Request: GET http://rmhost.domain:8088/ws/v1/cluster/app/application\_1324057493980\_0001

Response Status Line: HTTP/1.1 200 OK

Response Header:

  HTTP/1.1 200 OK
  Content-Type: application/json
  Transfer-Encoding: chunked
  Server: Jetty(6.1.26)

Response Body:

{
  app":
  {
    "id":"application_1324057493980_0001",
    "user":"user1",
    "name":"",
    "queue":"default",
    "state":"ACCEPTED",
    "finalStatus":"UNDEFINED",
    "progress":0,
    "trackingUI":"UNASSIGNED",
    "diagnostics":"",
    "clusterId":1324057493980,
    "startedTime":1324057495921,
    "finishedTime":0,
    "elapsedTime":2063,
    "amContainerLogs":"http:\/\/amNM:2\/node\/containerlogs\/container_1324057493980_0001_01_000001",
    "amHostHttpAddress":"amNM:2"
  }
}

JSON response with Error response

Here we request information about an application that doesn’t exist yet.

HTTP Request: GET http://rmhost.domain:8088/ws/v1/cluster/app/application\_1324057493980\_9999

Response Status Line: HTTP/1.1 404 Not Found

Response Header:

  HTTP/1.1 404 Not Found
  Content-Type: application/json
  Transfer-Encoding: chunked
  Server: Jetty(6.1.26)

Response Body:

{
   "RemoteException" : {
      "javaClassName" : "org.apache.hadoop.yarn.webapp.NotFoundException",
      "exception" : "NotFoundException",
      "message" : "java.lang.Exception: app with id: application_1324057493980_9999 not found"
   }
}

Sample Usage

You can use any number of ways/languages to use the web services REST API’s. This example uses the curl command line interface to do the REST GET calls.

In this example, a user submits a MapReduce application to the ResourceManager using a command like:

  hadoop jar hadoop-mapreduce-test.jar sleep -Dmapred.job.queue.name=a1 -m 1 -r 1 -rt 1200000 -mt 20

The client prints information about the job submitted along with the application id, similar to:

12/01/18 04:25:15 INFO mapred.ResourceMgrDelegate: Submitted application application_1326821518301_0010 to ResourceManager at host.domain.com/10.10.10.10:8032
12/01/18 04:25:15 INFO mapreduce.Job: Running job: job_1326821518301_0010
12/01/18 04:25:21 INFO mapred.ClientServiceDelegate: The url to track the job: host.domain.com:8088/proxy/application_1326821518301_0010/
12/01/18 04:25:22 INFO mapreduce.Job: Job job_1326821518301_0010 running in uber mode : false
12/01/18 04:25:22 INFO mapreduce.Job:  map 0% reduce 0%

The user then wishes to track the application. The users starts by getting the information about the application from the ResourceManager. Use the –comopressed option to request output compressed. curl handles uncompressing on client side.

curl --compressed -H "Accept: application/json" -X GET "http://host.domain.com:8088/ws/v1/cluster/apps/application_1326821518301_0010" 

Output:

{
   "app" : {
      "finishedTime" : 0,
      "amContainerLogs" : "http://host.domain.com:8042/node/containerlogs/container_1326821518301_0010_01_000001",
      "trackingUI" : "ApplicationMaster",
      "state" : "RUNNING",
      "user" : "user1",
      "id" : "application_1326821518301_0010",
      "clusterId" : 1326821518301,
      "finalStatus" : "UNDEFINED",
      "amHostHttpAddress" : "host.domain.com:8042",
      "progress" : 82.44703,
      "name" : "Sleep job",
      "startedTime" : 1326860715335,
      "elapsedTime" : 31814,
      "diagnostics" : "",
      "trackingUrl" : "http://host.domain.com:8088/proxy/application_1326821518301_0010/",
      "queue" : "a1"
   }
}

The user then wishes to get more details about the running application and goes directly to the MapReduce application master for this application. The ResourceManager lists the trackingUrl that can be used for this application: http://host.domain.com:8088/proxy/application\_1326821518301\_0010. This could either go to the web browser or use the web service REST API’s. The user uses the web services REST API’s to get the list of jobs this MapReduce application master is running:

 curl --compressed -H "Accept: application/json" -X GET "http://host.domain.com:8088/proxy/application_1326821518301_0010/ws/v1/mapreduce/jobs"

Output:

{
   "jobs" : {
      "job" : [
         {
            "runningReduceAttempts" : 1,
            "reduceProgress" : 72.104515,
            "failedReduceAttempts" : 0,
            "newMapAttempts" : 0,
            "mapsRunning" : 0,
            "state" : "RUNNING",
            "successfulReduceAttempts" : 0,
            "reducesRunning" : 1,
            "acls" : [
               {
                  "value" : " ",
                  "name" : "mapreduce.job.acl-modify-job"
               },
               {
                  "value" : " ",
                  "name" : "mapreduce.job.acl-view-job"
               }
            ],
            "reducesPending" : 0,
            "user" : "user1",
            "reducesTotal" : 1,
            "mapsCompleted" : 1,
            "startTime" : 1326860720902,
            "id" : "job_1326821518301_10_10",
            "successfulMapAttempts" : 1,
            "runningMapAttempts" : 0,
            "newReduceAttempts" : 0,
            "name" : "Sleep job",
            "mapsPending" : 0,
            "elapsedTime" : 64432,
            "reducesCompleted" : 0,
            "mapProgress" : 100,
            "diagnostics" : "",
            "failedMapAttempts" : 0,
            "killedReduceAttempts" : 0,
            "mapsTotal" : 1,
            "uberized" : false,
            "killedMapAttempts" : 0,
            "finishTime" : 0
         }
      ]
   }
}

The user then wishes to get the task details about the job with job id job_1326821518301_10_10 that was listed above.

 curl --compressed -H "Accept: application/json" -X GET "http://host.domain.com:8088/proxy/application_1326821518301_0010/ws/v1/mapreduce/jobs/job_1326821518301_10_10/tasks" 

Output:

{
   "tasks" : {
      "task" : [
         {
            "progress" : 100,
            "elapsedTime" : 5059,
            "state" : "SUCCEEDED",
            "startTime" : 1326860725014,
            "id" : "task_1326821518301_10_10_m_0",
            "type" : "MAP",
            "successfulAttempt" : "attempt_1326821518301_10_10_m_0_0",
            "finishTime" : 1326860730073
         },
         {
            "progress" : 72.104515,
            "elapsedTime" : 0,
            "state" : "RUNNING",
            "startTime" : 1326860732984,
            "id" : "task_1326821518301_10_10_r_0",
            "type" : "REDUCE",
            "successfulAttempt" : "",
            "finishTime" : 0
         }
      ]
   }
}

The map task has finished but the reduce task is still running. The users wishes to get the task attempt information for the reduce task task_1326821518301_10_10_r_0, note that the Accept header isn’t really required here since JSON is the default output format:

  curl --compressed -X GET "http://host.domain.com:8088/proxy/application_1326821518301_0010/ws/v1/mapreduce/jobs/job_1326821518301_10_10/tasks/task_1326821518301_10_10_r_0/attempts"

Output:

{
   "taskAttempts" : {
      "taskAttempt" : [
         {
            "elapsedMergeTime" : 158,
            "shuffleFinishTime" : 1326860735378,
            "assignedContainerId" : "container_1326821518301_0010_01_000003",
            "progress" : 72.104515,
            "elapsedTime" : 0,
            "state" : "RUNNING",
            "elapsedShuffleTime" : 2394,
            "mergeFinishTime" : 1326860735536,
            "rack" : "/10.10.10.0",
            "elapsedReduceTime" : 0,
            "nodeHttpAddress" : "host.domain.com:8042",
            "type" : "REDUCE",
            "startTime" : 1326860732984,
            "id" : "attempt_1326821518301_10_10_r_0_0",
            "finishTime" : 0
         }
      ]
   }
}

The reduce attempt is still running and the user wishes to see the current counter values for that attempt:

 curl --compressed -H "Accept: application/json"  -X GET "http://host.domain.com:8088/proxy/application_1326821518301_0010/ws/v1/mapreduce/jobs/job_1326821518301_10_10/tasks/task_1326821518301_10_10_r_0/attempts/attempt_1326821518301_10_10_r_0_0/counters" 

Output:

{
   "JobTaskAttemptCounters" : {
      "taskAttemptCounterGroup" : [
         {
            "counterGroupName" : "org.apache.hadoop.mapreduce.FileSystemCounter",
            "counter" : [
               {
                  "value" : 4216,
                  "name" : "FILE_BYTES_READ"
               }, 
               {
                  "value" : 77151,
                  "name" : "FILE_BYTES_WRITTEN"
               }, 
               {
                  "value" : 0,
                  "name" : "FILE_READ_OPS"
               },
               {
                  "value" : 0,
                  "name" : "FILE_LARGE_READ_OPS"
               },
               {
                  "value" : 0,
                  "name" : "FILE_WRITE_OPS"
               },
               {
                  "value" : 0,
                  "name" : "HDFS_BYTES_READ"
               },
               {
                  "value" : 0,
                  "name" : "HDFS_BYTES_WRITTEN"
               },
               {
                  "value" : 0,
                  "name" : "HDFS_READ_OPS"
               },
               {
                  "value" : 0,
                  "name" : "HDFS_LARGE_READ_OPS"
               },
               {
                  "value" : 0,
                  "name" : "HDFS_WRITE_OPS"
               }
            ]  
         }, 
         {
            "counterGroupName" : "org.apache.hadoop.mapreduce.TaskCounter",
            "counter" : [
               {
                  "value" : 0,
                  "name" : "COMBINE_INPUT_RECORDS"
               }, 
               {
                  "value" : 0,
                  "name" : "COMBINE_OUTPUT_RECORDS"
               }, 
               {  
                  "value" : 1767,
                  "name" : "REDUCE_INPUT_GROUPS"
               },
               {  
                  "value" : 25104,
                  "name" : "REDUCE_SHUFFLE_BYTES"
               },
               {
                  "value" : 1767,
                  "name" : "REDUCE_INPUT_RECORDS"
               },
               {
                  "value" : 0,
                  "name" : "REDUCE_OUTPUT_RECORDS"
               },
               {
                  "value" : 0,
                  "name" : "SPILLED_RECORDS"
               },
               {
                  "value" : 1,
                  "name" : "SHUFFLED_MAPS"
               },
               {
                  "value" : 0,
                  "name" : "FAILED_SHUFFLE"
               },
               {
                  "value" : 1,
                  "name" : "MERGED_MAP_OUTPUTS"
               },
               {
                  "value" : 50,
                  "name" : "GC_TIME_MILLIS"
               },
               {
                  "value" : 1580,
                  "name" : "CPU_MILLISECONDS"
               },
               {
                  "value" : 141320192,
                  "name" : "PHYSICAL_MEMORY_BYTES"
               },
              {
                  "value" : 1118552064,
                  "name" : "VIRTUAL_MEMORY_BYTES"
               }, 
               {  
                  "value" : 73728000,
                  "name" : "COMMITTED_HEAP_BYTES"
               }
            ]
         },
         {  
            "counterGroupName" : "Shuffle Errors",
            "counter" : [
               {  
                  "value" : 0,
                  "name" : "BAD_ID"
               },
               {  
                  "value" : 0,
                  "name" : "CONNECTION"
               },
               {  
                  "value" : 0,
                  "name" : "IO_ERROR"
               },
               {  
                  "value" : 0,
                  "name" : "WRONG_LENGTH"
               },
               {  
                  "value" : 0,
                  "name" : "WRONG_MAP"
               },
               {  
                  "value" : 0,
                  "name" : "WRONG_REDUCE"
               }
            ]
         },
         {  
            "counterGroupName" : "org.apache.hadoop.mapreduce.lib.output.FileOutputFormatCounter",
            "counter" : [
              {  
                  "value" : 0,
                  "name" : "BYTES_WRITTEN"
               }
            ]
         }
      ],
      "id" : "attempt_1326821518301_10_10_r_0_0"
   }
}

The job finishes and the user wishes to get the final job information from the history server for this job.

  curl --compressed -X GET "http://host.domain.com:19888/ws/v1/history/mapreduce/jobs/job_1326821518301_10_10" 

Output:

{
   "job" : {
      "avgReduceTime" : 1250784,
      "failedReduceAttempts" : 0,
      "state" : "SUCCEEDED",
      "successfulReduceAttempts" : 1,
      "acls" : [
         {
            "value" : " ",
            "name" : "mapreduce.job.acl-modify-job"
         },
         {
            "value" : " ",
            "name" : "mapreduce.job.acl-view-job"
         }
      ],
      "user" : "user1",
      "reducesTotal" : 1,
      "mapsCompleted" : 1,
      "startTime" : 1326860720902,
      "id" : "job_1326821518301_10_10",
      "avgMapTime" : 5059,
      "successfulMapAttempts" : 1,
      "name" : "Sleep job",
      "avgShuffleTime" : 2394,
      "reducesCompleted" : 1,
      "diagnostics" : "",
      "failedMapAttempts" : 0,
      "avgMergeTime" : 2552,
      "killedReduceAttempts" : 0,
      "mapsTotal" : 1,
      "queue" : "a1",
      "uberized" : false,
      "killedMapAttempts" : 0,
      "finishTime" : 1326861986164
   }
}

The user also gets the final applications information from the ResourceManager.

  curl --compressed -H "Accept: application/json" -X GET "http://host.domain.com:8088/ws/v1/cluster/apps/application_1326821518301_0010" 

Output:

{
   "app" : {
      "finishedTime" : 1326861991282,
      "amContainerLogs" : "http://host.domain.com:8042/node/containerlogs/container_1326821518301_0010_01_000001",
      "trackingUI" : "History",
      "state" : "FINISHED",
      "user" : "user1",
      "id" : "application_1326821518301_0010",
      "clusterId" : 1326821518301,
      "finalStatus" : "SUCCEEDED",
      "amHostHttpAddress" : "host.domain.com:8042",
      "progress" : 100,
      "name" : "Sleep job",
      "startedTime" : 1326860715335,
      "elapsedTime" : 1275947,
      "diagnostics" : "",
      "trackingUrl" : "http://host.domain.com:8088/proxy/application_1326821518301_0010/jobhistory/job/job_1326821518301_10_10",
      "queue" : "a1"
   }
}

© 2016 Apache Software Foundation - Privacy Policy
Hadoop YARN Change Log

Release 2.7.2 - 2016-01-25

  INCOMPATIBLE CHANGES

  NEW FEATURES

  IMPROVEMENTS

    YARN-4009. CORS support for ResourceManager REST API. ( Varun Vasudev via jeagles)

    YARN-3170. YARN architecture document needs updating. (Brahma Reddy Battula
    via ozawa)

    YARN-3967. Fetch the application report from the AHS if the RM does not know about it.
    (Mit Desai via xgong)

    YARN-2801. Add documentation for node labels feature. (Wangda Tan and Naganarasimha 
    G R  via ozawa)

    YARN-2513. Host framework UIs in YARN for use with the ATS (jeagles)

  OPTIMIZATIONS

  BUG FIXES

    YARN-3793. Several NPEs when deleting local files on NM recovery (Varun
    Saxena via jlowe)

    YARN-3508. Prevent processing preemption events on the main RM dispatcher. 
    (Varun Saxena via wangda)

    YARN-3690. [JDK8] 'mvn site' fails. (Brahma Reddy Battula via aajisaka)

    YARN-3905. Application History Server UI NPEs when accessing apps run after
    RM restart (Eric Payne via jeagles)

    YARN-3535. Scheduler must re-request container resources when RMContainer transitions
    from ALLOCATED to KILLED (rohithsharma and peng.zhang via asuresh)

    YARN-3878. AsyncDispatcher can hang while stopping if it is configured for
    draining events on stop. (Varun Saxena via jianhe)

    YARN-3969. Allow jobs to be submitted to reservation that is active 
    but does not have any allocations. (subru via curino)

    YARN-3925. ContainerLogsUtils#getContainerLogFile fails to read container
    log files from full disks. (zhihai xu via jlowe)

    YARN-3857: Memory leak in ResourceManager with SIMPLE mode.
    (mujunchao via zxu)

    YARN-3893. Both RM in active state when Admin#transitionToActive failure 
    from refeshAll() (Bibin A Chundatt via rohithsharmaks)

    YARN-4103. RM WebServices missing scheme for appattempts logLinks.
    (Jonathan Eagles via vvasudeb)

    YARN-4105. Capacity Scheduler headroom for DRF is wrong (Chang Li via
    jlowe)

    YARN-4096. App local logs are leaked if log aggregation fails to initialize
    for the app. (Jason Lowe via zxu)

    YARN-4153. TestAsyncDispatcher failed at branch-2.7 (Zhihai Xu via jianhe)

    YARN-3697. FairScheduler: ContinuousSchedulingThread can fail to shutdown.
    (Zhihai Xu via kasha)

    YARN-4158. Remove duplicate close for LogWriter in
    AppLogAggregatorImpl#uploadLogsForContainers (Zhihai Xu via jlowe)

    YARN-3975. WebAppProxyServlet should not redirect to RM page if AHS is
    enabled (Mit Desai via jlowe)

    YARN-3624. ApplicationHistoryServer should not reverse the order of the
    filters it gets. (Mit Desai via xgong)

    YARN-4180. AMLauncher does not retry on failures when talking to NM.
    (adhoot)

    YARN-3619. ContainerMetrics unregisters during getMetrics and leads to
    ConcurrentModificationException (Zhihai Xu via jlowe)

    YARN-4209. RMStateStore FENCED state doesnâ€™t work due to updateFencedState called 
    by stateMachine.doTransition. (Zhihai Xu via rohithsharmaks)

    YARN-4281. 2.7 RM app page is broken (Chang Li via jlowe)

    YARN-4000. RM crashes with NPE if leaf queue becomes parent queue during restart.
    (Varun Saxena via jianhe)

    YARN-4041. Slow delegation token renewal can severely prolong RM recovery
    (Sunil G via jlowe)

    YARN-2902. Killing a container that is localizing can orphan resources in
    the DOWNLOADING state (Varun Saxena via jlowe)

    YARN-4313. Race condition in MiniMRYarnCluster when getting history server
    address. (Jian He via xgong)

    YARN-3580. [JDK8] TestClientRMService.testGetLabelsToNodes fails. (Robert Kanter
    via junping_du)

    YARN-4312. TestSubmitApplicationWithRMHA fails on branch-2.7 and branch-2.6
    as some of the test cases time out. (Varun Saxena via ozawa)

    YARN-4320. TestJobHistoryEventHandler fails as AHS in MiniYarnCluster no longer
    binds to default port 8188. (Varun Saxena via ozawa)

    YARN-4321. Incessant retries if NoAuthException is thrown by Zookeeper in non
    HA mode.  (Varun Saxena via jianhe)

    YARN-3136. getTransferredContainers can be a bottleneck during AM registration.
    (Sunil G via jianhe)

    YARN-4127. RM fail with noAuth error if switched from failover mode to non-failover
    mode. (Varun Saxena via jianhe)

    YARN-4354. Public resource localization fails with NPE. (Jason Lowe via 
    junping_du)

Release 2.7.1 - 2015-07-06

  INCOMPATIBLE CHANGES

  NEW FEATURES

  IMPROVEMENTS

    YARN-3243. CapacityScheduler should pass headroom from parent to children
    to make sure ParentQueue obey its capacity limits. (Wangda Tan via jianhe)

    YARN-3539. Updated timeline server documentation and marked REST APIs evolving.
    (Steve Loughran via zjshen)

    YARN-3723. Need to clearly document primaryFilter and otherInfo value type.
    (Zhijie Shen via xgong)

    YARN-3489. RMServerUtils.validateResourceRequests should only obtain queue 
    info once. (Varun Saxena via wangda)

    YARN-3711. Documentation of ResourceManager HA should explain configurations
    about listen addresses. (Masatake Iwasaki via ozawa)

  OPTIMIZATIONS

    YARN-3006. Improve the error message when attempting manual failover with 
    auto-failover enabled. (Akira AJISAKA via wangda)

    YARN-3469. ZKRMStateStore: Avoid setting watches that are not required. 
    (Jun Gong via kasha)

  BUG FIXES

    YARN-3462. Patches applied for YARN-2424 are inconsistent between
    trunk and branch-2. (Naganarasimha G R via harsh)

    YARN-3497. ContainerManagementProtocolProxy modifies IPC timeout conf
    without making a copy. (Jason Lowe via jianhe)

    YARN-2605. [RM HA] Rest api endpoints doing redirect incorrectly.
    (Xuan Gong via stevel)

    YARN-3522. Fixed DistributedShell to instantiate TimeLineClient as the
    correct user. (Zhijie Shen via jianhe)

    YARN-3351. AppMaster tracking URL is broken in HA. (Anubhav Dhoot via kasha)

    YARN-3382. Some of UserMetricsInfo metrics are incorrectly set to root
    queue metrics. (Rohit Agarwal via jianhe)

    YARN-3472. Fixed possible leak in DelegationTokenRenewer#allTokens.
    (Rohith Sharmaks via jianhe)

    YARN-3465. Use LinkedHashMap to preserve order of resource requests. 
    (Zhihai Xu via kasha)

    YARN-3516. killing ContainerLocalizer action doesn't take effect when
    private localizer receives FETCH_FAILURE status.(zhihai xu via xgong)

    YARN-3485. FairScheduler headroom calculation doesn't consider 
    maxResources for Fifo and FairShare policies. (kasha)

    YARN-3301. Fixed the format issue of the new RM attempt web page.
    (Xuan Gong via jianhe)

    YARN-3385. Fixed a race-condition in ResourceManager's ZooKeeper based
    state-store to avoid crashing on duplicate deletes. (Zhihai Xu via vinodkv)

    YARN-3358. Audit log not present while refreshing Service ACLs. 
    (Varun Saxena via devaraj)

    YARN-3476. Nodemanager can fail to delete local logs if log aggregation
    fails (Rohith via jlowe)

    YARN-3434. Interaction between reservations and userlimit can result in 
    significant ULF violation. (Thomas Graves via wangda)

    YARN-3626. On Windows localized resources are not moved to the front
    of the classpath when they should be. (Craig Welch via xgong)

    YARN-3457. NPE when NodeManager.serviceInit fails and stopRecoveryStore called.
    (Bibin A Chundatt via ozawa)

    YARN-3537. NPE when NodeManager.serviceInit fails and stopRecoveryStore
    invoked (Brahma Reddy Battula via jlowe)

    YARN-3601. Fix UT TestRMFailover.testRMWebAppRedirect. (Weiwei Yang via xgong)

    YARN-3677. Fix findbugs warnings in yarn-server-resourcemanager.
    (Vinod Kumar Vavilapalli via ozawa)

    YARN-3681. yarn cmd says "could not find main class 'queue'" in windows.
    (Craig Welch and Varun Saxena via xgong)

    YARN-3609. Load node labels from storage inside RM serviceStart. (Wangda
    Tan via jianhe)

    YARN-3694. Fix dead link for TimelineServer REST API.
    (Jagadesh Kiran N via aajisaka)

    YARN-3646. Applications are getting stuck some times in case of retry
    policy forever. (Raju Bairishetti via devaraj)

    YARN-3675. FairScheduler: RM quits when node removal races with 
    continuous-scheduling on the same node. (Anubhav Dhoot via kasha)

    YARN-3701. Isolating the error of generating a single app report when 
    getting all apps from generic history service. (Zhijie Shen via xgong)

    YARN-2238. filtering on UI sticks even if I move away from the page.
    (Jian He via xgong)

    YARN-3686. CapacityScheduler should trim default_node_label_expression. 
    (Sunil G via wangda)

    YARN-3753. RM failed to come up with "java.io.IOException: Wait for
    ZKClient creation timed outâ€. (Jian He via xgong)

    YARN-3764. CapacityScheduler should forbid moving LeafQueue from one parent
    to another. (Wangda Tan via jianhe)

    YARN-3804. Both RM are on standBy state when kerberos user not in yarn.admin.acl
    (Varun Saxena via xgong)

    YARN-3842. NMProxy should retry on NMNotYetReadyException. 
    (Robert Kanter via kasha)

    YARN-3809. Failed to launch new attempts because
    ApplicationMasterLauncher's threads all hang (Jun Gong via jlowe)

Release 2.7.0 - 2015-04-20

  INCOMPATIBLE CHANGES

  NEW FEATURES
  
    YARN-2179. [YARN-1492] Initial cache manager structure and context. 
    (Chris Trezzo via kasha) 

    YARN-2180. [YARN-1492] In-memory backing store for cache manager. 
    (Chris Trezzo via kasha)

    YARN-2183. [YARN-1492] Cleaner service for cache manager.
    (Chris Trezzo and Sangjin Lee via kasha)

    YARN-2186. [YARN-1492] Node Manager uploader service for cache manager. 
    (Chris Trezzo and Sangjin Lee via kasha)

    YARN-2236. [YARN-1492] Shared Cache uploader service on the Node 
    Manager. (Chris Trezzo and Sangjin Lee via kasha)

    YARN-2188. [YARN-1492] Client service for cache manager. 
    (Chris Trezzo and Sangjin Lee via kasha)

    YARN-2189. [YARN-1492] Admin service for cache manager.
    (Chris Trezzo via kasha)

    YARN-2765. Added leveldb-based implementation for RMStateStore. (Jason Lowe
    via jianhe)

    YARN-2203. [YARN-1492] Web UI for cache manager. (Chris Trezzo via kasha)

    YARN-2738. [YARN-2574] Add FairReservationSystem for FairScheduler. 
    (Anubhav Dhoot via kasha)

    YARN-2881. [YARN-2574] Implement PlanFollower for FairScheduler. 
    (Anubhav Dhoot via kasha)

    YARN-2427. Added the API of moving apps between queues in RM web services.
    (Varun Vasudev via zjshen)

    YARN-2217. [YARN-1492] Shared cache client side changes. 
    (Chris Trezzo via kasha)

    YARN-2616 [YARN-913] Add CLI client to the registry to list, view
    and manipulate entries. (Akshay Radia via stevel)

    YARN-2994. Document work-preserving RM restart. (Jian He via ozawa)

    YARN-2786. Created a yarn cluster CLI and seeded with one command for listing
    node-labels collection. (Wangda Tan via vinodkv)

  IMPROVEMENTS

    YARN-3005. [JDK7] Use switch statement for String instead of if-else
    statement in RegistrySecurity.java (Kengo Seki via aajisaka)

    YARN-2950. Change message to mandate, not suggest JS requirement on UI.
    (Dustin Cote via harsh)

    YARN-2891. Failed Container Executor does not provide a clear error
    message. (Dustin Cote via harsh)

    YARN-1979. TestDirectoryCollection fails when the umask is unusual.
    (Vinod Kumar Vavilapalli and Tsuyoshi OZAWA via junping_du)

    YARN-2641. Decommission nodes on -refreshNodes instead of next 
    NM-RM heartbeat. (Zhihai Xu via kasha)

    YARN-2742. FairSchedulerConfiguration should allow extra spaces
    between value and unit. (Wei Yan via kasha)

    YARN-2712. TestWorkPreservingRMRestart: Augment FS tests with
    queue and headroom checks. (Tsuyoshi Ozawa via kasha)

    YARN-2735. diskUtilizationPercentageCutoff and diskUtilizationSpaceCutoff 
    are initialized twice in DirectoryCollection. (Zhihai Xu via kasha)

    YARN-570. Time strings are formated in different timezone. 
    (Akira Ajisaka and Peng Zhang via kasha)

    YARN-2780. Log aggregated resource allocation in rm-appsummary.log (Eric
    Payne via jlowe)

    YARN-2690. [YARN-2574] Make ReservationSystem and its dependent classes 
    independent of Scheduler type. (Anubhav Dhoot via kasha)

    YARN-2157. Added YARN metrics in the documentaion. (Akira AJISAKA via
    jianhe)

    YARN-2802. ClusterMetrics to include AM launch and register delays.
    (Zhihai Xu via kasha)

    YARN-2375. Allow enabling/disabling timeline server per framework.
    (Mit Desai via jeagles)

    YARN-2604. Scheduler should consider max-allocation-* in conjunction
    with the largest node. (Robert Kanter via kasha)

    YARN-2679. Add metric for container launch duration. (Zhihai Xu via kasha)

    YARN-2669. FairScheduler: queue names shouldn't allow periods
    (Wei Yan via Sandy Ryza)

    YARN-2404. Removed ApplicationAttemptState and ApplicationState class in
    RMStateStore. (Tsuyoshi OZAWA via jianhe)

    YARN-2165. Added the sanity check for the numeric configuration values of
    the timeline service. (Vasanth kumar RJ via zjshen)

    YARN-2907. SchedulerNode#toString should print all resource detail instead 
    of only memory. (Rohith via junping_du)

    YARN-2136. Changed RMStateStore to ignore store opearations when fenced.
    (Varun Saxena via jianhe)

    YARN-1156. Enhance NodeManager AllocatedGB and AvailableGB metrics 
    for aggregation of decimal values. (Tsuyoshi OZAWA via junping_du)

    YARN-2056. Disable preemption at Queue level (Eric Payne via jlowe)

    YARN-2762. Fixed RMAdminCLI to trim and check node-label related arguments
    before sending to RM. (Rohith Sharmaks via jianhe)

    YARN-2972. DelegationTokenRenewer thread pool never expands. (Jason Lowe
    via junping_du)

    YARN-2949. Add documentation for CGroups (Varun Vasudev via junping_du)

    YARN-2970. NodeLabel operations in RMAdmin CLI get missing in help command.
    (Varun Saxena via junping_du)

    YARN-2837. Support TimeLine server to recover delegation token when
    restarting. (Zhijie Shen via jianhe)

    YARN-2993. Several fixes (missing acl check, error log msg ...) and some 
    refinement in AdminService. (Yi Liu via junping_du)

    YARN-2943. Added node-labels page on RM web UI. (Wangda Tan via jianhe)

    YARN-2998. Abstract out scheduler independent PlanFollower components. 
    (Anubhav Dhoot via kasha)

    YARN-2360. Fair Scheduler: Display dynamic fair share for queues on the 
    scheduler page. (Ashwin Shankar and Wei Yan via kasha)

    YARN-2880. Added a test to make sure node labels will be recovered
    if RM restart is enabled. (Rohith Sharmaks via jianhe)

    YARN-2996. Improved synchronization and I/O operations of FS- and Mem-
    RMStateStore. (Yi Liu via zjshen)

    YARN-2956. Added missing links in YARN documentation. (Masatake Iwasaki via
    jianhe)

    YARN-2957. Create unit test to automatically compare YarnConfiguration
    and yarn-default.xml. (rchiang via rkanter)

    YARN-2643. Don't create a new DominantResourceCalculator on every
    FairScheduler.allocate call. (kasha via rkanter)

    YARN-3019. Make work-preserving-recovery the default mechanism for RM 
    recovery. (Jian He via junping_du)

    YARN-2807. Option "--forceactive" not works as described in usage of
    "yarn rmadmin -transitionToActive". (Masatake Iwasaki via xgong)

    YARN-2984. Metrics for container's actual memory usage. (kasha)

    YARN-2800. Remove MemoryNodeLabelsStore and add a way to enable/disable
    node labels feature. (Wangda Tan via ozawa)

    YARN-3086. Make NodeManager memory configurable in MiniYARNCluster.
    (Robert Metzger via ozawa)

    YARN-2897. CrossOriginFilter needs more log statements (Mit Desai via
    jeagles)

    YARN-3028. Better syntax for replaceLabelsOnNode in RMAdmin CLI
    (Rohith Sharmaks via wangda)

    YARN-2932. Add entry for "preemptable" status (enabled/disabled) to 
    scheduler web UI and queue initialize/refresh logging. 
    (Eric Payne via wangda)

    YARN-3108. ApplicationHistoryServer doesn't process -D arguments (Chang Li
    via jeagles)

    YARN-2808. Made YARN CLI list attemptâ€™s finished containers of a running
    application. (Naganarasimha G R via zjshen)

    YARN-3085. Application summary should include the application type (Rohith
    via jlowe)

    YARN-3022. Expose Container resource information from NodeManager for
    monitoring (adhoot via ranter)

    YARN-3075. NodeLabelsManager implementation to retrieve label to node 
    mapping (Varun Saxena via wangda)

    YARN-1393. SLS: Add how-to-use instructions. (Wei Yan via kasha)

    YARN-1723. AMRMClientAsync missing blacklist addition and removal
    functionality. (Bartosz Åugowski via sseth)

    YARN-3123. Made YARN CLI show a single completed container even if the app
    is running. (Naganarasimha G R via zjshen)

    YARN-1582. Capacity Scheduler: add a maximum-allocation-mb setting per
    queue (Thomas Graves via jlowe)

    YARN-1904. Ensure exceptions thrown in ClientRMService &
    ApplicationHistoryClientService are uniform when application-attempt is
    not found. (zjshen via acmurthy)

    YARN-3144. Configuration for making delegation token failures to timeline
    server not-fatal (Jonathan Eagles via jlowe)

    YARN-3155. Refactor the exception handling code for TimelineClientImpl's 
    retryOn method (Li Lu via wangda)

    YARN-3100. Made YARN authorization pluggable. (Jian He via zjshen)

    YARN-2683. [YARN-913] registry config options: document and move to
    core-default. (stevel)

    YARN-1237. Description for yarn.nodemanager.aux-services in 
    yarn-default.xml is misleading. (Brahma Reddy Battula via ozawa)

    YARN-3157. Refactor the exception handling in ConverterUtils#to*Id.
    (Bibin A Chundatt via ozawa)

    YARN-3147. Clean up RM web proxy code. (Steve Loughran via xgong)

    YARN-2079. Recover NonAggregatingLogHandler state upon nodemanager
    restart. (Jason Lowe via junping_du) 

    YARN-3158. Correct log messages in ResourceTrackerService.
    (Varun Saxena via xgong)

    YARN-3179. Update use of Iterator to Iterable in RMAdminCLI and
    CommonNodeLabelsManager. (Ray Chiang via xgong)

    YARN-3182. Cleanup switch statement in ApplicationMasterLauncher#handle().
    (Ray Chiang via ozawa)

    YARN-3203. Correct a log message in AuxServices. (Brahma Reddy Battula 
    via ozawa)

    YARN-1299. Improve a log message in AppSchedulingInfo by adding application 
    id. (Ashutosh Jindal and Devaraj K via ozawa)

    YARN-1514. Utility to benchmark ZKRMStateStore#loadState for RM HA.
    (Tsuyoshi OZAWA via jianhe)

    YARN-3076. Add API/Implementation to YarnClient to retrieve label-to-node 
    mapping. (Varun Saxena via wangda)

    YARN-2799. Cleanup TestLogAggregationService based on the change in YARN-90.
    (Zhihai Xu via junping_du)

    YARN-3237. AppLogAggregatorImpl fails to log error cause.
    (Rushabh S Shah via xgong)

    YARN-3236. Cleanup RMAuthenticationFilter#AUTH_HANDLER_PROPERTY.
    (zhihai xu via xgong)

    YARN-2797. TestWorkPreservingRMRestart should use ParametrizedSchedulerTestBase
    (Karthik Kambatla via xgong)

    YARN-2797. Add -help to yarn logs and nodes CLI command. 
    (Jagadesh Kiran N via devaraj)

    YARN-3217. Remove httpclient dependency from hadoop-yarn-server-web-proxy.
    (Brahma Reddy Battula via ozawa).

    YARN-3255. RM, NM, JobHistoryServer, and WebAppProxyServer's main()
    should support generic options. (shv)

    YARN-2820. Retry in FileSystemRMStateStore when FS's operations fail 
    due to IOException. (Zhihai Xu via ozawa)

    YARN-3262. Surface application outstanding resource requests table 
    in RM web UI. (Jian He via wangda)

    YARN-3281. Added RMStateStore to StateMachine visualization list.
    (Chengbing Liu via jianhe)

    YARN-3272. Surface container locality info in RM web UI.
    (Jian He via wangda)

    YARN-3285. (Backport YARN-3168) Convert branch-2 .apt.vm files of YARN to
    markdown. (Masatake Iwasaki via jianhe)

    YARN-3122. Metrics for container's actual CPU usage. 
    (Anubhav Dhoot via kasha)

    YARN-2190. Added CPU and memory limit options to the default container
    executor for Windows containers. (Chuan Liu via jianhe)

    YARN-3296. Mark ResourceCalculatorProcessTree class as Public for configurable
    resource monitoring. (Hitesh Shah via junping_du)

    YARN-3187. Documentation of Capacity Scheduler Queue mapping based on user
    or group. (Gururaj Shetty via jianhe)

    YARN-2854. Updated the documentation of the timeline service and the generic
    history service. (Naganarasimha G R via zjshen)

    Backport part of YARN-3273 to rename
    CapacitySchedulerLeafQueueInfo#aMResourceLimit to AMResourceLimit.
    (Rohith via jianhe)

    YARN-2777. Mark the end of individual log in aggregated log.
    (Varun Saxena via xgong)

    YARN-3273. Improve scheduler UI to facilitate scheduling analysis and
    debugging. (Rohith Sharmaks via jianhe)

  OPTIMIZATIONS

    YARN-2990. FairScheduler's delay-scheduling always waits for node-local and 
    rack-local delays, even for off-rack-only requests. (kasha)

  BUG FIXES

    YARN-3071. Remove invalid char from sample conf in doc of FairScheduler.
    (Masatake Iwasaki via aajisaka)

    YARN-2254. TestRMWebServicesAppsModification should run against both 
    CS and FS. (Zhihai Xu via kasha)

    YARN-2713. "RM Home" link in NM should point to one of the RMs in an 
    HA setup. (kasha)

    YARN-2857. ConcurrentModificationException in ContainerLogAppender
    (Mohammad Kamrul Islam via jlowe)

    YARN-2432. RMStateStore should process the pending events before close.
    (Varun Saxena via jianhe)

    YARN-1703. Fixed ResourceManager web-proxy to close connections correctly.
    (Rohith Sharma via vinodkv)

    YARN-2870. Updated the command to run the timeline server in the document.
    (Masatake Iwasaki via zjshen)

    YARN-2878. Fix DockerContainerExecutor.apt.vm formatting. (Abin Shahab via
    jianhe)

    YARN-2315. FairScheduler: Set current capacity in addition to capacity.
    (Zhihai Xu via kasha)

    YARN-2697. Remove useless RMAuthenticationHandler. (Haosong Huang via zjshen)

    YARN-2461. Fix PROCFS_USE_SMAPS_BASED_RSS_ENABLED property in
    YarnConfiguration. (rchiang via rkanter)

    YARN-2869. CapacityScheduler should trim sub queue names when parse
    configuration. (Wangda Tan via jianhe)

    YARN-2927. [YARN-1492] InMemorySCMStore properties are inconsistent. 
    (Ray Chiang via kasha)

    YARN-2931. PublicLocalizer may fail until directory is initialized by
    LocalizeRunner. (Anubhav Dhoot via kasha)

    YARN-2930. Fixed TestRMRestart#testRMRestartRecoveringNodeLabelManager
    intermittent failure. (Wangda Tan via jianhe)

    YARN-2924. Fixed RMAdminCLI to not convert node labels to lower case.
    (Wangda Tan via jianhe)

    YARN-2243. Order of arguments for Preconditions.checkNotNull() is wrong in 
    SchedulerApplicationAttempt ctor. (devaraj)

    YARN-2912 Jersey Tests failing with port in use. (varun saxena via stevel)

    YARN-2356. yarn status command for non-existent application/application 
    attempt/container is too verbose. (Sunil G via devaraj)

    YARN-2914. [YARN-1492] Potential race condition in Singleton implementation of 
    SharedCacheUploaderMetrics, CleanerMetrics, ClientSCMMetrics. (Varun Saxena via kasha)

    YARN-2945. FSLeafQueue#assignContainer - document the reason for using both write and
    read locks. (Tsuyoshi Ozawa via kasha)

    YARN-2944. InMemorySCMStore can not be instantiated with ReflectionUtils#newInstance.
    (Chris Trezzo via kasha)

    YARN-2675. containersKilled metrics is not updated when the container is killed 
    during localization. (Zhihai Xu via kasha)

    YARN-2975. FSLeafQueue app lists are accessed without required locks. (kasha)

    YARN-2977. Fixed intermittent TestNMClient failure.
    (Junping Du via ozawa)

    YARN-2939. Fix new findbugs warnings in hadoop-yarn-common. (Li Lu via junping_du)

    YARN-2940. Fix new findbugs warnings in rest of the hadoop-yarn components. (Li Lu 
    via junping_du)

    YARN-2937. Fixed new findbugs warnings in hadoop-yarn-nodemanager. (Varun Saxena
    via zjshen)

    YARN-2946. Fixed potential deadlock in RMStateStore. (Rohith Sharmaks via
    jianhe)

    YARN-2988. Graph#save() may leak file descriptors. (Ted Yu via ozawa)

    YARN-2938. Fixed new findbugs warnings in hadoop-yarn-resourcemanager and
    hadoop-yarn-applicationhistoryservice. (Varun Saxena via zjshen)

    YARN-2987. Fixed ClientRMService#getQueueInfo to check against queue and
    app ACLs. (Varun Saxena via jianhe)

    YARN-2991. Fixed DrainDispatcher to reuse the draining code path in
    AsyncDispatcher. (Rohith Sharmaks via zjshen)

    YARN-2958. Made RMStateStore not update the last sequence number when updating the
    delegation token. (Varun Saxena via zjshen)

    YARN-2230. Fixed few configs description in yarn-default.xml. (Vijay Bhat
    via jianhe)

    YARN-3010. Fixed findbugs warning in AbstractYarnScheduler. (Yi Liu via
    jianhe)

    YARN-2936. Changed YARNDelegationTokenIdentifier to set proto fields on
    getProto method. (Varun Saxena via jianhe)

    YARN-3014. Replaces labels on a host should update all NM's labels on that
    host. (Wangda Tan via jianhe)

    YARN-3027. Scheduler should use totalAvailable resource from node instead of
    availableResource for maxAllocation. (adhoot via rkanter)

    YARN-2861. Fixed Timeline DT secret manager to not reuse RM's configs.
    (Zhijie Shen via jianhe)

    YARN-3064. TestRMRestart/TestContainerResourceUsage/TestNodeManagerResync 
    failure with allocation timeout. (Jian He via junping_du)

    YARN-2815. Excluded transitive dependency of JLine in hadoop-yarn-server-common.
    (Ferdinand Xu via zjshen)

    YARN-3070. TestRMAdminCLI#testHelp fails for transitionToActive command. 
    (Contributed by Junping Du)

    YARN-3015. yarn classpath command should support same options as hadoop
    classpath. (Contributed by Varun Saxena)

    YARN-2933. Capacity Scheduler preemption policy should only consider capacity 
    without labels temporarily. (Mayank Bansal via wangda)

    YARN-2731. Fixed RegisterApplicationMasterResponsePBImpl to properly invoke 
    maybeInitBuilder. (Carlo Curino via wangda)

    YARN-3078. LogCLIHelpers lacks of a blank space before string 'does not exist'.
    (Sam Liu via ozawa)

    YARN-3082. Non thread safe access to systemCredentials in NodeHeartbeatResponse
    processing. (Anubhav Dhoot via ozawa)

    YARN-3088. LinuxContainerExecutor.deleteAsUser can throw NPE if native
    executor returns an error (Eric Payne via jlowe)

    YARN-3079. Scheduler should also update maximumAllocation when updateNodeResource.
    (Zhihai Xu via wangda)

    YARN-3029. FSDownload.unpack() uses local locale for FS case conversion, may not
    work everywhere. (Varun Saxena via ozawa)

    YARN-3077. Fixed RM to create zk root path recursively. (Chun Chen via jianhe)

    YARN-3113. Release audit warning for Sorting icons.psd. (stevel via kihwal)

    YARN-3056. Add verification for containerLaunchDuration
    in TestNodeManagerMetrics. (zhihai xu via xgong)

    YARN-2543. Made resource usage be published to the timeline server too.
    (Naganarasimha G R via zjshen)

    YARN-3058. Fix error message of tokens' activation delay configuration.
    (Yi Liu via ozawa)

    YARN-3101. In Fair Scheduler, fix canceling of reservations for exceeding
    max share (Anubhav Dhoot via Sandy Ryza)

    YARN-3149. Fix typo in message for invalid application id.
    (Bibin A Chundatt via xgong)

    YARN-3145. Fixed ConcurrentModificationException on CapacityScheduler
    ParentQueue#getQueueUserAclInfo. (Tsuyoshi OZAWA via jianhe)

    YARN-1537. Fix race condition in
    TestLocalResourcesTrackerImpl.testLocalResourceCache. (xgong via acmurthy)

    YARN-3089. LinuxContainerExecutor does not handle file arguments to
    deleteAsUser (Eric Payne via jlowe)

    YARN-3143. RM Apps REST API can return NPE or entries missing id and other
    fields (jlowe)

    YARN-2971. RM uses conf instead of token service address to renew timeline
    delegation tokens (jeagles)

    YARN-3090. DeletionService can silently ignore deletion task failures
    (Varun Saxena via jlowe)

    YARN-2809. Implement workaround for linux kernel panic when removing
    cgroup (Nathan Roberts via jlowe)

    YARN-3160. Fix non-atomic operation on nodeUpdateQueue in RMNodeImpl. 
    (Chengbing Liu via junping_du)

    YARN-3074. Nodemanager dies when localizer runner tries to write to a full
    disk (Varun Saxena via jlowe)

    YARN-3151. On Failover tracking url wrong in application cli for
    KILLED application (Rohith via xgong)

    YARN-1580. Documentation error regarding "container-allocation.expiry-interval-ms" 
    (Brahma Reddy Battula via junping_du)

    YARN-3104. Fixed RM to not generate new AMRM tokens on every heartbeat
    between rolling and activation. (Jason Lowe via jianhe)

    YARN-3191. Log object should be initialized with its own class. (Rohith via
    aajisaka)

    YARN-3164. RMAdmin command usage prints incorrect command name. 
    (Bibin A Chundatt via xgong)

    YARN-2847. Linux native container executor segfaults if default banned
    user detected (Olaf Flebbe via jlowe)

    YARN-2899. Run TestDockerContainerExecutorWithMocks on Linux only.
    (Ming Ma via cnauroth)

    YARN-2749. Fix some testcases from TestLogAggregationService fails in trunk. 
    (Xuan Gong via junping_du)

    YARN-3132. RMNodeLabelsManager should remove node from node-to-label mapping
    when node becomes deactivated. (Wangda Tan via jianhe)

    YARN-1615. Fix typos in description about delay scheduling. (Akira Ajisaka via 
    ozawa)

    YARN-933. Fixed InvalidStateTransitonException at FINAL_SAVING state in
    RMApp. (Rohith Sharmaks via jianhe)

    YARN-3247. TestQueueMappings should use CapacityScheduler explicitly.
    (Zhihai Xu via ozawa)

    YARN-3256. TestClientToAMTokens#testClientTokenRace is not running against 
    all Schedulers even when using ParameterizedSchedulerTestBase. 
    (Anubhav Dhoot via devaraj)

    YARN-3270. Fix node label expression not getting set in 
    ApplicationSubmissionContext (Rohit Agarwal via wangda)

    YARN-3265. Fixed a deadlock in CapacityScheduler by always passing a queue's
    available resource-limit from the parent queue. (Wangda Tan via vinodkv)

    YARN-3131. YarnClientImpl should check FAILED and KILLED state in
    submitApplication (Chang Li via jlowe)
    
    YARN-3275. CapacityScheduler: Preemption happening on non-preemptable
    queues (Eric Payne via jlowe)

    YARN-3300. Outstanding_resource_requests table should not be shown in AHS.
    (Xuan Gong via jianhe)

    YARN-3295. Fix documentation nits found in markdown conversion.
    (Masatake Iwasaki via ozawa)

    YARN-3338. Exclude jline dependency from YARN. (Zhijie Shen via xgong)

    YARN-3154. Added additional APIs in LogAggregationContext to avoid aggregating
    running logs of application when rolling is enabled. (Xuan Gong via vinodkv)

    YARN-1453. [JDK8] Fix Javadoc errors caused by incorrect or illegal tags in 
    doc comments. (Akira AJISAKA, Andrew Purtell, and Allen Wittenauer via ozawa)

    YARN-3349. Treat all exceptions as failure in
    TestFSRMStateStore#testFSRMStateStoreClientRetry. (Zhihai Xu via ozawa)

    YARN-3379. Fixed missing data in localityTable and ResourceRequests table
    in RM WebUI. (Xuan Gong via jianhe)

    YARN-3384. TestLogAggregationService.verifyContainerLogs fails after
    YARN-2777. (Naganarasimha G R via ozawa)

    YARN-3336. FileSystem memory leak in DelegationTokenRenewer.
    (Zhihai Xu via cnauroth)

    YARN-2213. Change proxy-user cookie log in AmIpFilter to DEBUG.
    (Varun Saxena via xgong)

    YARN-3304. Cleaning up ResourceCalculatorProcessTree APIs for public use and
    removing inconsistencies in the default values. (Junping Du and Karthik
    Kambatla via vinodkv)

    YARN-3430. Made headroom data available on app attempt page of RM WebUI.
    (Xuan Gong via zjshen)

    YARN-3466. Fix RM nodes web page to sort by node HTTP-address, #containers 
    and node-label column (Jason Lowe via wangda)

Release 2.6.3 - UNRELEASED

  INCOMPATIBLE CHANGES

  NEW FEATURES

  IMPROVEMENTS

  OPTIMIZATIONS

  BUG FIXES

    YARN-4424. Fix deadlock in RMAppImpl. (jianhe via wtan)

    YARN-4434. NodeManager Disk Checker parameter documentation is not correct.
    (Weiwei Yang via aajisaka)

    YARN-4348. ZKRMStateStore.syncInternal shouldn't wait for sync completion for
    avoiding blocking ZK's event thread. (ozawa)

    YARN-4365. FileSystemNodeLabelStore should check for root dir existence on
    startup (Kuhu Shukla via jlowe)

    YARN-4365. FileSystemNodeLabelStore should check for root dir existence on
    startup (Kuhu Shukla via jlowe)

    YARN-4344. NMs reconnecting with changed capabilities can lead to wrong
    cluster resource calculations (Varun Vasudev via jlowe)

    YARN-2859. ApplicationHistoryServer binds to default port 8188 in MiniYARNCluster.
    (Vinod Kumar Vavilapalli via xgong)

    YARN-4241. Fix typo of property name in yarn-default.xml.
    (Anthony Rojas via aajisaka)

    YARN-4326. Fix TestDistributedShell timeout as AHS in MiniYarnCluster no longer 
    binds to default port 8188. (Meng Ding via wangda)

Release 2.6.2 - 2015-10-28

  INCOMPATIBLE CHANGES

  NEW FEATURES

    YARN-2019. Retrospect on decision of making RM crashed if any exception throw 
    in ZKRMStateStore. (Jian He via junping_du)

  IMPROVEMENTS

    YARN-4092. Fixed UI redirection to print useful messages when both RMs are
    in standby mode. (Xuan Gong via jianhe)

    YARN-4101. RM should print alert messages if Zookeeper and Resourcemanager
    gets connection issue. (Xuan Gong via jianhe)

  OPTIMIZATIONS

  BUG FIXES

    YARN-4087. Followup fixes after YARN-2019 regarding RM behavior when
    state-store error occurs. (Jian He via xgong)

    YARN-3554. Default value for maximum nodemanager connect wait time is too
    high (Naganarasimha G R via jlowe)

    YARN-3727. For better error recovery, check if the directory exists before
    using it for localization. (Zhihai Xu via jlowe)

    YARN-4005. Completed container whose app is finished is possibly not
    removed from NMStateStore. (Jun Gong via jianhe)

    YARN-3780. Should use equals when compare Resource in RMNodeImpl#ReconnectNodeTransition.
    (zhihai xu via devaraj)

    YARN-3802. Two RMNodes for the same NodeId are used in RM sometimes
    after NM is reconnected. (zhihai xu via xgong)

    YARN-3194. RM should handle NMContainerStatuses sent by NM while
    registering if NM is Reconnected node (Rohith via jlowe)

    YARN-3896. RMNode transitioned from RUNNING to REBOOTED because its response id 
    has not been reset synchronously. (Jun Gong via rohithsharmaks)

    YARN-3798. ZKRMStateStore shouldn't create new session without occurrance of 
    SESSIONEXPIED. (ozawa and Varun Saxena)

Release 2.6.1 - 2015-09-23

  INCOMPATIBLE CHANGES

  NEW FEATURES

    YARN-3249. Add a 'kill application' button to Resource Manager's Web UI.
    (Ryu Kobayashi via ozawa)

  IMPROVEMENTS

    YARN-3230. Clarify application states on the web UI. (Jian He via wangda)

    YARN-1809. Synchronize RM and TimeLineServer Web-UIs. (Zhijie Shen and
    Xuan Gong via jianhe)

    YARN-3092. Created a common ResourceUsage class to track labeled resource
    usages in Capacity Scheduler. (Wangda Tan via jianhe)

    YARN-3098. Created common QueueCapacities class in Capacity Scheduler to
    track capacities-by-labels of queues. (Wangda Tan via jianhe)

    YARN-2301. Improved yarn container command. (Naganarasimha G R via jianhe)

    YARN-3978. Configurably turn off the saving of container info in Generic AHS
    (Eric Payne via jeagles)

    YARN-3248. Display count of nodes blacklisted by apps in the web UI.
    (Varun Vasudev via xgong)

  OPTIMIZATIONS

  BUG FIXES

    YARN-2856. Fixed RMAppImpl to handle ATTEMPT_KILLED event at ACCEPTED state
    on app recovery. (Rohith Sharmaks via jianhe)

    YARN-2816. NM fail to start with NPE during container recovery (Zhihai Xu
    via jlowe)

    YARN-2414. RM web UI: app page will crash if app is failed before any
    attempt has been created (Wangda Tan via jlowe)

    YARN-2865. Fixed RM to always create a new RMContext when transtions from
    StandBy to Active. (Rohith Sharmaks via jianhe)

    YARN-2906. CapacitySchedulerPage shows HTML tags for a queue's Active Users.
    (Jason Lowe via jianhe)

    YARN-2905. AggregatedLogsBlock page can infinitely loop if the aggregated
    log file is corrupted (Varun Saxena via jlowe)

    YARN-2890. MiniYARNCluster should start the timeline server based on the
    configuration. (Mit Desai via zjshen)

    YARN-2894. Fixed a bug regarding application view acl when RM fails over.
    (Rohith Sharmaks via jianhe)

    YARN-2874. Dead lock in "DelegationTokenRenewer" which blocks RM to execute
    any further apps. (Naganarasimha G R via kasha)

    YARN-2910. FSLeafQueue can throw ConcurrentModificationException. 
    (Wilfred Spiegelenburg via kasha)

    YARN-2917. Fixed potential deadlock when system.exit is called in AsyncDispatcher
    (Rohith Sharmaks via jianhe)

    YARN-2964. RM prematurely cancels tokens for jobs that submit jobs (oozie)
    (Jian He via jlowe)

    YARN-1984. LeveldbTimelineStore does not handle db exceptions properly
    (Varun Saxena via jlowe)

    YARN-2952. Fixed incorrect version check in StateStore. (Rohith Sharmaks
    via jianhe)

    YARN-2340. Fixed NPE when queue is stopped during RM restart.
    (Rohith Sharmaks via jianhe)

    YARN-2992. ZKRMStateStore crashes due to session expiry. (Karthik Kambatla
    via jianhe)

    YARN-2922. ConcurrentModificationException in CapacityScheduler's LeafQueue.
    (Rohith Sharmaks via ozawa)

    YARN-2997. Fixed NodeStatusUpdater to not send alreay-sent completed
    container statuses on heartbeat. (Chengbing Liu via jianhe)

    YARN-3011. Possible IllegalArgumentException in ResourceLocalizationService
    might lead NM to crash. (Varun Saxena via jianhe)

    YARN-3103. AMRMClientImpl does not update AMRM token properly. (Jason Lowe
    via jianhe)

    YARN-3094. Reset timer for liveness monitors after RM recovery. (Jun Gong
    via jianhe)

    YARN-2246. Made the proxy tracking URL always be
    http(s)://proxy addr:port/proxy/<appId> to avoid duplicate sections. (Devaraj
    K via zjshen)

    YARN-3207. Secondary filter matches entites which do not have the key being
    filtered for. (Zhijie Shen via xgong)

    YARN-3238. Connection timeouts to nodemanagers are retried at
    multiple levels (Jason Lowe via xgong)

    YARN-3239. WebAppProxy does not support a final tracking url which has
    query fragments and params (Jian He via jlowe)

    YARN-3222. Fixed RMNode to send scheduler events in sequential order when a
    node reconnects. (Rohith Sharma K S via jianhe)

    YARN-3231. FairScheduler: Changing queueMaxRunningApps interferes with pending 
    jobs. (Siqi Li via kasha)

    YARN-3242. Asynchrony in ZK-close can lead to ZKRMStateStore watcher receiving 
    events for old client. (Zhihai Xu via kasha)

    YARN-3227. Timeline renew delegation token fails when RM user's TGT is expired
    (Zhijie Shen via xgong)

    YARN-3287. Made TimelineClient put methods do as the correct login context.
    (Daryn Sharp and Jonathan Eagles via zjshen)

    YARN-3267. Timelineserver applies the ACL rules after applying the limit on
    the number of records (Chang Li via jeagles)

    YARN-3369. Missing NullPointer check in AppSchedulingInfo causes RM to die.
    (Brahma Reddy Battula via wangda)

    YARN-3393. Getting application(s) goes wrong when app finishes before
    starting the attempt. (Zhijie Shen via xgong)

    YARN-3055. Fixed ResourceManager's DelegationTokenRenewer to not stop token
    renewal of applications part of a bigger workflow. (Daryn Sharp via vinodkv)

    YARN-3493. RM fails to come up with error "Failed to load/recover state" 
    when mem settings are changed. (Jian He via wangda)

    YARN-3487. CapacityScheduler scheduler lock obtained unnecessarily when 
    calling getQueue (Jason Lowe via wangda)

    YARN-3024. LocalizerRunner should give DIE action when all resources are
    localized. (Chengbing Liu via xgong)

    YARN-3464. Race condition in LocalizerRunner kills localizer before 
    localizing all resources. (Zhihai Xu via kasha)

    YARN-3641. NodeManager: stopRecoveryStore() shouldn't be skipped when
    exceptions happen in stopping NM's sub-services. (Junping Du via jlowe)

    YARN-3526. ApplicationMaster tracking URL is incorrectly redirected
    on a QJM cluster. (Weiwei Yang via xgong)

    YARN-2766. Made ApplicationHistoryManager return a sorted list of apps,
    attempts and containers. (Robert Kanter via zjshen)

    YARN-3700. Made generic history service load a number of latest applications
    according to the parameter or the configuration. (Xuan Gong via zjshen)

    YARN-2900. Application (Attempt and Container) Not Found in AHS results
    in InternalServer Error (500). (Zhijie Shen and Mit Desai via xgong)

    YARN-3725. App submission via REST API is broken in secure mode due to
    Timeline DT service address is empty. (Zhijie Shen via wangda)

    YARN-3585. NodeManager cannot exit on SHUTDOWN event triggered and NM
    recovery is enabled (Rohith Sharmaks via jlowe)

    YARN-3832. Resource Localization fails on a cluster due to existing cache
    directories (Brahma Reddy Battula via jlowe)

    YARN-3850. NM fails to read files from full disks which can lead to
    container logs being lost and other issues (Varun Saxena via jlowe)

    YARN-3990. AsyncDispatcher may overloaded with RMAppNodeUpdateEvent when
    Node is connected/disconnected (Bibin A Chundatt via jlowe)

    YARN-2637. Fixed max-am-resource-percent calculation in CapacityScheduler
    when activating applications. (Craig Welch via jianhe)

    YARN-3733. Fix DominantRC#compare() does not work as expected if
    cluster resource is empty. (Rohith Sharmaks via wangda)

    YARN-2920. Changed CapacityScheduler to kill containers on nodes where
    node labels are changed. (Wangda Tan via jianhe)

    YARN-2978. Fixed potential NPE while getting queue info. (Varun Saxena via
    jianhe)

    YARN-3099. Capacity Scheduler LeafQueue/ParentQueue should use ResourceUsage
    to track used-resources-by-label.(Wangda Tan via jianhe)

    YARN-2694. Ensure only single node label specified in ResourceRequest.
    (Wangda Tan via jianhe)

    YARN-3124. Fixed CS LeafQueue/ParentQueue to use QueueCapacities to track
    capacities-by-label. (Wangda Tan via jianhe)

    YARN-2918. RM should not fail on startup if queue's configured labels do
    not exist in cluster-node-labels. (Wangda Tan via jianhe)

    YARN-3999. RM hangs on draing events. (Jian He via xgong)

    YARN-4047. ClientRMService getApplications has high scheduler lock contention.
    (Jason Lowe via jianhe)

    YARN-1884. Added nodeHttpAddress into ContainerReport and fixed the link to NM
    web page. (Xuan Gong via zjshen)

    YARN-3171. Sort by Application id, AppAttempt and ContainerID doesn't work
    in ATS / RM web ui. (Naganarasimha G R via xgong)

    YARN-3740. Fixed the typo in the configuration name:
    APPLICATION_HISTORY_PREFIX_MAX_APPS. (Xuan Gong via zjshen)

    YARN-3544. Got back AM logs link on the RM web UI for a completed app.
    (Xuan Gong via zjshen)

Release 2.6.0 - 2014-11-18

  INCOMPATIBLE CHANGES

  NEW FEATURES

    YARN-1964. Create Docker analog of the LinuxContainerExecutor in YARN. (Abin 
    Shahab via raviprak)

    YARN-2131. Add a way to format the RMStateStore. (Robert Kanter via kasha)

    YARN-1367. Changed NM to not kill containers on NM resync if RM work-preserving
    restart is enabled. (Anubhav Dhoot via jianhe)

    YARN-1366. Changed AMRMClient to re-register with RM and send outstanding requests
    back to RM on work-preserving RM restart. (Rohith via jianhe)

    YARN-2181. Added preemption info to logs and RM web UI. (Wangda Tan via
    jianhe)

    YARN-1354. Recover applications upon nodemanager restart. (Jason Lowe via 
    junping_du)

    YARN-1337. Recover containers upon nodemanager restart. (Jason Lowe via 
    junping_du)

    YARN-2277. Added cross-origin support for the timeline server web services.
    (Jonathan Eagles via zjshen)

    YARN-2378. Added support for moving applications across queues in
    CapacityScheduler. (Subramaniam Venkatraman Krishnan via jianhe)

    YARN-2411. Support simple user and group mappings to queues. (Ram Venkatesh
    via jianhe)

    YARN-2174. Enable HTTPs for the writer REST API of TimelineServer.
    (Zhijie Shen via jianhe)

    YARN-2393. FairScheduler: Add the notion of steady fair share. 
    (Wei Yan via kasha)

    YARN-2395. FairScheduler: Preemption timeout should be configurable per 
    queue. (Wei Yan via kasha)

    YARN-2394. FairScheduler: Configure fairSharePreemptionThreshold per queue.
    (Wei Yan via kasha)

    YARN-415. Capture aggregate memory allocation at the app-level for chargeback.
    (Eric Payne & Andrey Klochkov via jianhe)

    YARN-2440. Enabled Nodemanagers to limit the aggregate cpu usage across all
    containers to a preconfigured limit. (Varun Vasudev via vinodkv)

    YARN-2033. Merging generic-history into the Timeline Store 
    (Zhijie Shen via junping_du)

    YARN-611. Added an API to let apps specify an interval beyond which AM
    failures should be ignored towards counting max-attempts. (Xuan Gong via
    vinodkv)

    YARN-2531. Added a configuration for admins to be able to override app-configs
    and enforce/not-enforce strict control of per-container cpu usage. (Varun
    Vasudev via vinodkv)

    YARN-1250. Generic history service should support application-acls. (Zhijie Shen
    via junping_du)

    YARN-2569. Added the log handling APIs for the long running services. (Xuan
    Gong via zjshen)

    YARN-2102. Added the concept of a Timeline Domain to handle read/write ACLs
    on Timeline service event data. (Zhijie Shen via vinodkv)

    YARN-2581. Passed LogAggregationContext to NM via ContainerTokenIdentifier.
    (Xuan Gong via zjshen)

    YARN-1063. Augmented Hadoop common winutils to have the ability to create
    containers as domain users. (Remus Rusanu via vinodkv)

    YARN-2613. Support retry in NMClient for rolling-upgrades. (Jian He via 
    junping_du)

    YARN-1972. Added a secure container-executor for Windows. (Remus Rusanu via
    vinodkv)

    YARN-2446. Augmented Timeline service APIs to start taking in domains as a
    parameter while posting entities and events. (Zhijie Shen via vinodkv)

    YARN-2468. Enhanced NodeManager to support log handling APIs (YARN-2569) for
    use by long running services. (Xuan Gong via vinodkv)

    YARN-1051. Add a system for creating reservations of cluster capacity.
    (see breakdown below)

    YARN-913. Add a way to register long-lived services in a YARN cluster.
    (stevel)

    YARN-2493. Added user-APIs for using node-labels. (Wangda Tan via vinodkv)

    YARN-2544. Added admin-API objects for using node-labels. (Wangda Tan via
    vinodkv)

    YARN-2494. Added NodeLabels Manager internal API and implementation. (Wangda
    Tan via vinodkv)

    YARN-2501. Enhanced AMRMClient library to support requests against node
    labels. (Wangda Tan via vinodkv)

    YARN-2656. Made RM web services authentication filter support proxy user.
    (Varun Vasudev and Zhijie Shen via zjshen)

    YARN-2496. Enhanced Capacity Scheduler to have basic support for allocating
    resources based on node-labels. (Wangda Tan via vinodkv)

    YARN-2500. Enhaced ResourceManager to support schedulers allocating resources
    based on node-labels. (Wangda Tan via vinodkv)

    YARN-2504. Enhanced RM Admin CLI to support management of node-labels.
    (Wangda Tan via vinodkv)

    YARN-2198. Remove the need to run NodeManager as privileged account for
    Windows Secure Container Executor. (Remus Rusanu via jianhe)

    YARN-2647. Added a queue CLI for getting queue information. (Sunil Govind via
    vinodkv)

    YARN-2632. Document NM Restart feature. (Junping Du and Vinod Kumar
    Vavilapalli via jlowe)

    YARN-2505. Supported get/add/remove/change labels in RM REST API. (Craig Welch
    via zjshen)

    YARN-2811. In Fair Scheduler, reservation fulfillments shouldn't ignore max
    share (Siqi Li via Sandy Ryza)

  IMPROVEMENTS

    YARN-2242. Improve exception information on AM launch crashes. (Li Lu 
    via junping_du)

    YARN-2274. FairScheduler: Add debug information about cluster capacity, 
    availability and reservations. (kasha)

    YARN-2228. Augmented TimelineServer to load pseudo authentication filter when
    authentication = simple. (Zhijie Shen via vinodkv)

    YARN-1341. Recover NMTokens upon nodemanager restart. (Jason Lowe via 
    junping_du)

    YARN-2208. AMRMTokenManager need to have a way to roll over AMRMToken. (xgong)

    YARN-2323. FairShareComparator creates too many Resource objects (Hong Zhiguo
    via Sandy Ryza)

    YARN-2045. Data persisted in NM should be versioned (Junping Du via jlowe)

    YARN-2013. The diagnostics is always the ExitCodeException stack when the container 
    crashes. (Tsuyoshi OZAWA via junping_du)

    YARN-2295. Refactored DistributedShell to use public APIs of protocol records.
    (Li Lu via jianhe)

    YARN-1342. Recover container tokens upon nodemanager restart. (Jason Lowe via
    devaraj)

    YARN-2214. FairScheduler: preemptContainerPreCheck() in FSParentQueue delays 
    convergence towards fairness. (Ashwin Shankar via kasha)

    YARN-2211. Persist AMRMToken master key in RMStateStore for RM recovery.
    (Xuan Gong via jianhe)

    YARN-2328. FairScheduler: Verify update and continuous scheduling threads are
    stopped when the scheduler is stopped. (kasha)

    YARN-2347. Consolidated RMStateVersion and NMDBSchemaVersion into Version in
    yarn-server-common. (Junping Du via zjshen)

    YARN-1994. Expose YARN/MR endpoints on multiple interfaces. (Craig Welch,
    Milan Potocnik, Arpit Agarwal via xgong)

    YARN-2343. Improve NMToken expire exception message. (Li Lu via jianhe)

    YARN-2370. Fix comment in o.a.h.y.server.resourcemanager.schedulerAppSchedulingInfo 
    (Wenwu Peng via junping_du)

    YARN-2298. Move TimelineClient to yarn-common project (Zhijie Shen via 
    junping_du)

    YARN-2288. Made persisted data in LevelDB timeline store be versioned. (Junping Du
    via zjshen)

    YARN-2352. FairScheduler: Collect metrics on duration of critical methods that 
    affect performance. (kasha)

    YARN-2212. ApplicationMaster needs to find a way to update the AMRMToken
    periodically. (xgong)

    YARN-2026. Fair scheduler: Consider only active queues for computing fairshare. 
    (Ashwin Shankar via kasha)

    YARN-1954. Added waitFor to AMRMClient(Async). (Tsuyoshi Ozawa via zjshen)

    YARN-2302. Refactor TimelineWebServices. (Zhijie Shen via junping_du)

    YARN-2337. ResourceManager sets ClientRMService in RMContext multiple times.
    (Zhihai Xu via kasha)

    YARN-2138. Cleaned up notifyDone* APIs in RMStateStore. (Varun Saxena via
    jianhe)

    YARN-2373. Changed WebAppUtils to use Configuration#getPassword for
    accessing SSL passwords. (Larry McCay via jianhe)

    YARN-2317. Updated the document about how to write YARN applications. (Li Lu via
    zjshen)

    YARN-2399. FairScheduler: Merge AppSchedulable and FSSchedulerApp into 
    FSAppAttempt. (kasha)

    YARN-1370. Fair scheduler to re-populate container allocation state. 
    (Anubhav Dhoot via kasha)

    YARN-2197. Add a link to YARN CHANGES.txt in the left side of doc
      (Akira AJISAKA via aw)

    YARN-1918. Typo in description and error message for 
      'yarn.resourcemanager.cluster-id' (Anandha L Ranganathan via aw)

    YARN-2389. Added functionality for schedulers to kill all applications in a
    queue. (Subramaniam Venkatraman Krishnan via jianhe)

    YARN-1326. RM should log using RMStore at startup time. 
    (Tsuyoshi Ozawa via kasha)

    YARN-2182. Updated ContainerId#toString() to append RM Epoch number.
    (Tsuyoshi OZAWA via jianhe)

    YARN-2406. Move RM recovery related proto to
    yarn_server_resourcemanager_recovery.proto. (Tsuyoshi Ozawa via jianhe)

    YARN-1506. Changed RMNode/SchedulerNode to update resource with event
    notification. (Junping Du via jianhe)

    YARN-2509. Enable Cross Origin Filter for timeline server only and not all
    Yarn servers (Mit Desai via jeagles)

    YARN-2511. Allowed all origins by default when CrossOriginFilter is
    enabled. (Jonathan Eagles via zjshen)

    YARN-2508. Cross Origin configuration parameters prefix are not honored
    (Mit Desai via jeagles)

    YARN-2512. Allowed pattern matching for origins in CrossOriginFilter.
    (Jonathan Eagles via zjshen)

    YARN-2507. Documented CrossOriginFilter configurations for the timeline
    server. (Jonathan Eagles via zjshen)

    YARN-2515. Updated ConverterUtils#toContainerId to parse epoch.
    (Tsuyoshi OZAWA via jianhe)

    YARN-2448. Changed ApplicationMasterProtocol to expose RM-recognized resource
    types to the AMs. (Varun Vasudev via vinodkv)

    YARN-2538. Added logs when RM sends roll-overed AMRMToken to AM. (Xuan Gong
    via zjshen)

    YARN-2229. Changed the integer field of ContainerId to be long type.
    (Tsuyoshi OZAWA via jianhe)

    YARN-2547. Cross Origin Filter throws UnsupportedOperationException upon
    destroy (Mit Desai via jeagles)

    YARN-2557. Add a parameter "attempt_Failures_Validity_Interval" into
    DistributedShell. (xgong)

    YARN-2001. Added a time threshold for RM to wait before starting container
    allocations after restart/failover. (Jian He via vinodkv)

    YARN-1372. Ensure all completed containers are reported to the AMs across
    RM restart. (Anubhav Dhoot via jianhe)

    YARN-2539. FairScheduler: Set the default value for maxAMShare to 0.5. 
    (Wei Yan via kasha)

    YARN-1959. Fix headroom calculation in FairScheduler. 
    (Anubhav Dhoot via kasha)

    YARN-2577. Clarify ACL delimiter and how to configure ACL groups only
    (Miklos Christine via aw)

    YARN-2372. There are Chinese Characters in the FairScheduler's document 
    (Fengdong Yu via aw)

    YARN-668. Changed NMTokenIdentifier/AMRMTokenIdentifier/ContainerTokenIdentifier
    to use protobuf object as the payload. (Junping Du via jianhe)

    YARN-1769. CapacityScheduler: Improve reservations (Thomas Graves via
    jlowe)

    YARN-2627. Added the info logs of attemptFailuresValidityInterval and number
    of previous failed attempts. (Xuan Gong via zjshen)

    YARN-2562. Changed ContainerId#toString() to be more readable. (Tsuyoshi
    OZAWA via jianhe)

    YARN-2615. Changed ClientToAMTokenIdentifier/RM(Timeline)DelegationTokenIdentifier
    to use protobuf as payload. (Junping Du via jianhe)

    YARN-2629. Made the distributed shell use the domain-based timeline ACLs.
    (zjshen)

    YARN-2583. Modified AggregatedLogDeletionService to be able to delete rolling
    aggregated logs. (Xuan Gong via zjshen)

    YARN-2312. Deprecated old ContainerId#getId API and updated MapReduce to
    use ContainerId#getContainerId instead. (Tsuyoshi OZAWA via jianhe)

    YARN-2621. Simplify the output when the user doesn't have the access for
    getDomain(s). (Zhijie Shen via jianhe)

    YARN-1879. Marked Idempotent/AtMostOnce annotations to ApplicationMasterProtocol
    for RM fail over. (Tsuyoshi OZAWA via jianhe)

    YARN-2676. Enhanced Timeline auth-filter to support proxy users. (Zhijie Shen
    via vinodkv)

    YARN-2673. Made timeline client put APIs retry if ConnectException happens.
    (Li Lu via zjshen)

    YARN-2582. Fixed Log CLI and Web UI for showing aggregated logs of LRS. (Xuan
    Gong via zjshen)

    YARN-90. NodeManager should identify failed disks becoming good again
    (Varun Vasudev via jlowe)

    YARN-2709. Made timeline client getDelegationToken API retry if ConnectException
    happens. (Li Lu via zjshen)

    YARN-2682. Updated WindowsSecureContainerExecutor to not use
    DefaultContainerExecutor#getFirstApplicationDir and use getWorkingDir()
    instead. (Zhihai Xu via jianhe)

    YARN-2209. Replaced AM resync/shutdown command with corresponding exceptions and
    made related MR changes. (Jian He via zjshen)

    YARN-2703. Added logUploadedTime into LogValue for better display. (Xuan Gong
    via zjshen)

    YARN-2704. Changed ResourceManager to optionally obtain tokens itself for the
    sake of localization and log-aggregation for long-running services. (Jian He
    via vinodkv)

    YARN-2502. Changed DistributedShell to support node labels. (Wangda Tan via
    jianhe)

    YARN-2760. Remove 'experimental' from FairScheduler docs. (Harsh J via kasha)

    YARN-2503. Added node lablels in web UI. (Wangda Tan via jianhe)

    YARN-2779. Fixed ResourceManager to not require delegation tokens for 
    communicating with Timeline Service. (Zhijie Shen via vinodkv)

    YARN-2778. Moved node-lables' reports to the yarn nodes CLI from the admin
    CLI. (Wangda Tan via vinodkv)

    YARN-2770. Added functionality to renew/cancel TimeLineDelegationToken.
    (Zhijie Shen via jianhe)

    YARN-2818. Removed the now unnecessary user entity injection from Timeline
    service given we now have domains. (Zhijie Shen via vinodkv)

    YARN-2635. TestRM, TestRMRestart, TestClientToAMTokens should run 
    with both CS and FS. (Wei Yan and kasha via kasha)


  OPTIMIZATIONS

  BUG FIXES

    YARN-2251. Avoid negative elapsed time in JHS/MRAM web UI and services.
    (Zhijie Shen via junping_du)

    YARN-2088. Fixed a bug in GetApplicationsRequestPBImpl#mergeLocalToBuilder.
    (Binglin Chang via jianhe)

    YARN-2260. Fixed ResourceManager's RMNode to correctly remember containers
    when nodes resync during work-preserving RM restart. (Jian He via vinodkv)

    YARN-2264. Fixed a race condition in DrainDispatcher which may cause random
    test failures. (Li Lu via jianhe)

    YARN-2219. Changed ResourceManager to avoid AMs and NMs getting exceptions
    after RM recovery but before scheduler learns about apps and app-attempts.
    (Jian He via vinodkv)

    YARN-2244. FairScheduler missing handling of containers for unknown 
    application attempts. (Anubhav Dhoot via kasha)

    YARN-2321. NodeManager web UI can incorrectly report Pmem enforcement
    (Leitao Guo via jlowe)

    YARN-2273. NPE in ContinuousScheduling thread when we lose a node. 
    (Wei Yan via kasha)

    YARN-2313. Livelock can occur in FairScheduler when there are lots of
    running apps (Tsuyoshi Ozawa via Sandy Ryza)

    YARN-2147. client lacks delegation token exception details when
    application submit fails (Chen He via jlowe)

    YARN-1796. container-executor shouldn't require o-r permissions (atm)

    YARN-2354. DistributedShell may allocate more containers than client
    specified after AM restarts. (Li Lu via jianhe)

    YARN-2051. Fix bug in PBimpls and add more unit tests with reflection. 
    (Binglin Chang via junping_du)

    YARN-2374. Fixed TestDistributedShell#testDSShell failure due to hostname
    dismatch. (Varun Vasudev via jianhe)

    YARN-2359. Application hangs when it fails to launch AM container. 
    (Zhihai Xu via kasha)

    YARN-2388. Fixed TestTimelineWebServices failure due to HADOOP-10791. (zjshen)

    YARN-2008. Fixed CapacityScheduler to calculate headroom based on max available
    capacity instead of configured max capacity. (Craig Welch via jianhe)

    YARN-2400. Fixed TestAMRestart fails intermittently. (Jian He via xgong)

    YARN-2361. RMAppAttempt state machine entries for KILLED state has duplicate
    event entries. (Zhihai Xu via kasha)

    YARN-2070. Made DistributedShell publish the short user name to the timeline
    server. (Robert Kanter via zjshen)

    YARN-2397. Avoided loading two authentication filters for RM and TS web
    interfaces. (Varun Vasudev via zjshen)

    YARN-2409. RM ActiveToStandBy transition missing stoping previous rmDispatcher.
    (Rohith via jianhe)

    YARN-2249. Avoided AM release requests being lost on work preserving RM
    restart. (Jian He via zjshen)

    YARN-2034. Description for yarn.nodemanager.localizer.cache.target-size-mb
    is incorrect (Chen He via jlowe)

    YARN-1919. Potential NPE in EmbeddedElectorService#stop. 
    (Tsuyoshi Ozawa via kasha)

    YARN-2424. LCE should support non-cgroups, non-secure mode (Chris Douglas 
    via aw)

    YARN-2434. RM should not recover containers from previously failed attempt
    when AM restart is not enabled (Jian He via jlowe)

    YARN-2035. FileSystemApplicationHistoryStore should not make working dir
    when it already exists. (Jonathan Eagles via zjshen)

    YARN-2405. NPE in FairSchedulerAppsBlock. (Tsuyoshi Ozawa via kasha)

    YARN-2449. Fixed the bug that TimelineAuthenticationFilterInitializer
    is not automatically added when hadoop.http.filter.initializers is not
    configured. (Varun Vasudev via zjshen)

    YARN-2450. Fix typos in log messages. (Ray Chiang via hitesh)

    YARN-2447. RM web service app submission doesn't pass secrets correctly.
    (Varun Vasudev via jianhe)

    YARN-2462. TestNodeManagerResync#testBlockNewContainerRequestsOnStartAndResync
    should have a test timeout (Eric Payne via jlowe)

    YARN-2431. NM restart: cgroup is not removed for reacquired containers
    (jlowe)

    YARN-2519. Credential Provider related unit tests failed on Windows.
    (Xiaoyu Yao via cnauroth)

    YARN-2526. SLS can deadlock when all the threads are taken by AMSimulators. 
    (Wei Yan via kasha)

    YARN-1458. FairScheduler: Zero weight can lead to livelock. 
    (Zhihai Xu via kasha)

    YARN-2459. RM crashes if App gets rejected for any reason
    and HA is enabled. (Jian He and Mayank Bansal via xgong)

    YARN-2158. Fixed TestRMWebServicesAppsModification#testSingleAppKill test
    failure. (Varun Vasudev via jianhe)

    YARN-2534. FairScheduler: Potential integer overflow calculating totalMaxShare. 
    (Zhihai Xu via kasha)

    YARN-2541. Fixed ResourceManagerRest.apt.vm table syntax error. (jianhe)

    YARN-2484. FileSystemRMStateStore#readFile/writeFile should close
    FSData(In|Out)putStream in final block (Tsuyoshi OZAWA via jlowe)

    YARN-2456. Possible livelock in CapacityScheduler when RM is recovering apps.
    (Jian He via xgong)

    YARN-2542. Fixed NPE when retrieving ApplicationReport from TimeLineServer.
    (Zhijie Shen via jianhe)

    YARN-2528. Relaxed http response split vulnerability protection for the origins
    header and made it accept multiple origins in CrossOriginFilter. (Jonathan
    Eagles via zjshen)

    YARN-2549. TestContainerLaunch fails due to classpath problem with hamcrest
    classes. (cnauroth)

    YARN-2529. Generic history service RPC interface doesn't work when service
    authorization is enabled. (Zhijie Shen via jianhe)

    YARN-2558. Updated ContainerTokenIdentifier#read/write to use
    ContainerId#getContainerId. (Tsuyoshi OZAWA via jianhe)

    YARN-2559. Fixed NPE in SystemMetricsPublisher when retrieving
    FinalApplicationStatus. (Zhijie Shen via jianhe)

    YARN-1779. Fixed AMRMClient to handle AMRMTokens correctly across
    ResourceManager work-preserving-restart or failover. (Jian He via vinodkv)

    YARN-2363. Submitted applications occasionally lack a tracking URL (jlowe)

    YARN-2561. MR job client cannot reconnect to AM after NM restart. (Junping
    Du via jlowe)

    YARN-2563. Fixed YarnClient to call getTimeLineDelegationToken only if the
    Token is not present. (Zhijie Shen via jianhe)

    YARN-2568. Fixed the potential test failures due to race conditions when RM
    work-preserving recovery is enabled. (Jian He via zjshen)

    YARN-2565. Fixed RM to not use FileSystemApplicationHistoryStore unless
    explicitly set. (Zhijie Shen via jianhe)

    YARN-2460. Remove obsolete entries from yarn-default.xml (Ray Chiang via
    aw)

    YARN-2452. TestRMApplicationHistoryWriter fails with FairScheduler. 
    (Zhihai Xu via kasha)

    YARN-2453. TestProportionalCapacityPreemptionPolicy fails with 
    FairScheduler. (Zhihai Xu via kasha)
    
    YARN-2540. FairScheduler: Queue filters not working on scheduler page in 
    RM UI. (Ashwin Shankar via kasha)

    YARN-2584. TestContainerManagerSecurity fails on trunk. (Jian He via 
    junping_du)

    YARN-2252. Intermittent failure of 
    TestFairScheduler.testContinuousScheduling. 
    (Ratandeep Ratti and kasha via kasha)

    YARN-2161. Fix build on macosx: YARN parts (Binglin Chang via aw)

    YARN-2596. TestWorkPreservingRMRestart fails with FairScheduler. (kasha)

    YARN-2546. Made REST API for application creation/submission use numeric and
    boolean types instead of the string of them. (Varun Vasudev via zjshen)

    YARN-2523. ResourceManager UI showing negative value for "Decommissioned
    Nodes" field (Rohith via jlowe)

    YARN-2608. FairScheduler: Potential deadlocks in loading alloc files and 
    clock access. (Wei Yan via kasha)

    YARN-2606. Application History Server tries to access hdfs before doing
    secure login (Mit Desai via jeagles)

    YARN-2610. Hamlet should close table tags. (Ray Chiang via kasha)

    YARN-2387. Resource Manager crashes with NPE due to lack of
    synchronization (Mit Desai via jlowe)

    YARN-2594. Potential deadlock in RM when querying 
    ApplicationResourceUsageReport. (Wangda Tan via kasha)

    YARN-2602. Fixed possible NPE in ApplicationHistoryManagerOnTimelineStore.
    (Zhijie Shen via jianhe)

    YARN-2630. Prevented previous AM container status from being acquired by the
    current restarted AM. (Jian He via zjshen) 

    YARN-2617. Fixed NM to not send duplicate container status whose app is not
    running. (Jun Gong via jianhe)

    YARN-2624. Resource Localization fails on a cluster due to existing cache
    directories (Anubhav Dhoot via jlowe)

    YARN-2527. Fixed the potential NPE in ApplicationACLsManager and added test
    cases for it. (Benoy Antony via zjshen)

    YARN-2628. Capacity scheduler with DominantResourceCalculator carries out
    reservation even though slots are free. (Varun Vasudev via jianhe)

    YARN-2685. Fixed a bug in CommonNodeLabelsManager that caused wrong resource
    tracking per label when a host runs multiple node-managers. (Wangda Tan via
    vinodkv)

    YARN-2699. Fixed a bug in CommonNodeLabelsManager that caused tests to fail
    when using ephemeral ports on NodeIDs. (Wangda Tan via vinodkv)

    YARN-2705. Fixed bugs in ResourceManager node-label manager that were causing
    test-failures: added a dummy in-memory labels-manager. (Wangda Tan via
    vinodkv)

    YARN-2715. Fixed ResourceManager to respect common configurations for proxy
    users/groups beyond just the YARN level config. (Zhijie Shen via vinodkv)

    YARN-2743. Fixed a bug in ResourceManager that was causing RMDelegationToken
    identifiers to be tampered and thus causing app submission failures in
    secure mode. (Jian He via vinodkv)

  BREAKDOWN OF YARN-1051 SUBTASKS AND RELATED JIRAS

    YARN-1707. Introduce APIs to add/remove/resize queues in the
    CapacityScheduler. (Carlo Curino and Subru Krishnan via curino)

    YARN-2475. Logic for responding to capacity drops for the
    ReservationSystem. (Carlo Curino and Subru Krishnan via curino)

    YARN-1708. Public YARN APIs for creating/updating/deleting
    reservations. (Subru Krishnan and Carlo Curino via subru)

    YARN-1709. In-memory data structures used to track resources over
    time to enable reservations. (Subru Krishnan and Carlo Curino via
    subru)

    YARN-1710. Logic to find allocations within a Plan that satisfy
    user ReservationRequest(s). (Carlo Curino and Subru Krishnan via
    curino)

    YARN-1711. Policy to enforce instantaneous and over-time quotas
    on user reservations. (Carlo Curino and Subru Krishnan via curino)

    YARN-1712. Plan follower that synchronizes the current state of reservation
    subsystem with the scheduler. (Subru Krishnan and Carlo Curino via subru)

    YARN-2080. Integrating reservation system with ResourceManager and
    client-RM protocol. (Subru Krishnan and Carlo Curino via subru)

    MAPREDUCE-6103. Adding reservation APIs to MR resource manager
    delegate. (Subru Krishnan and Carlo Curino via subru)

    YARN-2576. Fixing compilation, javadocs and audit issues to pass
    test patch in branch. (Subru Krishnan and Carlo Curino via subru)

    YARN-2611. Fixing jenkins findbugs warning and TestRMWebServicesCapacitySched
    for branch YARN-1051. (Subru Krishnan and Carlo Curino via subru)

    YARN-2644. Fixed CapacityScheduler to return up-to-date headroom when
    AM allocates. (Craig Welch via jianhe)

    YARN-1857. CapacityScheduler headroom doesn't account for other AM's running.
    (Chen He and Craig Welch via jianhe)

    YARN-2649. Fixed TestAMRMRPCNodeUpdates test failure. (Ming Ma via jianhe)

    YARN-2662. TestCgroupsLCEResourcesHandler leaks file descriptors. (cnauroth)

  BREAKDOWN OF YARN-913 SUBTASKS AND RELATED JIRAS

    YARN-2652 Add hadoop-yarn-registry package under hadoop-yarn. (stevel)

    YARN-2668 yarn-registry JAR won't link against ZK 3.4.5. (stevel)
    
    YARN-2689 TestSecureRMRegistryOperations failing on windows:
    secure ZK won't start (stevel)

    YARN-2692 ktutil test hanging on some machines/ktutil versions (stevel)

    YARN-2700 TestSecureRMRegistryOperations failing on windows: auth problems
    (stevel)

    YARN-2677 registry punycoding of usernames doesn't fix all usernames to be
    DNS-valid (stevel)

    YARN-2768 Improved Yarn Registry service record structure (stevel)

    ---

    YARN-2598 GHS should show N/A instead of null for the inaccessible information
    (Zhijie Shen via mayank)

    YARN-2671. Fixed ApplicationSubmissionContext to still set resource for
    backward compatibility. (Wangda Tan via zjshen)

    YARN-2667. Fix the release audit warning caused by hadoop-yarn-registry
    (Yi Liu via jlowe)

    YARN-2651. Spun off LogRollingInterval from LogAggregationContext. (Xuan Gong
    via zjshen)

    YARN-2377. Localization exception stack traces are not passed as
    diagnostic info (Gera Shegalov via jlowe)

    YARN-2308. Changed CapacityScheduler to explicitly throw exception if the
    queue to which the apps were submitted is changed across RM restart.
    (Craig Welch & Chang Li via jianhe)

    YARN-2566. DefaultContainerExecutor should pick a working directory randomly. 
    (Zhihai Xu via kasha)

    YARN-2588. Standby RM fails to transitionToActive if previous
    transitionToActive failed with ZK exception. (Rohith Sharmaks via jianhe)

    YARN-2701. Potential race condition in startLocalizer when using
    LinuxContainerExecutor. (Xuan Gong via jianhe)

    YARN-2717. Avoided duplicate logging when container logs are not found. (Xuan
    Gong via zjshen)

    YARN-2720. Windows: Wildcard classpath variables not expanded against
    resources contained in archives. (Craig Welch via cnauroth)

    YARN-2721. Suppress NodeExist exception thrown by ZKRMStateStore when it
    retries creating znode. (Jian He via zjshen)

    YARN-2732. Fixed syntax error in SecureContainer.apt.vm. (Jian He via zjshen)

    YARN-2724. Skipped uploading a local log file to HDFS if exception is raised
    when opening it. (Xuan Gong via zjshen)

    YARN-1915. Fixed a race condition that client could use the ClientToAMToken
    to contact with AM before AM actually receives the ClientToAMTokenMasterKey.
    (Jason Lowe via jianhe)

    YARN-2314. Disable ContainerManagementProtocolProxy cache by default to
    prevent creating thousands of threads in a large cluster. (Jason Lowe via
    jianhe)

    YARN-2723. Fix rmadmin -replaceLabelsOnNode does not correctly parse port.
    (Naganarasimha G R via xgong)

    YARN-2734. Skipped sub-folders in the local log dir when aggregating logs.
    (Xuan Gong via zjshen)

    YARN-2726. CapacityScheduler should explicitly log when an accessible
    label has no capacity. (Wangda Tan via xgong)

    YARN-2591. Fixed AHSWebServices to return FORBIDDEN(403) if the request user
    doesn't have access to the history data. (Zhijie Shen via jianhe)

    YARN-2279. Add UTs to cover timeline server authentication.
    (Zhijie Shen via xgong)

    YARN-2758. Update TestApplicationHistoryClientService to use the new generic
    history store. (Zhijie Shen via xgong)

    YARN-2741. Made NM web UI serve logs on the drive other than C: on Windows. (Craig
    Welch via zjshen)

    YARN-2747. Fixed the test failure of TestAggregatedLogFormat when native I/O is
    enabled. (Xuan Gong via zjshen)

    YARN-2769. Fixed the problem that timeline domain is not set in distributed shell
    AM when using shell_command on Windows. (Varun Vasudev via zjshen)

    YARN-2755. NM fails to clean up usercache_DEL_<timestamp> dirs after
    YARN-661 (Siqi Li via jlowe)

    YARN-2698. Moved some node label APIs to be correctly placed in client
    protocol. (Wangda Tan via vinodkv)

    YARN-2789. Re-instated the NodeReport.newInstance private unstable API
    modified in YARN-2698 so that tests in YARN frameworks don't break. (Wangda
    Tan via vinodkv)

    YARN-2707. Potential null dereference in FSDownload (Gera Shegalov via
    jlowe)

    YARN-2711. Fixed TestDefaultContainerExecutor#testContainerLaunchError failure on
    Windows. (Varun Vasudev via zjshen)

    YARN-2790. Fixed a NodeManager bug that was causing log-aggregation to fail
    beyond HFDS delegation-token expiry even when RM is a proxy-user (YARN-2704).
    (Jian He via vinodkv)

    YARN-2785. Fixed intermittent TestContainerResourceUsage failure. (Varun Vasudev
    via zjshen)

    YARN-2730. DefaultContainerExecutor runs only one localizer at a time
    (Siqi Li via jlowe)

    YARN-2798. Fixed YarnClient to populate the renewer correctly for Timeline
    delegation tokens. (Zhijie Shen via vinodkv)

    YARN-2788. Fixed backwards compatiblity issues with log-aggregation feature
    that were caused when adding log-upload-time via YARN-2703. (Xuan Gong via
    vinodkv)

    YARN-2795. Fixed ResourceManager to not crash loading node-label data from
    HDFS in secure mode. (Wangda Tan via vinodkv)

    YARN-1922. Fixed NodeManager to kill process-trees correctly in the presence 
    of races between the launch and the stop-container call and when root
    processes crash. (Billie Rinaldi via vinodkv)

    YARN-2752. Made ContainerExecutor append "nice -n" arg only when priority
    adjustment flag is set. (Xuan Gong via zjshen)

    YARN-2010. Handle app-recovery failures gracefully. 
    (Jian He and Karthik Kambatla via kasha)

    YARN-2804. Fixed Timeline service to not fill the logs with JAXB bindings
    exceptions. (Zhijie Shen via vinodkv)

    YARN-2767. Added a test case to verify that http static user cannot kill or submit
    apps in the secure mode. (Varun Vasudev via zjshen)

    YARN-2805. Fixed ResourceManager to load HA configs correctly before kerberos
    login. (Wangda Tan via vinodkv)

    YARN-2579. Fixed a deadlock issue when EmbeddedElectorService and
    FatalEventDispatcher try to transition RM to StandBy at the same time.
    (Rohith Sharmaks via jianhe)

    YARN-2813. Fixed NPE from MemoryTimelineStore.getDomains. (Zhijie Shen via xgong)

    YARN-2812. TestApplicationHistoryServer is likely to fail on less powerful machine.
    (Zhijie Shen via xgong)

    YARN-2744. Fixed CapacityScheduler to validate node-labels correctly against
    queues. (Wangda Tan via vinodkv)

    YARN-2823. Fixed ResourceManager app-attempt state machine to inform
    schedulers about previous finished attempts of a running appliation to avoid
    expectation mismatch w.r.t transferred containers. (Jian He via vinodkv)

    YARN-2810. TestRMProxyUsersConf fails on Windows VMs. (Varun Vasudev via xgong)

    YARN-2824. Fixed Capacity Scheduler to not crash when some node-labels are
    not mapped to queues by making default capacities per label to be zero.
    (Wangda Tan via vinodkv)

    YARN-2827. Fixed bugs in "yarn queue" CLI. (Wangda Tan via vinodkv).

    YARN-2803. MR distributed cache not working correctly on Windows after
    NodeManager privileged account changes. (Craig Welch via cnauroth)

    YARN-2753. Fixed a bunch of bugs in the NodeLabelsManager classes. (Zhihai xu
    via vinodkv)

    YARN-2825. Container leak on NM (Jian He via jlowe)

    YARN-2819. NPE in ATS Timeline Domains when upgrading from 2.4 to 2.6.
    (Zhijie Shen via xgong)

    YARN-2826. Fixed user-groups mappings' refresh bug caused by YARN-2826.
    (Wangda Tan via vinodkv)

    YARN-2607. Fixed issues in TestDistributedShell. (Wangda Tan via vinodkv)

    YARN-2830. Add backwords compatible ContainerId.newInstance constructor.
    (jeagles via acmurthy) 

    YARN-2834. Fixed ResourceManager to ignore token-renewal failures on recovery
    consistent with the (somewhat incorrect) behaviour in the non-recovery case.
    (Jian He via vinodkv)

    YARN-2841. RMProxy should retry EOFException. (Jian He via xgong)

    YARN-2843. Fixed NodeLabelsManager to trim inputs for hosts and labels so
    as to make them work correctly. (Wangda Tan via vinodkv)

    YARN-2794. Fixed log messages about distributing system-credentials. (Jian He via
    zjshen)

    YARN-2846. Incorrect persist exit code for running containers in
    reacquireContainer() that interrupted by NodeManager restart. (Junping Du
    via jlowe)

    YARN-2853. Fixed a bug in ResourceManager causing apps to hang when the user
    kill request races with ApplicationMaster finish. (Jian He via vinodkv)

Release 2.5.2 - 2014-11-19

  INCOMPATIBLE CHANGES

  NEW FEATURES

  IMPROVEMENTS

  OPTIMIZATIONS

  BUG FIXES


Release 2.5.1 - 2014-09-05

  INCOMPATIBLE CHANGES

  NEW FEATURES

  IMPROVEMENTS

  OPTIMIZATIONS

  BUG FIXES

Release 2.5.0 - 2014-08-11

  INCOMPATIBLE CHANGES

  NEW FEATURES

    YARN-1757. NM Recovery. Auxiliary service support. (Jason Lowe via kasha)

    YARN-1864. Fair Scheduler Dynamic Hierarchical User Queues (Ashwin Shankar
    via Sandy Ryza)

    YARN-1362. Distinguish between nodemanager shutdown for decommission vs shutdown 
    for restart. (Jason Lowe via junping_du)

    YARN-1338. Recover localized resource cache state upon nodemanager restart 
    (Jason Lowe via junping_du)

    YARN-1368. Added core functionality of recovering container state into
    schedulers after ResourceManager Restart so as to preserve running work in
    the cluster. (Jian He via vinodkv)

    YARN-1702. Added kill app functionality to RM web services. (Varun Vasudev
    via vinodkv)

    YARN-1339. Recover DeletionService state upon nodemanager restart. (Jason Lowe
    via junping_du)

    YARN-1365. Changed ApplicationMasterService to allow an app to re-register
    after RM restart. (Anubhav Dhoot via jianhe)

    YARN-2052. Embedded an epoch number in container id to ensure the uniqueness
    of container id after RM restarts. (Tsuyoshi OZAWA via jianhe)

    YARN-1713. Added get-new-app and submit-app functionality to RM web services.
    (Varun Vasudev via vinodkv)

    YARN-2233. Implemented ResourceManager web-services to create, renew and
    cancel delegation tokens. (Varun Vasudev via vinodkv)

    YARN-2247. Made RM web services authenticate users via kerberos and delegation
    token. (Varun Vasudev via zjshen)

  IMPROVEMENTS

    YARN-1479. Invalid NaN values in Hadoop REST API JSON response (Chen He via
    jeagles)

    YARN-1736. FS: AppSchedulable.assignContainer's priority argument is 
    redundant. (Naren Koneru via kasha)

    YARN-1678. Fair scheduler gabs incessantly about reservations (Sandy Ryza)

    YARN-1561. Fix a generic type warning in FairScheduler. (Chen He via junping_du)

    YARN-1429. *nix: Allow a way for users to augment classpath of YARN daemons.
    (Jarek Jarcec Cecho via kasha)

    YARN-1520. update capacity scheduler docs to include necessary parameters
    (Chen He via jeagles)

    YARN-1845. Elapsed time for failed tasks that never started is wrong
    (Rushabh S Shah via jeagles)

    YARN-1136. Replace junit.framework.Assert with org.junit.Assert (Chen He
    via jeagles)

    YARN-1889. In Fair Scheduler, avoid creating objects on each call to
    AppSchedulable comparator (Hong Zhiguo via Sandy Ryza)

    YARN-1923. Make Fair Scheduler resource ratio calculations terminate faster
    (Anubhav Dhoot via Sandy Ryza)

    YARN-1870. FileInputStream is not closed in ProcfsBasedProcessTree#constructProcessSMAPInfo. 
    (Fengdong Yu via junping_du)

    YARN-1970. Prepare YARN codebase for JUnit 4.11. (cnauroth)

    YARN-483. Improve documentation on log aggregation in yarn-default.xml
    (Akira Ajisaka via Sandy Ryza)

    YARN-2036. Document yarn.resourcemanager.hostname in ClusterSetup (Ray
    Chiang via Sandy Ryza)

    YARN-766. TestNodeManagerShutdown in branch-2 should use Shell to form the output path and a format 
    issue in trunk. (Contributed by Siddharth Seth)

    YARN-1982. Renamed the daemon name to be TimelineServer instead of History
    Server and deprecated the old usage. (Zhijie Shen via vinodkv)

    YARN-1987. Wrapper for leveldb DBIterator to aid in handling database exceptions.
    (Jason Lowe via kasha)

    YARN-1751. Improve MiniYarnCluster for log aggregation testing (Ming Ma
    via jlowe)

    YARN-1981. Nodemanager version is not updated when a node reconnects (Jason
    Lowe via jeagles)

    YARN-1938. Added kerberos login for the Timeline Server. (Zhijie Shen via
    vinodkv)

    YARN-2017. Merged some of the common scheduler code. (Jian He via vinodkv)

    YARN-2049. Added delegation-token support for the Timeline Server. (Zhijie
    Shen via vinodkv)

    YARN-1936. Added security support for the Timeline Client. (Zhijie Shen via
    vinodkv)

    YARN-1937. Added owner-only ACLs support for Timeline Client and server.
    (Zhijie Shen via vinodkv)

    YARN-2012. Fair Scheduler: allow default queue placement rule to take an
    arbitrary queue (Ashwin Shankar via Sandy Ryza)

    YARN-2059. Added admin ACLs support to Timeline Server. (Zhijie Shen via
    vinodkv)
    
    YARN-2073. Fair Scheduler: Add a utilization threshold to prevent preempting
    resources when cluster is free (Karthik Kambatla via Sandy Ryza)

    YARN-2071. Modified levelDB store permissions to be readable only by the
    server user. (Zhijie Shen via vinodkv)

    YARN-2107. Refactored timeline classes into o.a.h.y.s.timeline package. (Vinod
    Kumar Vavilapalli via zjshen)

    YARN-596. Use scheduling policies throughout the queue hierarchy to decide
    which containers to preempt (Wei Yan via Sandy Ryza)

    YARN-2054. Better defaults for YARN ZK configs for retries and retry-inteval 
    when HA is enabled. (kasha)

    YARN-1877. Document yarn.resourcemanager.zk-auth and its scope. 
    (Robert Kanter via kasha)

    YARN-2115. Replaced RegisterNodeManagerRequest's ContainerStatus with a new
    NMContainerStatus which has more information that is needed for
    work-preserving RM-restart. (Jian He via vinodkv)

    YARN-1474. Make sechedulers services. (Tsuyoshi Ozawa via kasha)

    YARN-1913. With Fair Scheduler, cluster can logjam when all resources are
    consumed by AMs (Wei Yan via Sandy Ryza)

    YARN-2061. Revisit logging levels in ZKRMStateStore. (Ray Chiang via kasha)

    YARN-1977. Add tests on getApplicationRequest with filtering start time range. (junping_du)

    YARN-2122. In AllocationFileLoaderService, the reloadThread should be created
    in init() and started in start(). (Robert Kanter via kasha)

    YARN-2132. ZKRMStateStore.ZKAction#runWithRetries doesn't log the exception
    it encounters. (Vamsee Yarlagadda via kasha)

    YARN-2030. Augmented RMStateStore with state machine.(Binglin Chang via jianhe)

    YARN-1424. RMAppAttemptImpl should return the 
    DummyApplicationResourceUsageReport for all invalid accesses. 
    (Ray Chiang via kasha)

    YARN-2091. Add more values to ContainerExitStatus and pass it from NM to
    RM and then to app masters (Tsuyoshi OZAWA via bikas)

    YARN-2125. Changed ProportionalCapacityPreemptionPolicy to log CSV in debug
    level. (Wangda Tan via jianhe)

    YARN-2159. Better logging in SchedulerNode#allocateContainer.
    (Ray Chiang via kasha)

    YARN-2191. Added a new test to ensure NM will clean up completed applications
    in the case of RM restart. (Wangda Tan via jianhe)

    YARN-2195. Clean a piece of code in ResourceRequest. (Wei Yan via devaraj)

    YARN-2074. Changed ResourceManager to not count AM preemptions towards app
    failures. (Jian He via vinodkv)

    YARN-2192. TestRMHA fails when run with a mix of Schedulers.
    (Anubhav Dhoot via kasha)

    YARN-2109. Fix TestRM to work with both schedulers. (Anubhav Dhoot via kasha)

    YARN-2072. RM/NM UIs and webservices are missing vcore information.
    (Nathan Roberts via tgraves)

    YARN-2152. Added missing information into ContainerTokenIdentifier so that
    NodeManagers can report the same to RM when RM restarts. (Jian He via vinodkv)

    YARN-2171. Improved CapacityScheduling to not lock on nodemanager-count when
    AMs heartbeat in. (Jason Lowe via vinodkv)

    YARN-614. Changed ResourceManager to not count disk failure, node loss and
    RM restart towards app failures. (Xuan Gong via jianhe)

    YARN-2224. Explicitly enable vmem check in
    TestContainersMonitor#testContainerKillOnMemoryOverflow.
    (Anubhav Dhoot via kasha)

    YARN-2022. Preempting an Application Master container can be kept as least priority
    when multiple applications are marked for preemption by 
    ProportionalCapacityPreemptionPolicy (Sunil G via mayank)

    YARN-2241. ZKRMStateStore: On startup, show nicer messages if znodes already 
    exist. (Robert Kanter via kasha)

	YARN-1408 Preemption caused Invalid State Event: ACQUIRED at KILLED and caused 
	a task timeout for 30mins. (Sunil G via mayank)

    YARN-2300. Improved the documentation of the sample requests for RM REST API -
    submitting an app. (Varun Vasudev via zjshen)

  OPTIMIZATIONS

  BUG FIXES 

    YARN-1718. Fix a couple isTerminals in Fair Scheduler queue placement rules
    (Sandy Ryza)

    YARN-1790. Fair Scheduler UI not showing apps table (bc Wong via Sandy Ryza)

    YARN-1784. TestContainerAllocation assumes CapacityScheduler.
    (Robert Kanter via kasha)

    YARN-1940. deleteAsUser() terminates early without deleting more files on
    error (Rushabh S Shah via jlowe)

    YARN-1865. ShellScriptBuilder does not check for some error conditions.
    (Remus Rusanu via ivanmi)

    YARN-738. TestClientRMTokens is failing irregularly while running all yarn
    tests (Ming Ma via jlowe)

    YARN-2018. TestClientRMService.testTokenRenewalWrongUser fails after
    HADOOP-10562 (Ming Ma via Arpit Agarwal)

    YARN-2011. Fix typo and warning in TestLeafQueue (Chen He via junping_du)


    YARN-2042. String shouldn't be compared using == in
    QueuePlacementRule#NestedUserQueue#getQueueForApp (Chen He via Sandy Ryza)

    YARN-2050. Fix LogCLIHelpers to create the correct FileContext (Ming Ma
    via jlowe)

    YARN-2089. FairScheduler: QueuePlacementPolicy and QueuePlacementRule 
    are missing audience annotations. (Zhihai Xu via kasha)

    YARN-2096. Race in TestRMRestart#testQueueMetricsOnRMRestart.
    (Anubhav Dhoot via kasha)

    YARN-2105. Fix TestFairScheduler after YARN-2012. (Ashwin Shankar via
    Sandy Ryza)

    YARN-2112. Fixed yarn-common's pom.xml to include jackson dependencies so
    that both Timeline Server and client can access them. (Zhijie Shen via
    vinodkv)

    YARN-1868. YARN status web ui does not show correctly in IE 11.
    (Chuan Liu via cnauroth)

    YARN-2103. Inconsistency between viaProto flag and initial value of 
    SerializedExceptionProto.Builder (Binglin Chang via junping_du)

    YARN-1550. NPE in FairSchedulerAppsBlock#render. (Anubhav Dhoot via kasha)

    YARN-2119. DEFAULT_PROXY_ADDRESS should use DEFAULT_PROXY_PORT.
    (Anubhav Dhoot via kasha)

    YARN-2118. Fixed the type mismatch in Map#containsKey check of
    TimelineWebServices#injectOwnerInfo. (Ted Yu via zjshen)

    YARN-2117. Fixed the issue that secret file reader is potentially not
    closed in TimelineAuthenticationFilterInitializer. (Chen He via zjshen)

    YARN-2121. Fixed NPE handling in Timeline Server's TimelineAuthenticator.
    (Zhijie Shen via vinodkv)

    YARN-2128. FairScheduler: Incorrect calculation of amResource usage.
    (Wei Yan via kasha)

    YARN-2124. Fixed NPE in ProportionalCapacityPreemptionPolicy. (Wangda Tan
    via jianhe)

    YARN-2148. TestNMClient failed due more exit code values added and passed
    to AM (Wangda Tan via bikas)

    YARN-2075. Fixed the test failure of TestRMAdminCLI. (Kenji Kikushima via
    zjshen)

    YARN-2155. FairScheduler: Incorrect threshold check for preemption.
    (Wei Yan via kasha)

    YARN-1885. Fixed a bug that RM may not send application-clean-up signal
    to NMs where the completed applications previously ran in case of RM restart.
    (Wangda Tan via jianhe)

    YARN-2167. LeveldbIterator should get closed in
    NMLeveldbStateStoreService#loadLocalizationState() within finally block
    (Junping Du via jlowe)

    YARN-2187. FairScheduler: Disable max-AM-share check by default.
    (Robert Kanter via kasha)

    YARN-2111. In FairScheduler.attemptScheduling, we don't count containers
    as assigned if they have 0 memory but non-zero cores (Sandy Ryza)

    YARN-2204. TestAMRestart#testAMRestartWithExistingContainers assumes
    CapacityScheduler. (Robert Kanter via kasha)

    YARN-2163. WebUI: Order of AppId in apps table should be consistent with
    ApplicationId.compareTo(). (Wangda Tan via raviprak)

    YARN-2104. Scheduler queue filter failed to work because index of queue
    column changed. (Wangda Tan via jlowe)

    YARN-2201. Made TestRMWebServicesAppsModification be independent of the
    changes on yarn-default.xml. (Varun Vasudev via zjshen)

    YARN-2216 TestRMApplicationHistoryWriter sometimes fails in trunk.
    (Zhijie Shen via xgong)

    YARN-2216 YARN-2065 AM cannot create new containers after restart
    (Jian He via stevel)

    YARN-2232. Fixed ResourceManager to allow DelegationToken owners to be able
    to cancel their own tokens in secure mode. (Varun Vasudev via vinodkv)

    YARN-2250. FairScheduler.findLowestCommonAncestorQueue returns null when
    queues not identical (Krisztian Horvath via Sandy Ryza)

    YARN-2158. Improved assertion messages of TestRMWebServicesAppsModification.
    (Varun Vasudev via zjshen)

    YARN-2269. Remove External links from YARN UI. (Craig Welch via xgong)

    YARN-2270. Made TestFSDownload#testDownloadPublicWithStatCache be skipped
    when thereâ€™s no ancestor permissions. (Akira Ajisaka via zjshen)

    YARN-2319. Made the MiniKdc instance start/close before/after the class of
    TestRMWebServicesDelegationTokens. (Wenwu Peng via zjshen)

    YARN-2335. Annotate all hadoop-sls APIs as @Private. (Wei Yan via kasha)

    YARN-1726. ResourceSchedulerWrapper broken due to AbstractYarnScheduler.
    (Wei Yan via kasha)

    YARN-2216. TestRMApplicationHistoryWriter sometimes fails in trunk.
    (Zhijie Shen via xgong)

Release 2.4.1 - 2014-06-23 

  INCOMPATIBLE CHANGES

  NEW FEATURES

  IMPROVEMENTS

    YARN-1892. Improved some logs in the scheduler. (Jian He via zjshen)

    YARN-1696. Added documentation for ResourceManager fail-over. (Karthik
    Kambatla, Masatake Iwasaki, Tsuyoshi OZAWA via vinodkv)

    YARN-1701. Improved default paths of the timeline store and the generic
    history store. (Tsuyoshi Ozawa via zjshen)

    YARN-1962. Changed Timeline Service client configuration to be off by default
    given the non-readiness of the feature yet. (Mohammad Kamrul Islam via
    vinodkv)

  OPTIMIZATIONS

  BUG FIXES

    YARN-1898. Made Standby RM links conf, stacks, logLevel, metrics, jmx, logs
    and static not be redirected to Active RM. (Xuan Gong via zjshen)

    YARN-1837. Fixed TestMoveApplication#testMoveRejectedByScheduler failure.
    (Hong Zhiguo via jianhe)

    YARN-1905. TestProcfsBasedProcessTree must only run on Linux. (cnauroth)

    YARN-1883. TestRMAdminService fails due to inconsistent entries in
    UserGroups (Mit Desai via jeagles)

    YARN-1908. Fixed DistributedShell to not fail in secure clusters. (Vinod
    Kumar Vavilapalli and Jian He via vinodkv)

    YARN-1910. Fixed a race condition in TestAMRMTokens that causes the test to
    fail more often on Windows. (Xuan Gong via vinodkv)

    YARN-1920. Fixed TestFileSystemApplicationHistoryStore failure on windows.
    (Vinod Kumar Vavilapalli via zjshen)

    YARN-1914. Fixed resource-download on NodeManagers to skip permission
    verification of public cache files in Windows+local file-system environment.
    (Varun Vasudev via vinodkv)

    YARN-1903. Set exit code and diagnostics when container is killed at
    NEW/LOCALIZING state. (Zhijie Shen via jianhe)

    YARN-1924. Made ZKRMStateStore updateApplication(Attempt)StateInternal work
    when Application(Attempt) state hasn't been stored before. (Jian He via
    zjshen)

    YARN-1926. Changed DistributedShell to use appIDs as unique identifiers for
    HDFS paths and thus avoid test failures on Windows. (Varun Vasudev via
    vinodkv)

    YARN-1833. TestRMAdminService Fails in trunk and branch-2 (Mit Desai via
    jeagles)

    YARN-1907. TestRMApplicationHistoryWriter#testRMWritingMassiveHistory 
    intermittently fails. (Mit Desai via kihwal)

    YARN-1933. Fixed test issues with TestAMRestart and TestNodeHealthService.
    (Jian He via vinodkv)

    YARN-1928. Fixed a race condition in TestAMRMRPCNodeUpdates which caused it
    to fail occassionally. (Zhijie Shen via vinodkv)

    YARN-1934. Fixed a potential NPE in ZKRMStateStore caused by handling
    Disconnected event from ZK. (Karthik Kambatla via jianhe)

    YARN-1931. Private API change in YARN-1824 in 2.4 broke compatibility 
    with previous releases (Sandy Ryza via tgraves)
    
    YARN-1750. TestNodeStatusUpdater#testNMRegistration is incorrect in test 
    case. (Wangda Tan via junping_du)

    YARN-1947. TestRMDelegationTokens#testRMDTMasterKeyStateOnRollingMasterKey 
    is failing intermittently. (Jian He via junping_du)

    YARN-1281. Fixed TestZKRMStateStoreZKClientConnections to not fail
    intermittently due to ZK-client timeouts. (Tsuyoshi Ozawa via vinodkv) 

    YARN-1932. Javascript injection on the job status page (Mit Desai via
    jlowe)

    YARN-1975. Used resources shows escaped html in CapacityScheduler and
    FairScheduler page (Mit Desai via jlowe)

    YARN-1929. Fixed a deadlock in ResourceManager that occurs when failover
    happens right at the time of shutdown. (Karthik Kambatla via vinodkv)

    YARN-1201. TestAMAuthorization fails with local hostname cannot be resolved. 
    (Wangda Tan via junping_du)

    YARN-1861. Fixed a bug in RM to reset leader-election on fencing that was
    causing both RMs to be stuck in standby mode when automatic failover is
    enabled. (Karthik Kambatla and Xuan Gong via vinodkv)

    YARN-1957. Consider the max capacity of the queue when computing the ideal
    capacity for preemption. (Carlo Curino via cdouglas)

    YARN-1986. In Fifo Scheduler, node heartbeat in between creating app and
    attempt causes NPE (Hong Zhiguo via Sandy Ryza)

    YARN-1976. Fix yarn application CLI to print the scheme of the tracking url
    of failed/killed applications. (Junping Du via jianhe)

    YARN-2016. Fix a bug in GetApplicationsRequestPBImpl to add the missed fields
    to proto. (Junping Du via jianhe)

    YARN-2053. Fixed a bug in AMS to not add null NMToken into NMTokens list from
    previous attempts for work-preserving AM restart. (Wangda Tan via jianhe)

    YARN-2066. Wrong field is referenced in GetApplicationsRequestPBImpl#mergeLocalToBuilder()
    (Hong Zhiguo via junping_du)

    YARN-2081. Fixed TestDistributedShell failure after YARN-1962. (Zhiguo Hong
    via zjshen)

Release 2.4.0 - 2014-04-07 

  INCOMPATIBLE CHANGES

  NEW FEATURES

    YARN-930. Bootstrapping ApplicationHistoryService module. (vinodkv)
  
    YARN-947. Implementing the data objects to be used by the History reader
    and writer interfaces. (Zhijie Shen via vinodkv)
  
    YARN-934. Defined a Writer Interface for HistoryStorage. (Zhijie Shen via
    vinodkv)
  
    YARN-925. Defined a Reader Interface for HistoryStorage. (Mayank Bansal via
    vinodkv)
  
    YARN-978. Created ApplicationAttemptReport. (Mayank Bansal via vinodkv)
  
    YARN-956. Added a testable in-memory HistoryStorage. (Mayank Bansal via
    vinodkv)
  
    YARN-975. Added a file-system implementation for HistoryStorage. (Zhijie Shen
    via vinodkv)
  
    YARN-1123. Added a new ContainerReport and its Protobuf implementation. (Mayank
    Bansal via vinodkv)
  
    YARN-979. Added more APIs for getting information about ApplicationAttempts
    and Containers from ApplicationHistoryProtocol. (Mayank Bansal and Zhijie Shen
    via vinodkv)
  
    YARN-953. Changed ResourceManager to start writing history data. (Zhijie Shen
    via vinodkv)
  
    YARN-1266. Implemented PB service and client wrappers for
    ApplicationHistoryProtocol. (Mayank Bansal via vinodkv)
  
    YARN-955. Implemented ApplicationHistoryProtocol handler. (Mayank Bansal via
    vinodkv)
  
    YARN-1242. Changed yarn scripts to be able to start ApplicationHistoryServer
    as an individual process. (Mayank Bansal via vinodkv)
  
    YARN-954. Implemented web UI for the ApplicationHistoryServer and wired it into
    the HistoryStorage. (Zhijie Shen via vinodkv)
  
    YARN-967. Added the client and CLI interfaces for obtaining ApplicationHistory
    data. (Mayank Bansal via vinodkv)
  
    YARN-1023. Added Webservices REST APIs support for Application History. (Zhijie
    Shen via vinodkv)
  
    YARN-1413. Implemented serving of aggregated-logs in the ApplicationHistory
    server. (Mayank Bansal via vinodkv)

    YARN-1633. Defined user-facing entity, entity-info and event objects related
    to Application Timeline feature. (Zhijie Shen via vinodkv)

    YARN-1611. Introduced the concept of a configuration provider which can be
    used by ResourceManager to read configuration locally or from remote systems
    so as to help RM failover. (Xuan Gong via vinodkv)

    YARN-1659. Defined the ApplicationTimelineStore store as an abstraction for
    implementing different storage impls for storing timeline information.
    (Billie Rinaldi via vinodkv)

    YARN-1634. Added a testable in-memory implementation of
    ApplicationTimelineStore. (Zhijie Shen via vinodkv)

    YARN-1461. Added tags for YARN applications and changed RM to handle them.
    (Karthik Kambatla via zjshen)

    YARN-1636. Augmented Application-history server's web-services to also expose
    new APIs for retrieving and storing timeline information. (Zhijie Shen via
    vinodkv)

    YARN-1490. Introduced the ability to make ResourceManager optionally not kill
    all containers when an ApplicationMaster exits. (Jian He via vinodkv)

    YARN-1041. Added the ApplicationMasterProtocol API for applications to use the
    ability in ResourceManager to optionally not kill containers when the
    ApplicationMaster exits. (Jian He via vinodkv)

    YARN-1566. Changed Distributed Shell to retain containers across application
    attempts. (Jian He via vinodkv)

    YARN-1635. Implemented a Leveldb based ApplicationTimelineStore. (Billie
    Rinaldi via zjshen)

    YARN-1637. Implemented a client library for Java users to post timeline
    entities and events. (zjshen)

    YARN-1496. Protocol additions to allow moving apps between queues (Sandy
    Ryza)

    YARN-1498. Common scheduler changes for moving apps between queues (Sandy
    Ryza)

    YARN-1504. RM changes for moving apps between queues (Sandy Ryza)

    YARN-1499. Fair Scheduler changes for moving apps between queues (Sandy
    Ryza)

    YARN-1497. Command line additions for moving apps between queues (Sandy
    Ryza)

    YARN-1588. Enhanced RM and the scheduling protocol to also send NMTokens of
    transferred containers from previous app-attempts to new AMs after YARN-1490.
    (Jian He via vinodkv)

    YARN-1717. Enabled periodically discarding old data in LeveldbTimelineStore.
    (Billie Rinaldi via zjshen)

    YARN-1690. Made DistributedShell send timeline entities+events. (Mayank Bansal
    via zjshen)

    YARN-1775. Enhanced ProcfsBasedProcessTree to optionally add the ability to
    use smaps for obtaining used memory information. (Rajesh Balamohan via
    vinodkv)

    YARN-1838. Enhanced timeline service getEntities API to get entities from a
    given entity ID or insertion timestamp. (Billie Rinaldi via zjshen)

  IMPROVEMENTS

    YARN-1007. Enhance History Reader interface for Containers. (Mayank Bansal via
    devaraj)
  
    YARN-974. Added more information to RMContainer to be collected and recorded in
    Application-History. (Zhijie Shen via vinodkv)
  
    YARN-987. Added ApplicationHistoryManager responsible for exposing reports to
    all clients. (Mayank Bansal via vinodkv)

    YARN-1630. Introduce timeout for async polling operations in YarnClientImpl
    (Aditya Acharya via Sandy Ryza)

    YARN-1617. Remove ancient comment and surround LOG.debug in
    AppSchedulingInfo.allocate (Sandy Ryza)

    YARN-1639. Modified RM HA configuration handling to have a way of not
    requiring separate configuration files for each RM. (Xuan Gong via vinodkv)

    YARN-1668. Modified RM HA handling of admin-acls to be available across RM
    failover by making using of a remote configuration-provider. (Xuan Gong via
    vinodkv)

    YARN-1667. Modified RM HA handling of super users (with proxying ability) to
    be available across RM failover by making using of a remote
    configuration-provider. (Xuan Gong via vinodkv)

    YARN-1285. Changed the default value of yarn.acl.enable in yarn-default.xml
    to be consistent with what exists (false) in the code and documentation.
    (Kenji Kikushima via vinodkv)

    YARN-1669. Modified RM HA handling of protocol level service-ACLS to
    be available across RM failover by making using of a remote
    configuration-provider. (Xuan Gong via vinodkv)

    YARN-1665. Simplify the configuration of RM HA by having better default
    values. (Xuan Gong via vinodkv)

    YARN-1660. Simplified the RM HA configuration to accept and be able to simply
    depend just on configuration properties of the form
    yarn.resourcemanager.hostname.RMID and use the default ports for all service
    addresses. (Xuan Gong via vinodkv)

    YARN-1493. Changed ResourceManager and Scheduler interfacing to recognize
    app-attempts separately from apps. (Jian He via vinodkv)

    YARN-1459. Changed ResourceManager to depend its service initialization
    on the configuration-provider mechanism during startup too. (Xuan Gong via
    vinodkv)

    YARN-1706. Created an utility method to dump timeline records to JSON
    strings. (zjshen)

    YARN-1641. ZK store should attempt a write periodically to ensure it is 
    still Active. (kasha)

    YARN-1531. True up yarn command documentation (Akira Ajisaka via kasha)

    YARN-1345. Remove FINAL_SAVING state from YarnApplicationAttemptState
    (Zhijie Shen via jianhe)

    YARN-1676. Modified RM HA handling of user-to-group mappings to
    be available across RM failover by making using of a remote
    configuration-provider. (Xuan Gong via vinodkv)

    YARN-1666. Modified RM HA handling of include/exclude node-lists to be
    available across RM failover by making using of a remote
    configuration-provider. (Xuan Gong via vinodkv)

    YARN-1171. Add default queue properties to Fair Scheduler documentation
    (Naren Koneru via Sandy Ryza)

    YARN-1470. Add audience annotations to MiniYARNCluster. (Anubhav Dhoot
    via kasha)

    YARN-1732. Changed types of related-entities and primary-filters in the
    timeline-service to be sets instead of maps. (Billie Rinaldi via vinodkv)

    YARN-1687. Renamed user-facing records for the timeline-service to be simply
    named after 'timeline' instead of 'apptimeline'. (Zhijie Shen via vinodkv)

    YARN-1749. Updated application-history related configs to reflect the latest
    reality and to be consistently named. (Zhijie Shen via vinodkv)

    YARN-1301. Added the INFO level log of the non-empty blacklist additions
    and removals inside ApplicationMasterService. (Tsuyoshi Ozawa via zjshen)

    YARN-1528. Allow setting auth for ZK connections. (kasha)

    YARN-1704. Modified LICENSE and NOTICE files to reflect newly used levelDB
    related libraries. (Billie Rinaldi via vinodkv)

    YARN-1765. Added test cases to verify that killApplication API works across
    ResourceManager failover. (Xuan Gong via vinodkv) 

    YARN-1730. Implemented simple write-locking in the LevelDB based timeline-
    store. (Billie Rinaldi via vinodkv)

    YARN-986. Changed client side to be able to figure out the right RM Delegation
    token for the right ResourceManager when HA is enabled. (Karthik Kambatla via
    vinodkv)

    YARN-1761. Modified RMAdmin CLI to check whether HA is enabled or not before
    it executes any of the HA admin related commands. (Xuan Gong via vinodkv)

    YARN-1780. Improved logging in the Timeline client and server. (Zhijie Shen
    via vinodkv)

    YARN-1525. Web UI should redirect to active RM when HA is enabled. (Cindy Li
    via kasha)

    YARN-1781. Modified NodeManagers to allow admins to specify max disk
    utilization for local disks so as to be able to offline full disks. (Varun
    Vasudev via vinodkv)

    YARN-1410. Added tests to validate that clients can fail-over to a new RM
    after getting an application-ID but before submission and can still submit to
    the newly active RM with no issues. (Xuan Gong via vinodkv)

    YARN-1764. Modified YarnClient to correctly handle failover of ResourceManager
    after the submitApplication call goes through. (Xuan Gong via vinodkv)

    YARN-1389. Made ApplicationClientProtocol and ApplicationHistoryProtocol
    expose analogous getApplication(s)/Attempt(s)/Container(s) APIs. (Mayank
    Bansal via zjshen)

    YARN-1658. Modified web-app framework to let standby RMs redirect
    web-service calls to the active RM. (Cindy Li via vinodkv)

    YARN-1824. Improved NodeManager and clients to be able to handle cross
    platform application submissions. (Jian He via vinodkv)

    YARN-1512. Enhanced CapacityScheduler to be able to decouple scheduling from
    node-heartbeats. (Arun C Murthy via vinodkv)

    YARN-1570. Fixed formatting of the lines in YarnCommands.apt.vm docs source.
    (Akira Ajisaka via vinodkv)

    YARN-1536. Cleanup: Get rid of ResourceManager#get*SecretManager() methods 
    and use the RMContext methods instead. (Anubhav Dhoot via kasha)

    YARN-1850. Introduced the ability to optionally disable sending out timeline-
    events in the TimelineClient. (Zhijie Shen via vinodkv)

    YARN-1452. Added documentation about the configuration and usage of generic
    application history and the timeline data service. (Zhijie Shen via vinodkv)

    YARN-1891. Added documentation for NodeManager health-monitoring. (Varun
    Vasudev via vinodkv)

    YARN-1017. Added documentation for ResourceManager Restart.(jianhe)

  OPTIMIZATIONS

    YARN-1771. Reduce the number of NameNode operations during localization of
    public resources using a cache. (Sangjin Lee via cdouglas)

  BUG FIXES

    YARN-935. Correcting pom.xml to build applicationhistoryserver module
    successfully. (Zhijie Shen via vinodkv)
  
    YARN-962. Fixed bug in application-history proto file and renamed it be just
    a client proto file. (Zhijie Shen via vinodkv)
  
    YARN-984. Renamed the incorrectly named applicationhistoryservice.records.pb.impl
    package to be applicationhistoryservice.records.impl.pb. (Devaraj K via vinodkv)
  
    YARN-1534. Fixed failure of test TestAHSWebApp. (Shinichi Yamashita via vinodkv)
  
    YARN-1555. Fixed test failures in applicationhistoryservice.* (Vinod Kumar 
    Vavilapalli via mayank)
  
    YARN-1594. Updated pom.xml of applicationhistoryservice sub-project according to
    YARN-888. (Vinod Kumar Vavilapalli via zjshen)
  
    YARN-1596. Fixed Javadoc warnings on branch YARN-321. (Vinod Kumar Vavilapalli
    via zjshen)
  
    YARN-1597. Fixed Findbugs warnings on branch YARN-321. (Vinod Kumar Vavilapalli
    via zjshen)
  
    YARN-1595. Made enabling history service configurable and fixed test failures on
    branch YARN-321. (Vinod Kumar Vavilapalli via zjshen)
  
    YARN-1605. Fixed formatting issues in the new module on branch YARN-321. (Vinod
    Kumar Vavilapalli via zjshen)

    YARN-1625. Fixed RAT warnings after YARN-321 merge. (Shinichi Yamashita via
    vinodkv)

    YARN-1613. Fixed the typo with the configuration name
    YARN_HISTORY_SERVICE_ENABLED. (Akira Ajisaka via vinodkv)

    YARN-1618. Fix invalid RMApp transition from NEW to FINAL_SAVING (kasha)

    YARN-1600. RM does not startup when security is enabled without spnego
    configured (Haohui Mai via jlowe)

    YARN-1642. RMDTRenewer#getRMClient should use ClientRMProxy (kasha)

    YARN-1632. TestApplicationMasterServices should be under
    org.apache.hadoop.yarn.server.resourcemanager package (Chen He via jeagles)

    YARN-1673. Fix option parsing in YARN's application CLI after it is broken
    by YARN-967. (Mayank Bansal via vinodkv)

    YARN-1684. Fixed history server heap size in yarn script. (Billie Rinaldi
    via zjshen)

    YARN-1166. Fixed app-specific and attempt-specific QueueMetrics to be
    triggered by accordingly app event and attempt event. 

    YARN-1689. Made RMAppAttempt get killed when RMApp is at ACCEPTED. (Vinod
    Kumar Vavilapalli via zjshen)

    YARN-1661. Fixed DS ApplicationMaster to write the correct exit log. (Vinod
    Kumar Vavilapalli via zjshen)

    YARN-1672. YarnConfiguration is missing a default for 
    yarn.nodemanager.log.retain-seconds (Naren Koneru via kasha)

    YARN-1698. Fixed default TimelineStore in code to match what is documented
    in yarn-default.xml (Zhijie Shen via vinodkv)

    YARN-1697. NodeManager reports negative running containers (Sandy Ryza)

    YARN-1719. Fixed the root path related Jersey warnings produced in
    ATSWebServices. (Billie Rinaldi via zjshen)

    YARN-1692. ConcurrentModificationException in fair scheduler AppSchedulable
    (Sangjin Lee via Sandy Ryza)

    YARN-1578. Fixed reading incomplete application attempt and container data
    in FileSystemApplicationHistoryStore. (Shinichi Yamashita via zjshen)

    YARN-1417. Modified RM to generate container-tokens not at creation time, but
    at allocation time so as to prevent RM from shelling out containers with
    expired tokens. (Omkar Vinit Joshi and Jian He via vinodkv)

    YARN-1553. Modified YARN and MR to stop using HttpConfig.isSecure() and
    instead rely on the http policy framework. And also fix some bugs related
    to https handling in YARN web-apps. (Haohui Mai via vinodkv)

    YARN-1721. When moving app between queues in Fair Scheduler, grab lock on
    FSSchedulerApp (Sandy Ryza)

    YARN-1724. Race condition in Fair Scheduler when continuous scheduling is
    turned on (Sandy Ryza)

    YARN-1590. Fixed ResourceManager, web-app proxy and MR JobHistoryServer to
    expand _HOST properly in their kerberos principles. (Mohammad Kamrul Islam
    va vinodkv)

    YARN-1428. Fixed RM to write the final state of RMApp/RMAppAttempt to the 
    application history store in the transition to the final state. (Contributed
    by Zhijie Shen)

    YARN-713. Fixed ResourceManager to not crash while building tokens when DNS
    issues happen transmittently. (Jian He via vinodkv)

    YARN-1398. Fixed a deadlock in ResourceManager between users requesting
    queue-acls and completing containers. (vinodkv)

    YARN-1071. Enabled ResourceManager to recover cluster metrics
    numDecommissionedNMs after restarting. (Jian He via zjshen)

    YARN-1742. Fixed javadoc of configuration parameter
    DEFAULT_NM_MIN_HEALTHY_DISKS_FRACTION. (Akira Ajisaka via vinodkv)

    YARN-1686. Fixed NodeManager to properly handle any errors during
    re-registration after a RESYNC and thus avoid hanging. (Rohith Sharma via
    vinodkv)

    YARN-1734. Fixed ResourceManager to update the configurations when it
    transits from standby to active mode so as to assimilate any changes that
    happened while it was in standby mode. (Xuan Gong via vinodkv)

    YARN-1760. TestRMAdminService assumes CapacityScheduler. (kasha)

    YARN-1758. Fixed ResourceManager to not mandate the presence of site specific
    configuration files and thus fix failures in downstream tests. (Xuan Gong via
    vinodkv)

    YARN-1748. Excluded core-site.xml from hadoop-yarn-server-tests package's jar
    and thus avoid breaking downstream tests. (Sravya Tirukkovalur via vinodkv)

    YARN-1729. Made TimelineWebServices deserialize the string primary- and
    secondary-filters param into the JSON-compatible object. (Billie Rinaldi via
    zjshen)

    YARN-1766. Fixed a bug in ResourceManager to use configuration loaded from the
    configuration-provider when booting up. (Xuan Gong via vinodkv)

    YARN-1768. Fixed error message being too verbose when killing a non-existent
    application. (Tsuyoshi OZAWA via raviprak)
    
    YARN-1774. FS: Submitting to non-leaf queue throws NPE. (Anubhav Dhoot and
    Karthik Kambatla via kasha)

    YARN-1783. Fixed a bug in NodeManager's status-updater that was losing
    completed container statuses when NodeManager is forced to resync by the
    ResourceManager. (Jian He via vinodkv) 

    YARN-1787. Fixed help messages for applicationattempt and container
    sub-commands in bin/yarn. (Zhijie Shen via vinodkv)

    YARN-1793. Fixed ClientRMService#forceKillApplication not killing unmanaged
    application. (Karthik Kambatla via jianhe)

    YARN-1788. Fixed a bug in ResourceManager to set the apps-completed and
    apps-killed metrics correctly for killed applications. (Varun Vasudev via
    vinodkv)

    YARN-1821. NPE on registerNodeManager if the request has containers for 
    UnmanagedAMs. (kasha)

    YARN-1800. Fixed NodeManager to gracefully handle RejectedExecutionException
    in the public-localizer thread-pool. (Varun Vasudev via vinodkv)

    YARN-1444. Fix CapacityScheduler to deal with cases where applications
    specify host/rack requests without off-switch request. (Wangda Tan via
    acmurthy)

    YARN-1812. Fixed ResourceManager to synchrously renew tokens after recovery
    and thus recover app itself synchronously and avoid races with resyncing
    NodeManagers. (Jian He via vinodkv)

    YARN-1816. Fixed ResourceManager to get RMApp correctly handle
    ATTEMPT_FINISHED event at ACCEPTED state that can happen after RM restarts.
    (Jian He via vinodkv)

    YARN-1789. ApplicationSummary does not escape newlines in the app name
    (Tsuyoshi OZAWA via jlowe)

    YARN-1830. Fixed TestRMRestart#testQueueMetricsOnRMRestart failure due to
    race condition when app is submitted. (Zhijie Shen via jianhe)

    YARN-1685. Fixed few bugs related to handling of containers' log-URLs on
    ResourceManager and history-service. (Zhijie Shen via vinodkv)

    YARN-1206. Fixed AM container log to show on NM web page after application
    finishes if log-aggregation is disabled. (Rohith Sharmaks via jianhe)

    YARN-1591. Fixed AsyncDispatcher to handle interrupts on shutdown in a sane
    manner and thus fix failure of TestResourceTrackerService. (Tsuyoshi Ozawa
    via vinodkv)

    YARN-1839. Fixed handling of NMTokens in ResourceManager such that containers
    launched by AMs running on the same machine as the AM are correctly
    propagated. (Jian He via vinodkv)

    YARN-1640. Fixed manual failover of ResourceManagers to work correctly in
    secure clusters. (Xuan Gong via vinodkv)

    YARN-1855. Made Application-history server to be optional in MiniYARNCluster
    and thus avoid the failure of TestRMFailover#testRMWebAppRedirect. (Zhijie
    Shen via vinodkv)

    YARN-1859. Fixed WebAppProxyServlet to correctly handle applications absent
    on the ResourceManager. (Zhijie Shen via vinodkv)

    YARN-1811. Fixed AMFilters in YARN to correctly accept requests from either
    web-app proxy or the RMs when HA is enabled. (Robert Kanter via vinodkv)

    YARN-1670. Fixed a bug in log-aggregation that can cause the writer to write
    more log-data than the log-length that it records. (Mit Desai via vinodk)

    YARN-1849. Fixed NPE in ResourceTrackerService#registerNodeManager for UAM
    (Karthik Kambatla via jianhe )

    YARN-1863. Fixed test failure in TestRMFailover after YARN-1859. (Xuan Gong
    via vinodkv)

    YARN-1854. Fixed test failure in TestRMHA#testStartAndTransitions. (Rohith
    Sharma KS via vinodkv)

    YARN-1776. Fixed DelegationToken renewal to survive RM failover. (Zhijie
    Shen via jianhe)

    YARN-1577. Made UnmanagedAMLauncher do launchAM after the attempt reaches
    the LAUNCHED state. (Jian He via zjshen)

    YARN-1785. FairScheduler treats app lookup failures as ERRORs. 
    (bc Wong via kasha)

    YARN-1752. Fixed ApplicationMasterService to reject unregister request if
    AM did not register before. (Rohith Sharma via jianhe)

    YARN-1846. TestRM#testNMTokenSentForNormalContainer assumes CapacityScheduler.
    (Robert Kanter via kasha)

    YARN-1705. Reset cluster-metrics on transition to standby. (Rohith via kasha)

    YARN-1852. Fixed RMAppAttempt to not resend AttemptFailed/AttemptKilled
    events to already recovered Failed/Killed RMApps. (Rohith via jianhe)

    YARN-1866. Fixed an issue with renewal of RM-delegation tokens on restart or
    fail-over. (Jian He via vinodkv)

    YARN-1867. Fixed a bug in ResourceManager that was causing invalid ACL checks
    in the web-services after fail-over. (Vinod Kumar Vavilapalli)

    YARN-1521. Mark Idempotent/AtMostOnce annotations to the APIs in
    ApplicationClientProtcol, ResourceManagerAdministrationProtocol and
    ResourceTrackerProtocol so that they work in HA scenario. (Xuan Gong
    via jianhe)

    YARN-1873. Fixed TestDistributedShell failure when the test cases are out of
    order. (Mit Desai via zjshen)

    YARN-1893. Mark AtMostOnce annotation to ApplicationMasterProtocol#allocate.
    (Xuan Gong via jianhe)

Release 2.3.1 - UNRELEASED

  INCOMPATIBLE CHANGES

  NEW FEATURES

  IMPROVEMENTS

  OPTIMIZATIONS

  BUG FIXES

Release 2.3.0 - 2014-02-18

  INCOMPATIBLE CHANGES

  NEW FEATURES

    YARN-649. Added a new NM web-service to serve container logs in plain text
    over HTTP. (Sandy Ryza via vinodkv)

    YARN-1021. Yarn Scheduler Load Simulator. (ywskycn via tucu)

    YARN-1010. FairScheduler: decouple container scheduling from nodemanager
    heartbeats. (Wei Yan via Sandy Ryza)

    YARN-1253. Changes to LinuxContainerExecutor to run containers as a single 
    dedicated user in non-secure mode. (rvs via tucu)

    YARN-1027. Implement RMHAProtocolService (Karthik Kambatla via bikas)

    YARN-1068. Add admin support for HA operations (Karthik Kambatla via
    bikas)

    YARN-311. RM/scheduler support for dynamic resource configuration.
    (Junping Du via llu)

    YARN-1392. Allow sophisticated app-to-queue placement policies in the Fair
    Scheduler (Sandy Ryza)

    YARN-1447. Common PB type definitions for container resizing. (Wangda Tan
    via Sandy Ryza)

    YARN-1448. AM-RM protocol changes to support container resizing (Wangda Tan
    via Sandy Ryza)

    YARN-312. Introduced ResourceManagerAdministrationProtocol changes to support
    changing resources on node. (Junping Du via vinodkv)

    YARN-1028. Added FailoverProxyProvider capability to ResourceManager to help
    with RM failover. (Karthik Kambatla via vinodkv)

    YARN-1029. Added embedded leader election in the ResourceManager. (Karthik
    Kambatla via vinodkv)

    YARN-1033. Expose RM active/standby state to Web UI and REST API (kasha)

  IMPROVEMENTS

    YARN-305. Fair scheduler logs too many "Node offered to app" messages.
    (Lohit Vijayarenu via Sandy Ryza)

    YARN-1258. Allow configuring the Fair Scheduler root queue (Sandy Ryza)

    YARN-1288. Make Fair Scheduler ACLs more user friendly (Sandy Ryza)

    YARN-1315. TestQueueACLs should also test FairScheduler (Sandy Ryza)

    YARN-1335. Move duplicate code from FSSchedulerApp and FiCaSchedulerApp
    into SchedulerApplication (Sandy Ryza)

    YARN-1333. Support blacklisting in the Fair Scheduler (Tsuyoshi Ozawa via
    Sandy Ryza)

    YARN-1109. Demote NodeManager "Sending out status for container" logs to
    debug (haosdent via Sandy Ryza)

    YARN-1321. Changed NMTokenCache to support both singleton and an instance
    usage. (Alejandro Abdelnur via vinodkv) 

    YARN-1388. Fair Scheduler page always displays blank fair share (Liyin Liang
    via Sandy Ryza)

    YARN-7. Support CPU resource for DistributedShell. (Junping Du via llu)

    YARN-905. Add state filters to nodes CLI (Wei Yan via Sandy Ryza)

    YARN-1098. Separate out RM services into Always On and Active (Karthik
    Kambatla via bikas)

    YARN-353. Add Zookeeper-based store implementation for RMStateStore.
    (Bikas Saha, Jian He and Karthik Kambatla via hitesh)

    YARN-819. ResourceManager and NodeManager should check for a minimum allowed
    version (Robert Parker via jeagles)

    YARN-425. coverage fix for yarn api (Aleksey Gorshkov via jeagles)

    YARN-1199. Make NM/RM Versions Available (Mit Desai via jeagles)

    YARN-1232. Configuration to support multiple RMs (Karthik Kambatla via
    bikas)

    YARN-465. fix coverage org.apache.hadoop.yarn.server.webproxy (Aleksey
    Gorshkov and Andrey Klochkov via jlowe)

    YARN-976. Document the meaning of a virtual core. (Sandy Ryza)

    YARN-1182. MiniYARNCluster creates and inits the RM/NM only on start()
    (Karthik Kambatla via Sandy Ryza)

    HADOOP-9598. Improve code coverage of RMAdminCLI (Aleksey Gorshkov and
    Andrey Klochkov via jeagles)

    YARN-1306. Clean up hadoop-sls sample-conf according to YARN-1228 (Wei Yan
    via Sandy Ryza)

    YARN-891. Modified ResourceManager state-store to remember completed
    applications so that clients can get information about them post RM-restart.
    (Jian He via vinodkv)

    YARN-1290. Let continuous scheduling achieve more balanced task assignment
    (Wei Yan via Sandy Ryza)

    YARN-786. Expose application resource usage in RM REST API (Sandy Ryza)

    YARN-1323. Set HTTPS webapp address along with other RPC addresses in HAUtil
    (Karthik Kambatla via Sandy Ryza)

    YARN-1121. Changed ResourceManager's state-store to drain all events on
    shut-down. (Jian He via vinodkv)

    YARN-1387. RMWebServices should use ClientRMService for filtering
    applications (Karthik Kambatla via Sandy Ryza)

    YARN-1222. Make improvements in ZKRMStateStore for fencing (Karthik
    Kambatla via bikas)

    YARN-709. Added tests to verify validity of delegation tokens and logging of
    appsummary after RM restart. (Jian He via vinodkv)

    YARN-1210. Changed RM to start new app-attempts on RM restart only after
    ensuring that previous AM exited or after expiry time. (Omkar Vinit Joshi via
    vinodkv)

    YARN-674. Fixed ResourceManager to renew DelegationTokens on submission
    asynchronously to work around potential slowness in state-store. (Omkar Vinit
    Joshi via vinodkv)

    YARN-584. In scheduler web UIs, queues unexpand on refresh. (Harshit
    Daga via Sandy Ryza)

    YARN-1303. Fixed DistributedShell to not fail with multiple commands separated
    by a semi-colon as shell-command. (Xuan Gong via vinodkv)

    YARN-1423. Support queue placement by secondary group in the Fair Scheduler
    (Ted Malaska via Sandy Ryza)

    YARN-1314. Fixed DistributedShell to not fail with multiple arguments for a
    shell command separated by spaces. (Xuan Gong via vinodkv)

    YARN-1239. Modified ResourceManager state-store implementations to start
    storing version numbers. (Jian He via vinodkv)

    YARN-1241. In Fair Scheduler, maxRunningApps does not work for non-leaf
    queues. (Sandy Ryza)

    YARN-1318. Promoted AdminService to an Always-On service and merged it into
    RMHAProtocolService. (Karthik Kambatla via vinodkv)

    YARN-1332. In TestAMRMClient, replace assertTrue with assertEquals where
    possible (Sebastian Wong via Sandy Ryza)

    YARN-1403. Separate out configuration loading from QueueManager in the Fair
    Scheduler (Sandy Ryza)

    YARN-1181. Augment MiniYARNCluster to support HA mode (Karthik Kambatla)

    YARN-546. Allow disabling the Fair Scheduler event log (Sandy Ryza)

    YARN-807. When querying apps by queue, iterating over all apps is
    inefficient and limiting (Sandy Ryza)

    YARN-1378. Implemented a cleaner of old finished applications from the RM
    state-store. (Jian He via vinodkv)

    YARN-1481. Move internal services logic from AdminService to ResourceManager.
    (vinodkv via kasha)

    YARN-1491. Upgrade JUnit3 TestCase to JUnit 4 (Chen He via jeagles)

    YARN-408. Change CapacityScheduler to not disable delay-scheduling by default.
    (Mayank Bansal via vinodkv)

    YARN-1325. Modified RM HA configuration validation to also ensure that
    multiple RMs are configured. (Xuan Gong via vinodkv)

    YARN-1311. Fixed app specific scheduler-events' names to be app-attempt
    based. (vinodkv via jianhe)

    YARN-1485. Modified RM HA configuration validation to also ensure that
    service-address configuration are configured for every RM. (Xuan Gong via
    vinodkv)

    YARN-1435. Modified Distributed Shell to accept either the command or the
    custom script. (Xuan Gong via zjshen)

    YARN-1446. Changed client API to retry killing application till RM
    acknowledges so as to account for RM crashes/failover. (Jian He via vinodkv)

    YARN-1307. Redesign znode structure for Zookeeper based RM state-store for
    better organization and scalability. (Tsuyoshi OZAWA via vinodkv)

    YARN-1172. Convert SecretManagers in RM to services (Tsuyoshi OZAWA via kasha)

    YARN-1523. Use StandbyException instead of RMNotYetReadyException (kasha)

    YARN-1541. Changed ResourceManager to invalidate ApplicationMaster host/port
    information once an AM crashes. (Jian He via vinodkv)

    YARN-1482. Modified WebApplicationProxy to make it work across ResourceManager
    fail-over. (Xuan Gong via vinodkv)

    YARN-1568. Rename clusterid to clusterId in ActiveRMInfoProto (kasha)

    YARN-1579. ActiveRMInfoProto fields should be optional (kasha)

    YARN-888. Cleaned up POM files so that non-leaf modules don't include any
    dependencies and thus compact the dependency list for leaf modules.
    (Alejandro Abdelnur via vinodkv)

    YARN-1567. In Fair Scheduler, allow empty queues to change between leaf and
    parent on allocation file reload (Sandy Ryza)

    YARN-1616. RMFatalEventDispatcher should log the cause of the event (kasha)

    YARN-1624. QueuePlacementPolicy format is not easily readable via a JAXB
    parser (Aditya Acharya via Sandy Ryza)

    YARN-1623. Include queue name in RegisterApplicationMasterResponse (Sandy
    Ryza)

    YARN-1573. ZK store should use a private password for root-node-acls. 
    (kasha).

  OPTIMIZATIONS

  BUG FIXES

    YARN-1284. LCE: Race condition leaves dangling cgroups entries for killed
    containers. (Alejandro Abdelnur via Sandy Ryza)

    YARN-1283. Fixed RM to give a fully-qualified proxy URL for an application
    so that clients don't need to do scheme-mangling. (Omkar Vinit Joshi via
    vinodkv)

    YARN-879. Fixed tests w.r.t o.a.h.y.server.resourcemanager.Application.
    (Junping Du via devaraj)

    YARN-1265. Fair Scheduler chokes on unhealthy node reconnect (Sandy Ryza)

    YARN-1044. used/min/max resources do not display info in the scheduler page
    (Sangjin Lee via Sandy Ryza)

    YARN-1259. In Fair Scheduler web UI, queue num pending and num active apps
    switched. (Robert Kanter via Sandy Ryza)

    YARN-1295. In UnixLocalWrapperScriptBuilder, using bash -c can cause Text
    file busy errors (Sandy Ryza)

    YARN-1185. Fixed FileSystemRMStateStore to not leave partial files that
    prevent subsequent ResourceManager recovery. (Omkar Vinit Joshi via vinodkv)

    YARN-1331. yarn.cmd exits with NoClassDefFoundError trying to run rmadmin or
    logs. (cnauroth)

    YARN-1330. Fair Scheduler: defaultQueueSchedulingPolicy does not take effect
    (Sandy Ryza)
    
    YARN-1022. Unnecessary INFO logs in AMRMClientAsync (haosdent via bikas)
    
    YARN-1349. yarn.cmd does not support passthrough to any arbitrary class.
    (cnauroth)
    
    YARN-1357. TestContainerLaunch.testContainerEnvVariables fails on Windows.
    (Chuan Liu via cnauroth)

    YARN-1358. TestYarnCLI fails on Windows due to line endings. (Chuan Liu via
    cnauroth)

    YARN-1343. NodeManagers additions/restarts are not reported as node updates 
    in AllocateResponse responses to AMs. (tucu)

    YARN-1381. Same relaxLocality appears twice in exception message of
    AMRMClientImpl#checkLocalityRelaxationConflict() (Ted Yu via Sandy Ryza)

    YARN-1407. RM Web UI and REST APIs should uniformly use
    YarnApplicationState (Sandy Ryza)

    YARN-1438. Ensure container diagnostics includes exception from container
    launch. (stevel via acmurthy)

    YARN-1138. yarn.application.classpath is set to point to $HADOOP_CONF_DIR
    etc., which does not work on Windows. (Chuan Liu via cnauroth)

    YARN-461. Fair scheduler should not accept apps with empty string queue name. 
    (ywskycn via tucu)

    YARN-1060. Two tests in TestFairScheduler are missing @Test annotation
    (Niranjan Singh via Sandy Ryza)

    YARN-1188. The context of QueueMetrics becomes default when using
    FairScheduler (Tsuyoshi Ozawa via Sandy Ryza)

    YARN-1268. TestFairScheduler.testContinuousScheduling is flaky (Sandy Ryza)

    YARN-1300. SLS tests fail because conf puts YARN properties in
    fair-scheduler.xml (Ted Yu via Sandy Ryza)

    YARN-1183. MiniYARNCluster shutdown takes several minutes intermittently
    (Andrey Klochkov via jeagles)

    YARN-1305. RMHAProtocolService#serviceInit should handle HAUtil's
    IllegalArgumentException (Tsuyoshi Ozawa via bikas)

    YARN-1374. Changed ResourceManager to start the preemption policy monitors
    as active services. (Karthik Kambatla via vinodkv)

    YARN-1395. Distributed shell application master launched with debug flag can
    hang waiting for external ls process. (cnauroth)

    YARN-1400. yarn.cmd uses HADOOP_RESOURCEMANAGER_OPTS. Should be
    YARN_RESOURCEMANAGER_OPTS. (Raja Aluri via cnauroth)

    YARN-1401. With zero sleep-delay-before-sigkill.ms, no signal is ever sent
    (Gera Shegalov via Sandy Ryza)

    YARN-1411. HA config shouldn't affect NodeManager RPC addresses (Karthik
    Kambatla via bikas)

    YARN-1419. TestFifoScheduler.testAppAttemptMetrics fails intermittently
    under jdk7 (Jonathan Eagles via jlowe)

    YARN-744. Race condition in ApplicationMasterService.allocate .. It might
    process same allocate request twice resulting in additional containers
    getting allocated. (Omkar Vinit Joshi via bikas)

    YARN-1425. TestRMRestart fails because MockRM.waitForState(AttemptId) uses
    current attempt instead of the attempt passed as argument (Omkar Vinit
    Joshi via bikas)

    YARN-1053. Diagnostic message from ContainerExitEvent is ignored in
    ContainerImpl (Omkar Vinit Joshi via bikas)

    YARN-1320. Fixed Distributed Shell application to respect custom log4j
    properties file. (Xuan Gong via vinodkv)

    YARN-1416. Fixed a few invalid transitions in RMApp, RMAppAttempt and in some
    tests. (Jian He via vinodkv)

    YARN-895. Changed RM state-store to not crash immediately if RM restarts while
    the state-store is down. (Jian He via vinodkv)

    YARN-1454. Fixed test failure issue with TestRMRestart. (Karthik Kambatla
    via vinodkv)

    YARN-1450. Fixed test failure in TestUnmanagedAMLauncher by removing its
    dependency on distributed-shell. (Binglin Chang via vinodkv)

    YARN-1405. Fixed ResourceManager to not hang when init/start fails with an
    exception w.r.t state-store. (Jian He via vinodkv)

    YARN-1505. Fixed Webapplication proxy server to not hardcode its bind
    address. (Xuan Gong via vinodkv)

    YARN-1145. Fixed a potential file-handle leak in the web interface for
    displaying aggregated logs. (Rohith Sharma via vinodkv)

    YARN-1451. TestResourceManager relies on the scheduler assigning multiple
    containers in a single node update. (Sandy Ryza via kasha)

    YARN-1527. Fix yarn rmadmin command to print the correct usage info.
    (Akira AJISAKA via jianhe)

    YARN-1522. Fixed a race condition in the test TestApplicationCleanup that was
    causing it to randomly fail. (Liyin Liang via vinodkv)

    YARN-1549. Fixed a bug in ResourceManager's ApplicationMasterService that
    was causing unamanged AMs to not finish correctly. (haosdent via vinodkv)

    YARN-1559. Race between ServerRMProxy and ClientRMProxy setting 
    RMProxy#INSTANCE. (kasha and vinodkv via kasha)

    YARN-1560. Fixed TestYarnClient#testAMMRTokens failure with null AMRM token.
    (Ted Yu via jianhe)

    YARN-1409. NonAggregatingLogHandler can throw RejectedExecutionException
    (Tsuyoshi OZAWA via jlowe)

    YARN-1293. Fixed TestContainerLaunch#testInvalidEnvSyntaxDiagnostics failure
    caused by non-English system locale. (Tsuyoshi OZAWA via jianhe)

    YARN-1574. RMDispatcher should be reset on transition to standby. (Xuan Gong
    via kasha)

    YARN-1598. HA-related rmadmin commands don't work on a secure cluster (kasha)

    YARN-1603. Remove two *.orig files which were unexpectedly committed. 
    (Zhijie Shen via junping_du)

    YARN-1601. 3rd party JARs are missing from hadoop-dist output. (tucu)

    YARN-1351. Invalid string format in Fair Scheduler log warn message
    (Konstantin Weitz via Sandy Ryza)

    YARN-1608. LinuxContainerExecutor has a few DEBUG messages at INFO level
    (kasha)

    YARN-1606. Fix the default value of yarn.resourcemanager.zk-timeout-ms 
    in yarn-default.xml (kasha)

    YARN-1607. TestRM relies on the scheduler assigning multiple containers in
    a single node update (Sandy Ryza)

    YARN-1575. Public localizer crashes with "Localized unkown resource"
    (jlowe)

    YARN-1629. IndexOutOfBoundsException in MaxRunningAppsEnforcer (Sandy Ryza)

    YARN-1628. Fixed the test failure in TestContainerManagerSecurity. (Vinod
    Kumar Vavilapalli via zjshen)

Release 2.2.0 - 2013-10-13

  INCOMPATIBLE CHANGES
  
    YARN-1229. Define constraints on Auxiliary Service names. Change
    ShuffleHandler service name from mapreduce.shuffle to
    mapreduce_shuffle (Xuan Gong via sseth)

  NEW FEATURES

  IMPROVEMENTS

    YARN-1246. Added application finish-status to ApplicationSummary for the sake
    of testing given ApplicationHistoryServer is not yet ready. (Arpit Gupta via
    vinodkv)

    YARN-899. Added back queue level administrator-acls so that there is no
    regression w.r.t 1.x. (Xuan Gong via vinodkv)

    YARN-1228. Clean up Fair Scheduler configuration loading. (Sandy Ryza)

    YARN-1213. Restore config to ban submitting to undeclared pools in the
    Fair Scheduler. (Sandy Ryza)

    YARN-1277. Added a policy based configuration for http/https in common
    HttpServer and using the same in YARN - related to per project https config
    support via HADOOP-10022. (Suresh Srinivas and Omkar Vinit Joshi via vinodkv)

  OPTIMIZATIONS

  BUG FIXES

    YARN-1128. FifoPolicy.computeShares throws NPE on empty list of Schedulables
    (Karthik Kambatla via Sandy Ryza)

    YARN-1214. Register ClientToken MasterKey in SecretManager after it is
    saved (Jian He via bikas)

    YARN-49. Improve distributed shell application to work on a secure cluster.
    (Vinod Kumar Vavilapalli via hitesh)

    YARN-1157. Fixed ResourceManager UI to behave correctly when apps like
    distributed-shell do not set tracking urls. (Xuan Gong via vinodkv)

    YARN-1221. With Fair Scheduler, reserved MB reported in RM web UI increases
    indefinitely (Siqi Li via Sandy Ryza)

    YARN-1247. test-container-executor has gotten out of sync with the changes to
    container-executor. (rvs via tucu)

    YARN-1070. Fixed race conditions in NodeManager during container-kill.
    (Zhijie Shen via vinodkv)

    YARN-1215. Yarn URL should include userinfo. (Chuan Liu via cnauroth)

    YARN-1262. TestApplicationCleanup relies on all containers assigned in a
    single heartbeat (Karthik Kambatla via Sandy Ryza)

    YARN-1260. Added webapp.http.address to yarn-default.xml so that default
    install with https enabled doesn't have broken link on NM UI. (Omkar Vinit
    Joshi via vinodkv)

    YARN-1141. Updating resource requests should be decoupled with updating
    blacklist (Zhijie Shen via bikas)

    YARN-876. Node resource is added twice when node comes back from unhealthy
    to healthy. (Peng Zhang via Sandy Ryza)

    YARN-890. Ensure CapacityScheduler doesn't round-up metric for available 
    resources. (Xuan Gong & Hitesh Shah via acmurthy)

    YARN-621. Changed YARN web app to not add paths that can cause duplicate
    additions of authenticated filters there by causing kerberos replay errors.
    (Omkar Vinit Joshi via vinodkv)

    YARN-1236. FairScheduler setting queue name in RMApp is not working.
    (Sandy Ryza)

    YARN-1256. NM silently ignores non-existent service in
    StartContainerRequest (Xuan Gong via bikas)

    YARN-1149. NM throws InvalidStateTransitonException: Invalid event:
    APPLICATION_LOG_HANDLING_FINISHED at RUNNING (Xuan Gong via hitesh)

    YARN-1271. "Text file busy" errors launching containers again
    (Sandy Ryza)

    YARN-1131. $yarn logs command should return an appropriate error message if
    YARN application is still running. (Siddharth Seth via hitesh)

    YARN-1219. FSDownload changes file suffix making FileUtil.unTar() throw
    exception. (Shanyu Zhao via cnauroth)

    YARN-1251. TestDistributedShell#TestDSShell failed with timeout. (Xuan Gong
    via hitesh)

    YARN-1167. Fixed Distributed Shell to not incorrectly show empty hostname
    on RM UI. (Xuan Gong via vinodkv)

    YARN-1254. Fixed NodeManager to not pollute container's credentials. (Omkar
    Vinit Joshi via vinodkv)

    YARN-1273. Fixed Distributed-shell to account for containers that failed
    to start. (Hitesh Shah via vinodkv)

    YARN-1032. Fixed NPE in RackResolver. (Lohit Vijayarenu via acmurthy)

    YARN-1090. Fixed CS UI to better reflect applications as non-schedulable
    and not as pending. (Jian He via acmurthy)

    YARN-1274. Fixed NodeManager's LinuxContainerExecutor to create user, app-dir
    and log-dirs correctly even when there are no resources to localize for the
    container. (Siddharth Seth via vinodkv)

    YARN-1278. Fixed NodeManager to not delete local resources for apps on resync
    command from RM - a bug caused by YARN-1149. (Hitesh Shah via vinodkv)

    YARN-1463. Tests should avoid starting http-server where possible or creates 
    spnego keytab/principals (vinodkv via kasha)

Release 2.1.1-beta - 2013-09-23

  INCOMPATIBLE CHANGES

    YARN-707. Added user information also in the YARN ClientToken so that AMs
    can implement authorization based on incoming users. (Jason Lowe via vinodkv)

    YARN-1170. YARN & MapReduce proto definitions fixed to specify protobuf
    package as hadoop.yarn and hadoop.mapreduce respectively. (Binglin Chang
    via acmurthy)

  NEW FEATURES

  IMPROVEMENTS

    YARN-589. Expose a REST API for monitoring the fair scheduler (Sandy Ryza).

    YARN-1074. Cleaned up YARN CLI application list to only display running
    applications by default. (Xuan Gong via vinodkv)
    
    YARN-1093. Corrections to Fair Scheduler documentation (Wing Yew Poon via
    Sandy Ryza)

    YARN-942. In Fair Scheduler documentation, inconsistency on which
    properties have prefix (Akira Ajisaka via Sandy Ryza)

    YARN-1083. Changed ResourceManager to fail when the expiry interval is less
    than the configured node-heartbeat interval. (Zhijie Shen via vinodkv)

    YARN-1081. Made a trivial change to YARN node CLI header to avoid potential
    confusion. (Akira AJISAKA via vinodkv)

    YARN-1034. Remove "experimental" in the Fair Scheduler documentation.
    (Karthik Kambatla via Sandy Ryza)

    YARN-1080. Improved help message for "yarn logs" command. (Xuan Gong via
    vinodkv)

    YARN-771. AMRMClient support for resource blacklisting (Junping Du via
    bikas)

    YARN-1117. Improved help messages for "yarn application" and "yarn node"
    commands. (Xuan Gong via vinodkv)

    YARN-1120. Made ApplicationConstants.Environment.USER definition OS neutral
    as the corresponding value is now set correctly end-to-end. (Chuan Liu via
    vinodkv)

    YARN-1124. Modified YARN CLI application list to display new and submitted
    applications together with running apps by default, following up YARN-1074.
    (Xuan Gong via vinodkv)

    YARN-1065. NM should provide AuxillaryService data to the container (Xuan
    Gong via bikas)

    YARN-758. Augment MockNM to use multiple cores (Karthik Kambatla via
    Sandy Ryza)

    YARN-696. Changed RMWebservice apps call to take in multiple application
    states. (Trevor Lorimer via vinodkv)

    YARN-910. Augmented auxiliary services to listen for container starts and
    completions in addition to application events. (Alejandro Abdelnur via
    vinodkv)

    YARN-1137. Add support whitelist for system users to Yarn 
    container-executor.c. (rvs via tucu)

    YARN-1001. Added a web-service to get statistics about per application-type
    per state for consumption by downstream projects. (Zhijie Shen via vinodkv)

    YARN-1203. Changed YARN web-app proxy to handle http and https URLs from
    AM registration and finish correctly. (Omkar Vinit Joshi via vinodkv)

    YARN-1204. Added separate configuration properties for https for RM and NM
    without which servers enabled with https will also start on http ports.
    (Omkar Vinit Joshi via vinodkv)

  OPTIMIZATIONS

  BUG FIXES

    YARN-948. Changed ResourceManager to validate the release container list
    before actually releasing them. (Omkar Vinit Joshi via vinodkv)

    YARN-966. Fixed ContainerLaunch to not fail quietly when there are no
    localized resources due to some other failure. (Zhijie Shen via vinodkv)

    YARN-502. Fixed a state machine issue with RMNode inside ResourceManager
    which was crashing scheduler. (Mayank Bansal via vinodkv)

    YARN-573. Shared data structures in Public Localizer and Private Localizer
    are not Thread safe. (Omkar Vinit Joshi via jlowe)

    YARN-903. Changed ContainerManager to suppress unnecessary warnings when
    stopping already stopped containers. (Omkar Vinit Joshi via vinodkv)

    YARN-906. Fixed a bug in NodeManager where cancelling ContainerLaunch at
    KILLING state causes that the container to hang. (Zhijie Shen via vinodkv)

    YARN-994. HeartBeat thread in AMRMClientAsync does not handle runtime
    exception correctly (Xuan Gong via bikas)

    YARN-337. RM handles killed application tracking URL poorly (jlowe)

    YARN-107. Fixed ResourceManager and clients to better handle
    forceKillApplication on non-running and finished applications. (Xuan Gong
    via vinodkv)

    YARN-643. Fixed ResourceManager to remove all tokens consistently on app
    finish. (Xuan Gong via vinodkv)

    YARN-1006. Fixed broken rendering in the Nodes list web page on the RM web
    UI. (Xuan Gong via vinodkv)

    YARN-881. Priority#compareTo method seems to be wrong. (Jian He via bikas)

    YARN-1082. Create base directories on HDFS after RM login to ensure RM
    recovery doesn't fail in secure mode. (vinodkv via acmurthy)

    YARN-1085. Modified YARN and MR2 web-apps to do HTTP authentication in
    secure setup with kerberos. (Omkar Vinit Joshi via vinodkv)

    YARN-1094. Fixed a blocker with RM restart code because of which RM crashes
    when try to recover an existing app. (vinodkv)

    YARN-1008. MiniYARNCluster with multiple nodemanagers, all nodes have same 
    key for allocations. (tucu)

    YARN-981. Fixed YARN webapp so that /logs servlet works like before. (Jian He
    via vinodkv)

    YARN-602. Fixed NodeManager to not let users override some mandatory 
    environmental variables. (Kenji Kikushima  via vinodkv)

    YARN-1101. Active nodes can be decremented below 0 (Robert Parker 
    via tgraves)

    YARN-1077. Fixed TestContainerLaunch test failure on Windows. (Chuan Liu via
    vinodkv)

    YARN-957. Fixed a bug in CapacityScheduler because of which requests that
    need more than a node's total capability were incorrectly allocated on that
    node causing apps to hang. (Omkar Vinit Joshi via vinodkv)

    YARN-1107. Fixed a bug in ResourceManager because of which RM in secure mode
    fails to restart. (Omkar Vinit Joshi via vinodkv)

    YARN-1049. ContainerExistStatus should define a status for preempted 
    containers. (tucu)

    YARN-1144. Unmanaged AMs registering a tracking URI should not be 
    proxy-fied. (tucu)

    YARN-1152. Fixed a bug in ResourceManager that was causing clients to get
    invalid client token key errors when an appliation is about to finish.
    (Jason Lowe via vinodkv)

    YARN-292. Fixed FifoScheduler and FairScheduler to make their applications
    data structures thread safe to avoid RM crashing with
    ArrayIndexOutOfBoundsException. (Zhijie Shen via vinodkv)

    YARN-1025. ResourceManager and NodeManager do not load native libraries on
    Windows. (cnauroth)

    YARN-1176. RM web services ClusterMetricsInfo total nodes doesn't include 
    unhealthy nodes (Jonathan Eagles via tgraves)

    YARN-1078. TestNodeManagerResync, TestNodeManagerShutdown, and
    TestNodeStatusUpdater fail on Windows. (Chuan Liu via cnauroth)

    YARN-1194. TestContainerLogsPage fails with native builds (Roman Shaposhnik
    via jlowe)

    YARN-1116. Populate AMRMTokens back to AMRMTokenSecretManager after RM
    restarts (Jian He via bikas)

    YARN-1189. NMTokenSecretManagerInNM is not being told when applications
    have finished (Omkar Vinit Joshi via jlowe)

    YARN-540. Race condition causing RM to potentially relaunch already
    unregistered AMs on RM restart (Jian He via bikas)

    YARN-1184. ClassCastException during preemption enforcement. (cdouglas)

Release 2.1.0-beta - 2013-08-22

  INCOMPATIBLE CHANGES

    YARN-396. Rationalize AllocateResponse in RM Scheduler API. (Zhijie Shen
    via hitesh)

    YARN-439. Flatten NodeHeartbeatResponse. (Xuan Gong via sseth)

    YARN-440. Flatten RegisterNodeManagerResponse. (Xuan Gong via sseth)

    YARN-536. Removed the unused objects ContainerStatus and ContainerStatus from
    Container which also don't belong to the container. (Xuan Gong via vinodkv)

    YARN-486. Changed NM's startContainer API to accept Container record given by
    RM as a direct parameter instead of as part of the ContainerLaunchContext
    record. (Xuan Gong via vinodkv)

    YARN-444. Moved special container exit codes from YarnConfiguration to API
    where they belong. (Sandy Ryza via vinodkv)

    YARN-441. Removed unused utility methods for collections from two API
    records. (Xuan Gong via vinodkv)

    YARN-561. Modified NodeManager to set key information into the environment
    of every container that it launches. (Xuan Gong via vinodkv)

    YARN-579. Stop setting the Application Token in the AppMaster env, in
    favour of the copy present in the container token field. 
    (Vinod Kumar Vavilapalli via sseth)

    YARN-629. Make YarnRemoteException not be rooted at IOException. (Xuan Gong
    via vinodkv)

    YARN-633. Changed RMAdminProtocol api to throw IOException and
    YarnRemoteException. (Xuan Gong via vinodkv)

    YARN-632. Changed ContainerManager api to throw IOException and
    YarnRemoteException. (Xuan Gong via vinodkv)

    YARN-631. Changed ClientRMProtocol api to throw IOException and
    YarnRemoteException. (Xuan Gong via vinodkv)

    YARN-630. Changed AMRMProtocol api to throw IOException and
    YarnRemoteException. (Xuan Gong via vinodkv)

    YARN-615. Rename ContainerLaunchContext.containerTokens to tokens.
    (Vinod Kumar Vavilapalli via sseth)

    YARN-571. Remove user from ContainerLaunchContext. (Omkar Vinit Joshi via
    vinodkv)

    YARN-716. Making ApplicationID immutable. (Siddharth Seth via vinodkv)

    YARN-684. ContainerManager.startContainer should use
    ContainerTokenIdentifier instead of the entire Container.
    (Vinod Kumar Vavilapalli via sseth)

    YARN-735. Make ApplicationAttemptId, ContaienrId and NodeId immutable.
    (Jian He via sseth)

    YARN-749. Rename ResourceRequest.(get,set)HostName to
    ResourceRequest.(get,set)ResourceName. (acmurthy)

    YARN-720. container-log4j.properties should not refer to mapreduce
    property names. (Zhijie Shen via sseth)

    YARN-748. Moved BuilderUtils from yarn-common to yarn-server-common for
    eventual retirement. (Jian He via vinodkv)

    YARN-635. Renamed YarnRemoteException to YarnException. (Siddharth Seth via
    vinodkv)

    YARN-755. Renamed AllocateResponse.reboot to AllocateResponse.resync. (Bikas
    Saha via vinodkv)

    YARN-753. Added individual factory methods for all api protocol records and
    converted the records to be abstract classes. (Jian He via vinodkv)

    YARN-724. Moved ProtoBase from api.records to api.records.impl.pb. (Jian He
    via vinodkv)

    YARN-759. Create Command enum in AllocateResponse (bikas)

    YARN-777. Removed unreferenced objects from .proto files. (Jian He via
    vinodkv) 

    YARN-642. Removed health parameter from ResourceManager /nodes web-service
    and cleaned the behaviour of the status parameter. (Sandy Ryza vid vinodkv)

    YARN-530. Defined Service model strictly, implemented AbstractService for
    robust subclassing and migrated yarn-common services. (Steve Loughran via
    vinodkv)

    YARN-746. Renamed Service.register() and Service.unregister() to
    registerServiceListener() & unregisterServiceListener() respectively.
    (Steve Loughran via vinodkv)

    YARN-792. Moved NodeHealthStatus from yarn.api.record to
    yarn.server.api.record. (Jian He via vinodkv)

    YARN-806. Moved ContainerExitStatus from yarn.api to yarn.api.records. (Jian
    He via vinodkv)

    YARN-821. Renamed setFinishApplicationStatus to setFinalApplicationStatus in
    FinishApplicationMasterRequest for consistency. (Jian He via vinodkv)

    YARN-787. Removed minimum resource from RegisterApplicationMasterResponse.
    (tucu via acmurthy)

    YARN-829. Renamed RMTokenSelector to be RMDelegationTokenSelector. (Zhijie
    Shen via vinodkv)

    YARN-828. Removed the unsed YarnVersionAnnotation. (Zhijie Shen via vinodkv)

    YARN-823. Moved RMAdmin from yarn.client to yarn.client.cli and renamed it to
    be RMAdminCLI. (Jian He via vinodkv)

    YARN-387. Renamed YARN protocols for consistency.
    ClientRMProtocol -> ApplicationClientProtocol
    AMRMProtocol -> ApplicationMasterProtocol
    ContainerManager -> ContainerManagementProtocol
    (vinodkv via acmurthy)

    YARN-831. Removed minimum resource from GetNewApplicationResponse as a
    follow-up to YARN-787. (Jian He via acmurthy)

    YARN-824. Added static factory methods to hadoop-yarn-client interfaces. 
    (Jian He via acmurthy)

    YARN-826. Moved Clock and SystemClock into yarn.util package. (Zhijie Shen
    via vinodkv)

    YARN-837. Moved yarn.ClusterInfo into MapReduce project as it doesn't belong
    to YARN. (Zhijie Shen via vinodkv)

    YARN-822. Renamed ApplicationToken to be AMRMToken, and similarly the
    corresponding TokenSelector and SecretManager. (Omkar Vinit Joshi via vinodkv)

    YARN-610. ClientToken is no longer set in the environment of the Containers.
    (Omkar Vinit Joshi via vinodkv)

    YARN-834. Fixed annotations for yarn-client module, reorganized packages and
    clearly differentiated *Async apis. (Arun C Murthy and Zhijie Shen via
    vinodkv)

    YARN-840. Moved ProtoUtils to yarn.api.records.pb.impl. (Jian He via
    acmurthy) 

    YARN-841. Move Auxiliary service to yarn-api, annotate and document it.
    (vinodkv)

    YARN-850. Rename getClusterAvailableResources to getAvailableResources in
    AMRMClients (Jian He via bikas)

    YARN-694. Starting to use NMTokens to authenticate all communication with
    NodeManagers. (Omkar Vinit Joshi via vinodkv) 

    YARN-553. Replaced YarnClient.getNewApplication with
    YarnClient.createApplication which provides a directly usable
    ApplicationSubmissionContext to simplify the api. (Karthik Kambatla via
    acmurthy) 

    YARN-851. Share NMTokens using NMTokenCache (api-based) between AMRMClient
    and NMClient instead of memory based approach which is used currently. (Omkar
    Vinit Joshi via vinodkv)

    YARN-869. Move ResourceManagerAdministrationProtocol out of main YARN api.
    (vinodkv via acmurthy)

    YARN-791. Changed RM APIs and web-services related to nodes to ensure that
    both are consistent with each other. (Sandy Ryza via vinodkv)

    YARN-727. ClientRMProtocol.getAllApplications should accept ApplicationType as
    a parameter. (Xuan Gong via hitesh)

    YARN-701. Use application tokens irrespective of secure or non-secure
    mode. (vinodkv via acmurthy)

    YARN-918. Remove ApplicationAttemptId from
    RegisterApplicationMasterRequestProto. (vinodkv via acmurthy)

    YARN-926. Modified ContainerManagerProtcol APIs to take in requests for
    multiple containers. (Jian He via vinodkv)

  NEW FEATURES

    YARN-482. FS: Extend SchedulingMode to intermediate queues. 
    (kkambatl via tucu)

    YARN-45. Add protocol for schedulers to request containers back from
    ApplicationMasters. (Carlo Curino, cdouglas)

    YARN-563. Add the concept of an application-type for each application.
    (Mayank Bansal via vinodkv)

    HADOOP-8562. Enhancements to support Hadoop on Windows Server and Windows
    Azure environments. (See breakdown of tasks below for subtasks and
    contributors)

    YARN-422. Add a NM Client library to help application-writers. (Zhijie Shen
    via vinodkv)

    YARN-392. Make it possible to specify hard locality constraints in resource
    requests. (sandyr via tucu)

    YARN-326. Add multi-resource scheduling to the fair scheduler. 
    (sandyr via tucu)

    YARN-398. Make it possible to specify hard locality constraints in resource
    requests for CapacityScheduler. (acmurthy)

    YARN-781. Exposing LOGDIR in all containers' environment which should be used
    by containers for logging purposes. (Jian He via vinodkv)

  IMPROVEMENTS

    YARN-347. Node CLI should show CPU info besides memory in node status.
    (Junping Du via llu)

    YARN-365. Change NM heartbeat handling to not generate a scheduler event
    on each heartbeat. (Xuan Gong via sseth)

    YARN-380. Fix yarn node -status output to be better readable. (Omkar Vinit
    Joshi via vinodkv)

    YARN-410. Fixed RM UI so that the new lines diagnostics for a failed app on
    the per-application page are translated to html line breaks. (Omkar Vinit
    Joshi via vinodkv)

    YARN-198. Added a link to RM pages from the NodeManager web app. (Jian He
    via vinodkv)

    YARN-237. Refreshing the RM page forgets how many rows I had in my
    Datatables (jian he via bobby)

    YARN-481. Add AM Host and RPC Port to ApplicationCLI Status Output 
    (Chris Riccomini via bikas)

    YARN-297. Improve hashCode implementations for PB records. (Xuan Gong via
    hitesh)

    YARN-417. Create AMRMClient wrapper that provides asynchronous callbacks.
    (Sandy Ryza via bikas)

    YARN-497. Yarn unmanaged-am launcher jar does not define a main class in
    its manifest (Hitesh Shah via bikas)

    YARN-469. Make scheduling mode in FS pluggable. (kkambatl via tucu)

    YARN-450. Define value for * in the scheduling protocol (Zhijie Shen via
    bikas)

    YARN-475. Remove a unused constant in the public API -
    ApplicationConstants.AM_APP_ATTEMPT_ID_ENV. (Hitesh Shah via vinodkv)

    YARN-309. Changed NodeManager to obtain heart-beat interval from the
    ResourceManager. (Xuan Gong via vinodkv)

    YARN-447. Move ApplicationComparator in CapacityScheduler to use comparator
    in ApplicationId. (Nemon Lou via vinodkv)

    YARN-381. Improve fair scheduler docs. (Sandy Ryza via tomwhite)

    YARN-458. YARN daemon addresses must be placed in many different configs. 
    (sandyr via tucu)

    YARN-193. Scheduler.normalizeRequest does not account for allocation
    requests that exceed maximumAllocation limits (Zhijie Shen via bikas)

    YARN-479. NM retry behavior for connection to RM should be similar for
    lost heartbeats (Jian He via bikas)

    YARN-495. Changed NM reboot behaviour to be a simple resync - kill all
    containers  and re-register with RM. (Jian He via vinodkv)

    YARN-514. Delayed store operations should not result in RM unavailability
    for app submission (Zhijie Shen via bikas)

    YARN-586. Fixed a typo in ApplicationSubmissionContext#setApplicationId.
    (Zhijie Shen via vinodkv)

    YARN-542. Changed the default global AM max-attempts value to be not one.
    (Zhijie Shen via vinodkv)

    YARN-583. Moved application level local resources to be localized under the
    filecache sub-directory under application directory. (Omkar Vinit Joshi via
    vinodkv)

    YARN-581. Added a test to verify that app delegation tokens are restored
    after RM restart. (Jian He via vinodkv)

    YARN-577. Add application-progress also to ApplicationReport. (Hitesh Shah
    via vinodkv)

    YARN-595. Refactor fair scheduler to use common Resources. (Sandy Ryza
    via tomwhite)

    YARN-562. Modified NM to reject any containers allocated by a previous
    ResourceManager. (Jian He via vinodkv)

    YARN-591. Moved RM recovery related records out of public API as they do not
    belong there. (vinodkv)

    YARN-599. Refactoring submitApplication in ClientRMService and RMAppManager
    to separate out various validation checks depending on whether they rely on
    RM configuration or not. (Zhijie Shen via vinodkv)

    YARN-618. Modified RM_INVALID_IDENTIFIER to be -1 instead of zero. (Jian He
    via vinodkv)

    YARN-625. Move the utility method unwrapAndThrowException from
    YarnRemoteExceptionPBImpl to RPCUtil. (Siddharth Seth via vinodkv)

    YARN-645. Moved RMDelegationTokenSecretManager from yarn-server-common to
    yarn-server-resourcemanager where it really belongs. (Jian He via vinodkv)

    YARN-651. Changed PBClientImpls of ContainerManager and RMAdmin to throw
    IOExceptions also. (Xuan Gong via vinodkv)

    YARN-582. Changed ResourceManager to recover Application token and client
    tokens for app attempt so that RM can be restarted while preserving current
    applications. (Jian He via vinodkv)

    YARN-568. Add support for work preserving preemption to the FairScheduler.
    (Carlo Curino and Sandy Ryza via cdouglas)

    YARN-598. Add virtual cores to queue metrics. (sandyr via tucu)

    YARN-634. Modified YarnRemoteException to be not backed by PB and introduced
    a separate SerializedException record. (Siddharth Seth via vinodkv)

    YARN-663. Changed ResourceTracker API and LocalizationProtocol API to throw
    YarnRemoteException and IOException. (Xuan Gong via vinodkv)

    YARN-590. Added an optional mesage to be returned by ResourceMaanger when RM
    asks an RM to shutdown/resync etc so that NMs can log this message locally
    for better debuggability. (Mayank Bansal via vinodkv)

    YARN-617. Made ContainerTokens to be used for validation at NodeManager
    also in unsecure mode to prevent AMs from faking resource requirements in
    unsecure mode. (Omkar Vinit Joshi via vinodkv)

    YARN-708. Moved RecordFactory classes to hadoop-yarn-api, and put some
    miscellaneous fixes to the interfaces. (Siddharth Seth via vinodkv)

    YARN-711. Copied BuilderUtil methods in individual API records as
    BuilderUtils is going to be dismantled. (Jian He via vinodkv)

    YARN-714. Added NMTokens to be sent to AMs as part of heart-beat response.
    (Omkar Vinit Joshi via vinodkv)

    YARN-638. Modified ResourceManager to restore RMDelegationTokens after
    restarting. (Jian He via vinodkv)

    YARN-660. Improve AMRMClient with matching requests (bikas)

    YARN-717. Put object creation factories for Token in the class itself and
    remove useless derivations for specific tokens. (Jian He via vinodkv)

    YARN-756. Move Preemption* records to yarn.api where they really belong.
    (Jian He via vinodkv)

    YARN-750. Allow for black-listing resources in YARN API and Impl in CS
    (acmurthy via bikas)

    YARN-877. Support resource blacklisting for FifoScheduler.
    (Junping Du via llu)

    YARN-686. Flatten NodeReport. (sandyr via tucu)

    YARN-737. Throw some specific exceptions directly instead of wrapping them
    in YarnException. (Jian He via sseth)

    YARN-731. RPCUtil.unwrapAndThrowException should unwrap remote
    RuntimeExceptions. (Zhijie Shen via sseth)

    YARN-600. Hook up cgroups CPU settings to the number of virtual cores 
    allocated. (sandyr via tucu)

    YARN-648. FS: Add documentation for pluggable policy. (kkambatl via tucu)

    YARN-773. Moved YarnRuntimeException from package api.yarn to
    api.yarn.exceptions. (Jian He via vinodkv)

    YARN-692. Creating NMToken master key on RM and sharing it with NM as a part
    of RM-NM heartbeat. (Omkar Vinit Joshi via vinodkv)

    YARN-782. vcores-pcores ratio functions differently from vmem-pmem ratio in 
    misleading way. (sandyr via tucu)

    YARN-803. factor out scheduler config validation from the ResourceManager 
    to each scheduler implementation. (tucu)

    YARN-789. Enable zero capabilities resource requests in fair scheduler. 
    (tucu)

    YARN-639. Modified Distributed Shell application to start using the new
    NMClient library. (Zhijie Shen via vinodkv)

    YARN-693. Modified RM to send NMTokens on allocate call so that AMs can then
    use them for authentication with NMs. (Omkar Vinit Joshi via vinodkv)

    YARN-752. In AMRMClient, automatically add corresponding rack requests for 
    requested nodes. (sandyr via tucu)

    YARN-825. Fixed javadoc and annotations for yarn-common module. (vinodkv)

    YARN-833. Moved Graph and VisualizeStateMachine into yarn.state package.
    (Zhijie Shen via vinodkv)

    YARN-805. Fix javadoc and annotations on classes in the yarn-api
    package. (Jian He via sseth)

    YARN-846.  Move pb Impl classes from yarn-api to yarn-common. (Jian He via
    vinodkv)

    YARN-827. Need to make Resource arithmetic methods accessible (Jian He via
    bikas)

    YARN-866. Add test for class ResourceWeights. (ywskycn via tucu)

    YARN-736. Add a multi-resource fair sharing metric. (sandyr via tucu)

    YARN-883. Expose Fair Scheduler-specific queue metrics. (sandyr via tucu)

    YARN-569. Add support for requesting and enforcing preemption requests via
    a capacity monitor. (Carlo Curino, cdouglas)

    YARN-521. Augment AM - RM client module to be able to request containers
    only at specific locations (Sandy Ryza via bikas)

    YARN-513. Create common proxy client for communicating with RM. (Xuan Gong
    & Jian He via bikas)

    YARN-927. Change ContainerRequest to not have more than 1 container count
    and remove StoreContainerRequest (bikas)

    YARN-922. Change FileSystemRMStateStore to use directories (Jian He via
    bikas)

    YARN-865. RM webservices can't query based on application Types. (Xuan Gong
    via hitesh)

    YARN-912. Move client facing exceptions to yarn-api module. (Mayank Bansal
    via vinodkv)

    YARN-84. Use Builder to build RPC server. (Brandon Li via szetszwo)

    YARN-1046. Disable mem monitoring by default in MiniYARNCluster. (Karthik
    Kambatla via Sandy Ryza)

    YARN-1045. Improve toString implementation for PBImpls. (Jian He via sseth)  

  OPTIMIZATIONS

    YARN-512. Log aggregation root directory check is more expensive than it
    needs to be. (Maysam Yabandeh via jlowe)

    YARN-719. Move RMIdentifier from Container to ContainerTokenIdentifier.
    (Vinod Kumar Vavilapalli via sseth)

  BUG FIXES

    YARN-383. AMRMClientImpl should handle null rmClient in stop()
    (Hitesh Shah via sseth)

    YARN-385. Add missing fields - location and #containers to
    ResourceRequestPBImpl's toString(). (Sandy Ryza via sseth)

    YARN-377. Use the new StringUtils methods added by HADOOP-9252 and fix
    TestContainersMonitor.  (Chris Nauroth via szetszwo)

    YARN-391. Formatting fixes for LCEResourceHandler classes.
    (Steve Loughran via sseth)

    YARN-390. ApplicationCLI and NodeCLI hard-coded platform-specific line
    separator causes test failures on Windows. (Chris Nauroth via suresh)

    YARN-406. Fix TestRackResolver to function in networks where "host1"
    resolves to a valid host. (Hitesh Shah via sseth)

    YARN-376. Fixes a bug which would prevent the NM knowing about completed
    containers and applications. (Jason Lowe via sseth)

    YARN-196. Nodemanager should be more robust in handling connection failure
    to ResourceManager when a cluster is started (Xuan Gong via hitesh)

    YARN-485. TestProcfsProcessTree#testProcessTree() doesn't wait long enough 
    for the process to die. (kkambatl via tucu)
 
    YARN-71. Fix the NodeManager to clean up local-dirs on restart.
    (Xuan Gong via sseth)

    YARN-378. Fix RM to make the AM max attempts/retries to be configurable
    per application by clients. (Zhijie Shen via vinodkv)

    YARN-498. Unmanaged AM launcher does not set various constants in env for
    an AM, also does not handle failed AMs properly. (Hitesh Shah via bikas)

    YARN-496. Fair scheduler configs are refreshed inconsistently in
    reinitialize. (Sandy Ryza via tomwhite)

    YARN-474. Fix CapacityScheduler to trigger application-activation when
    am-resource-percent configuration is refreshed. (Zhijie Shen via vinodkv)

    YARN-209. Fix CapacityScheduler to trigger application-activation when
    the cluster capacity changes. (Zhijie Shen via vinodkv)

    YARN-24. Nodemanager fails to start if log aggregation enabled and 
    namenode unavailable. (sandyr via tucu)

    YARN-515. Node Manager not getting the master key. (Robert Joseph Evans
    via jlowe)

    YARN-382. SchedulerUtils improve way normalizeRequest sets the resource
    capabilities. (Zhijie Shen via bikas)

    YARN-467. Modify public distributed cache to localize files such that no
    local directory hits unix file count limits and thus prevent job failures.
    (Omkar Vinit Joshi via vinodkv)

    YARN-101. Fix NodeManager heartbeat processing to not lose track of completed
    containers in case of dropped heartbeats. (Xuan Gong via vinodkv)

    YARN-538. RM address DNS lookup can cause unnecessary slowness on every JHS 
    page load. (sandyr via tucu)

    YARN-532. Change RMAdmin and Localization client protocol PB implementations
    to implement closeable so that they can be stopped when needed via
    RPC.stopProxy(). (Siddharth Seth via vinodkv)

    YARN-99. Modify private distributed cache to localize files such that no
    local directory hits unix file count limits and thus prevent job failures.
    (Omkar Vinit Joshi via vinodkv)

    YARN-112. Fixed a race condition during localization that fails containers.
    (Omkar Vinit Joshi via vinodkv)

    YARN-534. Change RM restart recovery to also account for AM max-attempts 
    configuration after the restart. (Jian He via vinodkv)

    YARN-539. Addressed memory leak of LocalResource objects NM when a resource
    localization fails. (Omkar Vinit Joshi via vinodkv)

    YARN-319. Submitting a job to a fair scheduler queue for which the user
    does not have permission causes the client to wait forever.
    (shenhong via tomwhite)

    YARN-412. Fixed FifoScheduler to check hostname of a NodeManager rather
    than its host:port during scheduling which caused incorrect locality for
    containers. (Roger Hoover via acmurthy)

    YARN-500. Fixed YARN webapps to not roll-over ports when explicitly asked
    to use non-ephemeral ports. (Kenji Kikushima via vinodkv)

    YARN-518. Fair Scheduler's document link could be added to the hadoop 2.x 
    main doc page. (sandyr via tucu)

    YARN-476. ProcfsBasedProcessTree info message confuses users. 
    (sandyr via tucu)

    YARN-585. Fix failure in TestFairScheduler#testNotAllowSubmitApplication
    caused by YARN-514. (Zhijie Shen via vinodkv)

    YARN-547. Fixed race conditions in public and private resource localization
    which used to cause duplicate downloads. (Omkar Vinit Joshi via vinodkv)

    YARN-594. Update test and add comments in YARN-534 (Jian He via bikas)

    YARN-549. YarnClient.submitApplication should wait for application to be
    accepted by the RM (Zhijie Shen via bikas)

    YARN-605. Fix failing unit test in TestNMWebServices when versionInfo has
    parantheses like when running on a git checkout. (Hitesh Shah via vinodkv)

    YARN-289. Fair scheduler allows reservations that won't fit on node.
    (Sandy Ryza via tomwhite)

    YARN-576. Modified ResourceManager to reject NodeManagers that don't satisy
    minimum resource requirements. (Kenji Kikushima via vinodkv)

    YARN-646. Fix two typos in Fair Scheduler user guide. (Dapeng Sun via atm)

    YARN-507. Add interface visibility and stability annotations to FS 
    interfaces/classes. (kkambatl via tucu)

    YARN-637. FS: maxAssign is not honored. (kkambatl via tucu)

    YARN-655. Fair scheduler metrics should subtract allocated memory from 
    available memory. (sandyr via tucu)

    YARN-628. Fix the way YarnRemoteException is being unrolled to extract out
    the underlying exception. (Siddharth Seth via vinodkv)

    YARN-695. Remove masterContainer and status unused fields from
    ApplicationReportProto and fix bugs in ApplicationReportPBImpl. (Zhijie Shen
    via vinodkv)

    YARN-706. Fixed race conditions in TestFSDownload. (Zhijie Shen via vinodkv).

    YARN-715. Fixed unit test failures - TestDistributedShell and
    TestUnmanagedAMLauncher. (Vinod Kumar Vavilapalli via sseth)

    YARN-578. Fixed NM to use SecureIOUtils for reading and aggregating logs.
    (Omkar Vinit Joshi via vinodkv) 

    YARN-733. Fixed TestNMClient from failing occasionally. (Zhijie Shen via
    vinodkv)

    YARN-730. Fix NMClientAsync to remove completed containers. (Zhijie Shen
    via acmurthy)

    YARN-726. Fix queue & finish time fields in web-ui for ResourceManager.
    (Mayank Bansal via acmurthy) 

    YARN-757. Changed TestRMRestart to use the default scheduler to avoid test
    failures. (Bikas Saha via vinodkv)

    YARN-742. Log aggregation causes a lot of redundant setPermission calls.
    (jlowe via kihwal)

    YARN-764. blank Used Resources on Capacity Scheduler page (Nemon Lou via
    tgraves)

    YARN-761. TestNMClientAsync fails sometimes (Zhijie Shen via bikas)

    YARN-760. NodeManager throws AvroRuntimeException on failed start.
    (Niranjan Singh via jlowe)

    YARN-767. Initialize application metrics at RM bootup. (Jian He via
    acmurthy) 

    YARN-700. TestInfoBlock fails on Windows because of line ending missmatch.
    (Ivan Mitic via cnauroth)

    YARN-117. Migrated rest of YARN to the new service model. (Steve Louhran via
    vinodkv)

    YARN-812. Set default logger for application summary logger to
    hadoop.root.logger. (sseth via acmurthy)

    YARN-848. Nodemanager does not register with RM using the fully qualified
    hostname. (Hitesh Shah via sseth)

    YARN-854. Fixing YARN bugs that are failing applications in secure
    environment. (Omkar Vinit Joshi via vinodkv)

    YARN-861. TestContainerManager is failing. (Vinod Kumar Vavilapalli via
    hitesh)

    YARN-874. Making common RPC to switch to not switch to simple when other
    mechanisms are enabled and thus fix YARN/MR test failures after HADOOP-9421.
    (Daryn Sharp and Vinod Kumar Vavilapalli via vinodkv)

    YARN-845. RM crash with NPE on NODE_UPDATE (Mayank Bansal via bikas)

    YARN-369. Handle ( or throw a proper error when receiving) status updates
    from application masters that have not registered (Mayank Bansal &
    Abhishek Kapoor via bikas)

    YARN-541. getAllocatedContainers() is not returning all the allocated
    containers (bikas)

    YARN-763. AMRMClientAsync should stop heartbeating after receiving
    shutdown from RM (Xuan Gong via bikas)

    YARN-654. AMRMClient: Perform sanity checks for parameters of public
    methods (Xuan Gong via bikas)"

    YARN-919. Document setting default heap sizes in yarn-env.sh (Mayank
    Bansal via hitesh)

    YARN-795. Fair scheduler queue metrics should subtract allocated vCores from 
    available vCores. (ywskycn via tucu) 

    YARN-799. Fix CgroupsLCEResourcesHandler to use /tasks instead of
    /cgroup.procs. (Chris Riccomini via acmurthy) 

    YARN-333. Schedulers cannot control the queue-name of an 
    application. (sandyr via tucu)

    YARN-368. Fixed a typo in error message in Auxiliary services. (Albert Chu
    via vinodkv)

    YARN-295. Fixed a race condition in ResourceManager RMAppAttempt state
    machine. (Mayank Bansal via vinodkv)

    YARN-523. Modified a test-case to validate container diagnostics on
    localization failures. (Jian He via vinodkv)

    YARN-661. Fixed NM to cleanup users' local directories correctly when
    starting up. (Omkar Vinit Joshi via vinodkv)

    YARN-820. Fixed an invalid state transition in NodeManager caused by failing
    resource localization. (Mayank Bansal via vinodkv)

    YARN-62. Modified NodeManagers to avoid AMs from abusing container tokens for
    repetitive container launches. (Omkar Vinit Joshi via vinodkv)

    YARN-814. Improving diagnostics when containers fail during launch due to
    various reasons like invalid env etc. (Jian He via vinodkv)

    YARN-897. Ensure child queues are ordered correctly to account for
    completed containers. (Djellel Eddine Difallah via acmurthy)

    YARN-853. Fixed CapacityScheduler's maximum-am-resource-percent to properly
    work beyond refreshing queues. (Devaraj K via vinodkv)

    YARN-873. YARNClient.getApplicationReport(unknownAppId) returns a null
    report (Xuan Gong via bikas)

    YARN-875. Application can hang if AMRMClientAsync callback thread has
    exception (Xuan Gong via bikas)

    YARN-968. RM admin commands don't work. (vinodkv via kihwal)

    YARN-688. Fixed NodeManager to properly cleanup containers when it is shut
    down. (Jian He via vinodkv)

    YARN-960. Fixed ResourceManager to propagate client-submitted credentials
    irrespective of security. (Daryn Sharp via vinodkv)

    YARN-937. Fix unmanaged AM in non-secure/secure setup post YARN-701. (tucu)

    YARN-932. TestResourceLocalizationService.testLocalizationInit can fail on
    JDK7. (Karthik Kambatla via Sandy Ryza)

    YARN-961. Changed ContainerManager to enforce Token auth irrespective of
    security. (Omkar Vinit Joshi via vinodkv)

    YARN-945. Removed setting of AMRMToken's service from ResourceManager
    and changed client libraries do it all the time and correctly. (vinodkv)

    YARN-656. In scheduler UI, including reserved memory in Memory Total can 
    make it exceed cluster capacity. (Sandy Ryza)

  BREAKDOWN OF HADOOP-8562/YARN-191 SUBTASKS AND RELATED JIRAS

    YARN-158. Yarn creating package-info.java must not depend on sh.
    (Chris Nauroth via suresh)

    YARN-176. Some YARN tests fail to find winutils. (Chris Nauroth via suresh)
    
    YARN-207. YARN distribution build fails on Windows. (Chris Nauroth via
    suresh)

    YARN-199. Yarn cmd line scripts for windows. (Ivan Mitic via suresh)

    YARN-213. YARN build script would be more readable using abspath.
    (Chris Nauroth via suresh)

    YARN-233. Added support for running containers in MS Windows to YARN. (Chris
    Nauroth via acmurthy)

    YARN-234. Added support for process tree and resource calculator in MS Windows 
    to YARN. (Chris Nauroth via acmurthy)

    YARN-259. Fix LocalDirsHandlerService to use Path rather than URIs. (Xuan
    Gong via acmurthy) 

    YARN-316. YARN container launch may exceed maximum Windows command line 
    length due to long classpath. (Chris Nauroth via suresh)

    YARN-359. Fixing commands for container signalling in Windows. (Chris Nauroth
    via vinodkv)

    YARN-506. Move to common utils FileUtil#setReadable/Writable/Executable and 
    FileUtil#canRead/Write/Execute. (Ivan Mitic via suresh)

    YARN-488. TestContainerManagerSecurity fails on Windows. (Chris Nauroth
    via hitesh)

    YARN-490. TestDistributedShell fails on Windows. (Chris Nauroth via hitesh)

    YARN-491. TestContainerLogsPage fails on Windows. (Chris Nauroth via hitesh)

    YARN-487. Modify path manipulation in LocalDirsHandlerService to let
    TestDiskFailures pass on Windows. (Chris Nauroth via vinodkv)

    YARN-593. container launch on Windows does not correctly populate
    classpath with new process's environment variables and localized resources
    (Chris Nauroth via bikas)

    YARN-493. Fixed some shell related flaws in YARN on Windows. (Chris Nauroth
    via vinodkv)

    YARN-839. TestContainerLaunch.testContainerEnvVariables fails on Windows.
    (Chuan Liu via cnauroth)

    YARN-597. TestFSDownload fails on Windows due to dependencies on
    tar/gzip/jar tools. (Ivan Mitic via acmurthy) 

    YARN-852. TestAggregatedLogFormat.testContainerLogsFileAccess fails on
    Windows. (Chuan Liu via cnauroth)

    YARN-894. NodeHealthScriptRunner timeout checking is inaccurate on Windows.
    (Chuan Liu via cnauroth)

    YARN-909. Disable TestLinuxContainerExecutorWithMocks on Windows. (Chuan Liu
    via cnauroth)

    YARN-1043. Push all metrics consistently. (Jian He via acmurthy) 

    YARN-1056. Remove dual use of string 'resourcemanager' in
    yarn.resourcemanager.connect.{max.wait.secs|retry_interval.secs}
    (Karthik Kambatla via acmurthy)

Release 2.0.6-alpha - 08/22/2013

  INCOMPATIBLE CHANGES

  NEW FEATURES

  IMPROVEMENTS

  OPTIMIZATIONS

  BUG FIXES

    YARN-854. Fixing YARN bugs that are failing applications in secure
    environment. (Omkar Vinit Joshi and shv)

Release 2.0.5-alpha - 06/06/2013

  INCOMPATIBLE CHANGES

  NEW FEATURES

  IMPROVEMENTS

  OPTIMIZATIONS

  BUG FIXES

Release 2.0.4-alpha - 2013-04-25 

  INCOMPATIBLE CHANGES

  NEW FEATURES

  IMPROVEMENTS

  OPTIMIZATIONS

  BUG FIXES

    YARN-429. capacity-scheduler config missing from yarn-test artifact.
    (sseth via hitesh)

    YARN-470. Support a way to disable resource monitoring on the NodeManager.
    (Siddharth Seth via hitesh)
    
Release 2.0.3-alpha - 2013-02-06 

  INCOMPATIBLE CHANGES

  NEW FEATURES

    YARN-145. Add a Web UI to the fair share scheduler. (Sandy Ryza via tomwhite)

    YARN-3. Add support for CPU isolation/monitoring of containers. 
    (adferguson via tucu)

    YARN-230. RM Restart phase 1 - includes support for saving/restarting all
    applications on an RM bounce. (Bikas Saha via acmurthy)

    YARN-103. Add a yarn AM-RM client module. (Bikas Saha via sseth)

    YARN-286. Add a YARN ApplicationClassLoader. (tomwhite)

    YARN-2. Enhanced CapacityScheduler to account for CPU alongwith memory for
    multi-dimensional resource scheduling. (acmurthy)

    YARN-328. Use token request messages defined in hadoop common. (suresh)

    YARN-231. RM Restart - Add FS-based persistent store implementation for
    RMStateStore (Bikas Saha via hitesh)

  IMPROVEMENTS

    YARN-223. Update process tree instead of getting new process trees.
    (Radim Kolar via llu)

    YARN-57. Allow process-tree based resource calculation et al. to be
    pluggable to support it on multiple platforms. (Radim Kolar via acmurthy)

    YARN-78. Changed UnManagedAM application to use YarnClient. (Bikas Saha via
    vinodkv)

    YARN-93. Fixed RM to propagate diagnostics from applications that have
    finished but failed (Jason Lowe via vinodkv). 

    YARN-28. Fixed TestCompositeService to not depend on test-order and thus
    made it pass on JDK7 (Thomas Graves via vinodkv).

    YARN-82. Change the default local and log dirs to be based on
    hadoop.tmp.dir and yarn.log.dir. (Hemanth Yamijala via sseth)

    YARN-53. Added the missing getGroups API to ResourceManager. (Bo Wang via
    vinodkv)

    YARN-116. Add the ability to change the RM include/exclude file without
    a restart. (xieguiming and Harsh J via sseth)

    YARN-23. FairScheduler: FSQueueSchedulable#updateDemand() - potential 
    redundant aggregation. (kkambatl via tucu)

    YARN-127. Move RMAdmin tool to its correct location - the client module.
    (vinodkv)

    YARN-40. Provided support for missing YARN commands (Devaraj K and Vinod
    Kumar Vavilapalli via vinodkv)

    YARN-33. Change LocalDirsHandlerService to validate the configured local and
    log dirs. (Mayank Bansal via sseth)

    YARN-94. Modify DistributedShell to point to main-class by default, clean up
    the help message, and hard-code the AM class. (Hitesh Shah via vinodkv)

    YARN-146. Add unit tests for computing fair share in the fair scheduler.
    (Sandy Ryza via tomwhite)

    HADOOP-8911. CRLF characters in source and text files.
    (Raja Aluri via suresh)

    YARN-136. Make ClientToAMTokenSecretManager part of RMContext (Vinod Kumar
    Vavilapalli via sseth)

    YARN-183. Clean up fair scheduler code. (Sandy Ryza via tomwhite)

    YARN-129. Simplify classpath construction for mini YARN tests. (tomwhite)

    YARN-254. Update fair scheduler web UI for hierarchical queues. 
    (sandyr via tucu)

    YARN-315. Using the common security token protobuf definition from hadoop
    common. (Suresh Srinivas via vinodkv) 

    YARN-170. Change NodeManager stop to be reentrant. (Sandy Ryza via vinodkv)

    YARN-331. Fill in missing fair scheduler documentation. (sandyr via tucu)

    YARN-277. Use AMRMClient in DistributedShell to exemplify the approach.
    (Bikas Saha via hitesh)

    YARN-360. Allow apps to concurrently register tokens for renewal.
    (Daryn Sharp via sseth)

  OPTIMIZATIONS

  BUG FIXES 
    
    YARN-131. Fix incorrect ACL properties in capacity scheduler documentation.
    (Ahmed Radwan via sseth)

    YARN-102. Move the apache header to the top of the file in MemStore.java.
    (Devaraj K via sseth)
    
    YARN-134. ClientToAMSecretManager creates keys without checking for
    validity of the appID. (Vinod Kumar Vavilapalli via sseth)

    YARN-30. Fixed tests verifying web-services to work on JDK7. (Thomas Graves
    via vinodkv)

    YARN-150. Fixes AppRejectedTransition does not unregister a rejected
    app-attempt from the ApplicationMasterService (Bikas Saha via sseth)

    YARN-140. Add capacity-scheduler-default.xml to provide a default set of
    configurations for the capacity scheduler. (ahmed via tucu)

    YARN-179. Fix some unit test failures. (Vinod Kumar Vavilapalli via sseth)

    YARN-181. Fixed eclipse settings broken by capacity-scheduler.xml move via
    YARN-140. (Siddharth Seth via vinodkv)

    YARN-169. Update log4j.appender.EventCounter to use
    org.apache.hadoop.log.metrics.EventCounter (Anthony Rojas via tomwhite)

    YARN-184. Remove unnecessary locking in fair scheduler, and address 
    findbugs excludes. (sandyr via tucu)

    YARN-224. Fair scheduler logs too many nodeUpdate INFO messages.
    (Sandy Ryza via tomwhite)

    YARN-222. Fair scheduler should create queue for each user by default.
    (Sandy Ryza via tomwhite)

    MAPREDUCE-4778. Fair scheduler event log is only written if directory
    exists on HDFS. (Sandy Ryza via tomwhite)

    YARN-229. Remove old unused RM recovery code. (Bikas Saha via acmurthy) 

    YARN-187. Add hierarchical queues to the fair scheduler.
    (Sandy Ryza via tomwhite)

    YARN-72. NM should handle cleaning up containers when it shuts down.
    (Sandy Ryza via tomwhite)

    YARN-267. Fix fair scheduler web UI. (Sandy Ryza via tomwhite)

    YARN-264. y.s.rm.DelegationTokenRenewer attempts to renew token even 
    after removing an app. (kkambatl via tucu)

    YARN-271. Fair scheduler hits IllegalStateException trying to reserve
    different apps on same node. (Sandy Ryza via tomwhite)

    YARN-272. Fair scheduler log messages try to print objects without 
    overridden toString methods. (sandyr via tucu)

    YARN-278. Fair scheduler maxRunningApps config causes no apps to make
    progress. (sandyr via tucu)

    YARN-282. Fair scheduler web UI double counts Apps Submitted. 
    (sandyr via tucu)

    YARN-283. Fair scheduler fails to get queue info without root prefix. 
    (sandyr via tucu)

    YARN-192. Node update causes NPE in the fair scheduler.
    (Sandy Ryza via tomwhite)

    YARN-288. Fair scheduler queue doesn't accept any jobs when ACLs are
    configured. (Sandy Ryza via tomwhite)

    YARN-300. After YARN-271, fair scheduler can infinite loop and not
    schedule any application. (Sandy Ryza via tomwhite)

    YARN-301. Fair scheduler throws ConcurrentModificationException when
    iterating over app's priorities. (Sandy Ryza via tomwhite)

    YARN-217. Fix RMAdmin protocol description to make it work in secure mode
    also. (Devaraj K via vinodkv)

    YARN-253. Fixed container-launch to not fail when there are no local
    resources to localize. (Tom White via vinodkv)

    YARN-330. Fix flakey test: TestNodeManagerShutdown#testKillContainersOnShutdown.
    (Sandy Ryza via hitesh)
    
    YARN-335. Fair scheduler doesn't check whether rack needs containers
    before assigning to node. (Sandy Ryza via tomwhite)
    
    YARN-336. Fair scheduler FIFO scheduling within a queue only allows 1
    app at a time. (Sandy Ryza via tomwhite)

    YARN-135. Client tokens should be per app-attempt, and should be
    unregistered on App-finish. (vinodkv via sseth)

    YARN-302. Fair scheduler assignmultiple should default to false. (sandyr via tucu)
    
    YARN-372. Move InlineDispatcher from hadoop-yarn-server-resourcemanager to
    hadoop-yarn-common (sseth via hitesh)

    YARN-370. Fix SchedulerUtils to correctly round up the resource for
    containers. (Zhijie Shen via acmurthy) 

    YARN-355. Fixes a bug where RM app submission could jam under load.
    (Daryn Sharp via sseth)

Release 2.0.2-alpha - 2012-09-07 

    YARN-9. Rename YARN_HOME to HADOOP_YARN_HOME. (vinodkv via acmurthy)

  NEW FEATURES

    YARN-1. Promote YARN to be a sub-project of Apache Hadoop. (acmurthy)

  IMPROVEMENTS

    YARN-29. Add a yarn-client module. (Vinod Kumar Vavilapalli via sseth)

    YARN-10. Fix DistributedShell module to not have a dependency on 
    hadoop-mapreduce-client-core. (Hitesh Shah via vinodkv)

    YARN-80. Add support for delaying rack-local containers in
    CapacityScheduler. (acmurthy) 

    YARN-137. Change the default YARN scheduler to be the CapacityScheduler. 
    (sseth via acmurthy) 

  OPTIMIZATIONS

  BUG FIXES

    YARN-13. Fix pom versions for YARN in branch-2 (todd)

    MAPREDUCE-2374. "Text File Busy" errors launching MR tasks. (Andy Isaacson
    via atm)

    YARN-12. Fix findbugs warnings in FairScheduler. (Junping Du via acmurthy) 

    YARN-22. Fix ContainerLogs to work if the log-dir is specified as a URI.
    (Mayank Bansal via sseth)

    YARN-37. Change TestRMAppTransitions to use the DrainDispatcher.
    (Mayank Bansal via sseth)

    YARN-79. Implement close on all clients to YARN so that RPC clients don't
    throw exceptions on shut-down. (Vinod Kumar Vavilapalli)

    YARN-42. Modify NM's non-aggregating logs' handler to stop properly so that
    NMs don't get NPEs on startup errors. (Devaraj K via vinodkv)

    YARN-15. Updated default classpath for YARN applications to reflect split of
    YARN into a sub-project. (Arun C Murthy via vinodkv)

    YARN-75. Modified ResourceManager's RMContainer to handle a valid RELEASE
    event at RUNNING state. (Siddharth Seth via vinodkv)

    YARN-138. Ensure default values for minimum/maximum container sizes is
    sane. (harsh & sseth via acmurthy)

Release 0.23.11 - UNRELEASED

  INCOMPATIBLE CHANGES

  NEW FEATURES

  IMPROVEMENTS

  OPTIMIZATIONS

  BUG FIXES

    YARN-1180. Update capacity scheduler docs to include types on the configs
    (Chen He via jeagles)

Release 0.23.10 - 2013-12-09

  INCOMPATIBLE CHANGES

  NEW FEATURES

  IMPROVEMENTS

    YARN-985. Nodemanager should log where a resource was localized (Ravi
    Prakash via jeagles)

    YARN-1119. Add ClusterMetrics checks to tho TestRMNodeTransitions tests
    (Mit Desai via jeagles)

  OPTIMIZATIONS

  BUG FIXES

    YARN-337. RM handles killed application tracking URL poorly (jlowe)

    YARN-1101. Active nodes can be decremented below 0 (Robert Parker 
    via tgraves)

    YARN-1176. RM web services ClusterMetricsInfo total nodes doesn't include 
    unhealthy nodes (Jonathan Eagles via tgraves)

    YARN-1386. NodeManager mistakenly loses resources and relocalizes them
    (Jason Lowe via jeagles)

Release 0.23.9 - 2013-07-08

  INCOMPATIBLE CHANGES

  NEW FEATURES

  IMPROVEMENTS

    YARN-427. Coverage fix for org.apache.hadoop.yarn.server.api.* (Aleksey
    Gorshkov via jeagles)

    YARN-478. fix coverage org.apache.hadoop.yarn.webapp.log (Aleksey Gorshkov
    via jeagles)
    
  OPTIMIZATIONS

  BUG FIXES

Release 0.23.8 - 2013-06-05

  INCOMPATIBLE CHANGES

  NEW FEATURES

  IMPROVEMENTS

  OPTIMIZATIONS

    YARN-548. Add tests for YarnUncaughtExceptionHandler (Vadim Bondarev via
    jeagles)

  BUG FIXES

    YARN-363. Add webapps/proxy directory without which YARN proxy-server fails
    when started in stand-alone mode. (Kenji Kikushima via vinodkv)

    YARN-690. RM exits on token cancel/renew problems (daryn via bobby)

Release 0.23.7 - 2013-04-18

  INCOMPATIBLE CHANGES

  NEW FEATURES

  IMPROVEMENTS

    YARN-133 Update web services docs for RM clusterMetrics (Ravi Prakash via
    kihwal)

    YARN-249. Capacity Scheduler web page should show list of active users per 
    queue like it used to (in 1.x) (Ravi Prakash via tgraves)

    YARN-236. RM should point tracking URL to RM web page when app fails to
    start (Jason Lowe via jeagles)

    YARN-269. Resource Manager not logging the health_check_script result when
    taking it out (Jason Lowe via kihwal)

    YARN-227. Application expiration difficult to debug for end-users
    (Jason Lowe via jeagles)

    YARN-443. allow OS scheduling priority of NM to be different than the 
    containers it launches (tgraves)

    YARN-468. coverage fix for org.apache.hadoop.yarn.server.webproxy.amfilter
    (Aleksey Gorshkov via bobby)

    YARN-200. yarn log does not output all needed information, and is in a
    binary format (Ravi Prakash via jlowe)

    YARN-525. make CS node-locality-delay refreshable (Thomas Graves via jlowe)

  OPTIMIZATIONS

    YARN-357. App submission should not be synchronized (daryn)

  BUG FIXES

    YARN-343. Capacity Scheduler maximum-capacity value -1 is invalid (Xuan 
    Gong via tgraves)

    YARN-364. AggregatedLogDeletionService can take too long to delete logs
    (jlowe)

    YARN-362. Unexpected extra results when using webUI table search (Ravi
    Prakash via jlowe)

    YARN-400. RM can return null application resource usage report leading to 
    NPE in client (Jason Lowe via tgraves)

    YARN-426. Failure to download a public resource prevents further downloads
    (Jason Lowe via bobby)

    YARN-448. Remove unnecessary hflush from log aggregation (Kihwal Lee via
    bobby)

    YARN-345. Many InvalidStateTransitonException errors for ApplicationImpl
    in Node Manager (Robert Parker via jlowe)

    YARN-109. .tmp file is not deleted for localized archives (Mayank Bansal 
    via bobby)

    YARN-460. CS user left in list of active users for the queue even when 
    application finished (tgraves)

Release 0.23.6 - 2013-02-06

  INCOMPATIBLE CHANGES

  NEW FEATURES

  IMPROVEMENTS

    YARN-285. Added a temporary plugin interface for RM to be able to redirect
    to JobHistory server for apps that it no longer tracks. (Derek Dagit via
    vinodkv)

  OPTIMIZATIONS

  BUG FIXES

    YARN-188. Coverage fixing for CapacityScheduler (Aleksey Gorshkov via
    bobby)

    YARN-214. RMContainerImpl does not handle event EXPIRE at state RUNNING
    (jeagles via bobby)

    YARN-151. Browser thinks RM main page JS is taking too long 
    (Ravi Prakash via bobby)

    YARN-204. test coverage for org.apache.hadoop.tools (Aleksey Gorshkov via
    bobby)

    YARN-251. Proxy URI generation fails for blank tracking URIs (Tom White
    via jlowe)

    YARN-258. RM web page UI shows Invalid Date for start and finish times
    (Ravi Prakash via jlowe)

    YARN-266. RM and JHS Web UIs are blank because AppsBlock is not escaping
    string properly (Ravi Prakash via jlowe)

    YARN-280. RM does not reject app submission with invalid tokens 
    (Daryn Sharp via tgraves)

    YARN-225. Proxy Link in RM UI thows NPE in Secure mode 
    (Devaraj K via bobby)

    YARN-293. Node Manager leaks LocalizerRunner object for every Container
    (Robert Joseph Evans via jlowe)

    YARN-50. Implement renewal / cancellation of Delegation Tokens
    (Siddharth Seth via tgraves)

    YARN-320. RM should always be able to renew its own tokens. 
    (Daryn Sharp via sseth)

    YARN-325. RM CapacityScheduler can deadlock when getQueueInfo() is 
    called and a container is completing (Arun C Murthy via tgraves)

    YARN-334. Maven RAT plugin is not checking all source files (tgraves)

    YARN-354. WebAppProxyServer exits immediately after startup (Liang Xie via
    jlowe)

Release 0.23.5 - 2012-11-28

  INCOMPATIBLE CHANGES

  NEW FEATURES

  IMPROVEMENTS

    YARN-161. Fix multiple compiler warnings for unchecked operations in YARN
    common. (Chris Nauroth via vinodkv)

    YARN-43. Fix TestResourceTrackerService to not depend on test order and thus
    pass on JDK7. (Thomas Graves via vinodkv)

    YARN-32. Fix TestApplicationTokens to not depend on test order and thus pass
    on JDK7. (vinodkv)

    YARN-186. Coverage fixing LinuxContainerExecutor (Aleksey Gorshkov via
    bobby)

    YARN-216. Remove jquery theming support. (Robert Joseph Evans via jlowe)

  OPTIMIZATIONS

  BUG FIXES

    YARN-163. Retrieving container log via NM webapp can hang with multibyte
    characters in log (jlowe via bobby)

    YARN-174. Modify NodeManager to pass the user's configuration even when
    rebooting. (vinodkv)

    YARN-177. CapacityScheduler - adding a queue while the RM is running has 
    wacky results (acmurthy vai tgraves)

    YARN-178. Fix custom ProcessTree instance creation (Radim Kolar via bobby)

    YARN-180. Capacity scheduler - containers that get reserved create 
    container token to early (acmurthy and bobby)

    YARN-139. Interrupted Exception within AsyncDispatcher leads to user
    confusion. (Vinod Kumar Vavilapalli via jlowe)

    YARN-165. RM should point tracking URL to RM web page for app when AM fails
    (jlowe via bobby)

    YARN-159. RM web ui applications page should be sorted to display last app
    first (tgraves via bobby)

    YARN-166. capacity scheduler doesn't allow capacity < 1.0 (tgraves via
    bobby)

    YARN-189. Fixed a deadlock between RM's ApplicationMasterService and the
    dispatcher. (Thomas Graves via vinodkv)

    YARN-202. Log Aggregation generates a storm of fsync() for namenode 
    (Kihwal Lee via bobby)

    YARN-201. Fix CapacityScheduler to be less conservative for starved 
    off-switch requests. (jlowe via acmurthy) 

    YARN-206. TestApplicationCleanup.testContainerCleanup occasionally fails.
    (jlowe via jeagles)

    YARN-212. NM state machine ignores an APPLICATION_CONTAINER_FINISHED event
    when it shouldn't (Nathan Roberts via jlowe)

    YARN-219. NM should aggregate logs when application finishes. (bobby)

Release 0.23.4

  INCOMPATIBLE CHANGES

  NEW FEATURES

  IMPROVEMENTS

    Change package of YarnClient to org.apache.hadoop. (Bikas Saha via vinodkv)

    YARN-108. FSDownload can create cache directories with the wrong
    permissions (Jason Lowe via bobby)

    YARN-57. Allow process-tree based resource calculation et al. to be
    pluggable to support it on multiple platforms. (Radim Kolar via acmurthy)

  OPTIMIZATIONS

  BUG FIXES

    YARN-88. DefaultContainerExecutor can fail to set proper permissions.
    (Jason Lowe via sseth)

    YARN-106. Nodemanager needs to set permissions of local directories (jlowe
    via bobby)

Release 0.23.3

  INCOMPATIBLE CHANGES

  NEW FEATURES

  IMPROVEMENTS

  OPTIMAZATIONS

  BUG FIXES

    YARN-14. Symlinks to peer distributed cache files no longer work 
    (Jason Lowe via bobby) 

    YARN-25. remove old aggregated logs  (Robert Evans via tgraves)

    YARN-27. Failed refreshQueues due to misconfiguration prevents further 
    refreshing of queues (Arun Murthy via tgraves)

    MAPREDUCE-4323. NM leaks filesystems (Jason Lowe via jeagles)
    
    YARN-39. RM-NM secret-keys should be randomly generated and rolled every
    so often. (vinodkv and sseth via sseth)

    YARN-31. Fix TestDelegationTokenRenewer to not depend on test order so as to
    pass tests on jdk7. (Thomas Graves via vinodkv)

    YARN-63. RMNodeImpl is missing valid transitions from the UNHEALTHY state
    (Jason Lowe via bobby)

    YARN-60. Fixed a bug in ResourceManager which causes all NMs to get NPEs and
    thus causes all containers to be rejected. (vinodkv)

    YARN-66. aggregated logs permissions not set properly (tgraves via bobby)

    YARN-68. NodeManager will refuse to shutdown indefinitely due to container
    log aggregation (daryn via bobby)

    YARN-87. NM ResourceLocalizationService does not set permissions of local 
    cache directories (Jason Lowe via tgraves)
name	value	description
hadoop.common.configuration.version	0.23.0	version of this configuration file
hadoop.tmp.dir	/tmp/hadoop-${user.name}	A base for other temporary directories.
io.native.lib.available	true	Controls whether to use native libraries for bz2 and zlib compression codecs or not. The property does not control any other native libraries.
hadoop.http.filter.initializers	org.apache.hadoop.http.lib.StaticUserWebFilter	A comma separated list of class names. Each class in the list must extend org.apache.hadoop.http.FilterInitializer. The corresponding Filter will be initialized. Then, the Filter will be applied to all user facing jsp and servlet web pages. The ordering of the list defines the ordering of the filters.
hadoop.security.authorization	false	Is service-level authorization enabled?
hadoop.security.instrumentation.requires.admin	false	Indicates if administrator ACLs are required to access instrumentation servlets (JMX, METRICS, CONF, STACKS).
hadoop.security.authentication	simple	Possible values are simple (no authentication), and kerberos
hadoop.security.group.mapping	org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback	Class for user to group mapping (get groups for a given user) for ACL. The default implementation, org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, will determine if the Java Native Interface (JNI) is available. If JNI is available the implementation will use the API within hadoop to resolve a list of groups for a user. If JNI is not available then the shell implementation, ShellBasedUnixGroupsMapping, is used. This implementation shells out to the Linux/Unix environment with the bash -c groups command to resolve a list of groups for a user.
hadoop.security.groups.cache.secs	300	This is the config controlling the validity of the entries in the cache containing the user->group mapping. When this duration has expired, then the implementation of the group mapping provider is invoked to get the groups of the user and then cached back.
hadoop.security.groups.negative-cache.secs	30	Expiration time for entries in the the negative user-to-group mapping caching, in seconds. This is useful when invalid users are retrying frequently. It is suggested to set a small value for this expiration, since a transient error in group lookup could temporarily lock out a legitimate user. Set this to zero or negative value to disable negative user-to-group caching.
hadoop.security.groups.cache.warn.after.ms	5000	If looking up a single user to group takes longer than this amount of milliseconds, we will log a warning message.
hadoop.security.group.mapping.ldap.url		The URL of the LDAP server to use for resolving user groups when using the LdapGroupsMapping user to group mapping.
hadoop.security.group.mapping.ldap.ssl	false	Whether or not to use SSL when connecting to the LDAP server.
hadoop.security.group.mapping.ldap.ssl.keystore		File path to the SSL keystore that contains the SSL certificate required by the LDAP server.
hadoop.security.group.mapping.ldap.ssl.keystore.password.file		The path to a file containing the password of the LDAP SSL keystore. IMPORTANT: This file should be readable only by the Unix user running the daemons.
hadoop.security.group.mapping.ldap.bind.user		The distinguished name of the user to bind as when connecting to the LDAP server. This may be left blank if the LDAP server supports anonymous binds.
hadoop.security.group.mapping.ldap.bind.password.file		The path to a file containing the password of the bind user. IMPORTANT: This file should be readable only by the Unix user running the daemons.
hadoop.security.group.mapping.ldap.base		The search base for the LDAP connection. This is a distinguished name, and will typically be the root of the LDAP directory.
hadoop.security.group.mapping.ldap.search.filter.user	(&(objectClass=user)(sAMAccountName={0}))	An additional filter to use when searching for LDAP users. The default will usually be appropriate for Active Directory installations. If connecting to an LDAP server with a non-AD schema, this should be replaced with (&(objectClass=inetOrgPerson)(uid={0}). {0} is a special string used to denote where the username fits into the filter.
hadoop.security.group.mapping.ldap.search.filter.group	(objectClass=group)	An additional filter to use when searching for LDAP groups. This should be changed when resolving groups against a non-Active Directory installation. posixGroups are currently not a supported group class.
hadoop.security.group.mapping.ldap.search.attr.member	member	The attribute of the group object that identifies the users that are members of the group. The default will usually be appropriate for any LDAP installation.
hadoop.security.group.mapping.ldap.search.attr.group.name	cn	The attribute of the group object that identifies the group name. The default will usually be appropriate for all LDAP systems.
hadoop.security.group.mapping.ldap.directory.search.timeout	10000	The attribute applied to the LDAP SearchControl properties to set a maximum time limit when searching and awaiting a result. Set to 0 if infinite wait period is desired. Default is 10 seconds. Units in milliseconds.
hadoop.security.service.user.name.key		For those cases where the same RPC protocol is implemented by multiple servers, this configuration is required for specifying the principal name to use for the service when the client wishes to make an RPC call.
hadoop.security.uid.cache.secs	14400	This is the config controlling the validity of the entries in the cache containing the userId to userName and groupId to groupName used by NativeIO getFstat().
hadoop.rpc.protection	authentication	A comma-separated list of protection values for secured sasl connections. Possible values are authentication, integrity and privacy. authentication means authentication only and no integrity or privacy; integrity implies authentication and integrity are enabled; and privacy implies all of authentication, integrity and privacy are enabled. hadoop.security.saslproperties.resolver.class can be used to override the hadoop.rpc.protection for a connection at the server side.
hadoop.security.saslproperties.resolver.class		SaslPropertiesResolver used to resolve the QOP used for a connection. If not specified, the full set of values specified in hadoop.rpc.protection is used while determining the QOP used for the connection. If a class is specified, then the QOP values returned by the class will be used while determining the QOP used for the connection.
hadoop.work.around.non.threadsafe.getpwuid	false	Some operating systems or authentication modules are known to have broken implementations of getpwuid_r and getpwgid_r, such that these calls are not thread-safe. Symptoms of this problem include JVM crashes with a stack trace inside these functions. If your system exhibits this issue, enable this configuration parameter to include a lock around the calls as a workaround. An incomplete list of some systems known to have this issue is available at http://wiki.apache.org/hadoop/KnownBrokenPwuidImplementations
hadoop.kerberos.kinit.command	kinit	Used to periodically renew Kerberos credentials when provided to Hadoop. The default setting assumes that kinit is in the PATH of users running the Hadoop client. Change this to the absolute path to kinit if this is not the case.
hadoop.security.auth_to_local		Maps kerberos principals to local user names
io.file.buffer.size	4096	The size of buffer for use in sequence files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.
io.bytes.per.checksum	512	The number of bytes per checksum. Must not be larger than io.file.buffer.size.
io.skip.checksum.errors	false	If true, when a checksum error is encountered while reading a sequence file, entries are skipped, instead of throwing an exception.
io.compression.codecs		A comma-separated list of the compression codec classes that can be used for compression/decompression. In addition to any classes specified with this property (which take precedence), codec classes on the classpath are discovered using a Java ServiceLoader.
io.compression.codec.bzip2.library	system-native	The native-code library to be used for compression and decompression by the bzip2 codec. This library could be specified either by by name or the full pathname. In the former case, the library is located by the dynamic linker, usually searching the directories specified in the environment variable LD_LIBRARY_PATH. The value of "system-native" indicates that the default system library should be used. To indicate that the algorithm should operate entirely in Java, specify "java-builtin".
io.serializations	org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization	A list of serialization classes that can be used for obtaining serializers and deserializers.
io.seqfile.local.dir	${hadoop.tmp.dir}/io/local	The local directory where sequence file stores intermediate data files during merge. May be a comma-separated list of directories on different devices in order to spread disk i/o. Directories that do not exist are ignored.
io.map.index.skip	0	Number of index entries to skip between each entry. Zero by default. Setting this to values larger than zero can facilitate opening large MapFiles using less memory.
io.map.index.interval	128	MapFile consist of two files - data file (tuples) and index file (keys). For every io.map.index.interval records written in the data file, an entry (record-key, data-file-position) is written in the index file. This is to allow for doing binary search later within the index file to look up records by their keys and get their closest positions in the data file.
fs.defaultFS	file:///	The name of the default file system. A URI whose scheme and authority determine the FileSystem implementation. The uri's scheme determines the config property (fs.SCHEME.impl) naming the FileSystem implementation class. The uri's authority is used to determine the host, port, etc. for a filesystem.
fs.default.name	file:///	Deprecated. Use (fs.defaultFS) property instead
fs.trash.interval	0	Number of minutes after which the checkpoint gets deleted. If zero, the trash feature is disabled. This option may be configured both on the server and the client. If trash is disabled server side then the client side configuration is checked. If trash is enabled on the server side then the value configured on the server is used and the client configuration value is ignored.
fs.trash.checkpoint.interval	0	Number of minutes between trash checkpoints. Should be smaller or equal to fs.trash.interval. If zero, the value is set to the value of fs.trash.interval. Every time the checkpointer runs it creates a new checkpoint out of current and removes checkpoints created more than fs.trash.interval minutes ago.
fs.AbstractFileSystem.file.impl	org.apache.hadoop.fs.local.LocalFs	The AbstractFileSystem for file: uris.
fs.AbstractFileSystem.har.impl	org.apache.hadoop.fs.HarFs	The AbstractFileSystem for har: uris.
fs.AbstractFileSystem.hdfs.impl	org.apache.hadoop.fs.Hdfs	The FileSystem for hdfs: uris.
fs.AbstractFileSystem.viewfs.impl	org.apache.hadoop.fs.viewfs.ViewFs	The AbstractFileSystem for view file system for viewfs: uris (ie client side mount table:).
fs.AbstractFileSystem.ftp.impl	org.apache.hadoop.fs.ftp.FtpFs	The FileSystem for Ftp: uris.
fs.ftp.host	0.0.0.0	FTP filesystem connects to this server
fs.ftp.host.port	21	FTP filesystem connects to fs.ftp.host on this port
fs.df.interval	60000	Disk usage statistics refresh interval in msec.
fs.du.interval	600000	File space usage statistics refresh interval in msec.
fs.s3.block.size	67108864	Block size to use when writing files to S3.
fs.s3.buffer.dir	${hadoop.tmp.dir}/s3	Determines where on the local filesystem the S3 filesystem should store files before sending them to S3 (or after retrieving them from S3).
fs.s3.maxRetries	4	The maximum number of retries for reading or writing files to S3, before we signal failure to the application.
fs.s3.sleepTimeSeconds	10	The number of seconds to sleep between each S3 retry.
fs.swift.impl	org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem	The implementation class of the OpenStack Swift Filesystem
fs.automatic.close	true	By default, FileSystem instances are automatically closed at program exit using a JVM shutdown hook. Setting this property to false disables this behavior. This is an advanced option that should only be used by server applications requiring a more carefully orchestrated shutdown sequence.
fs.s3n.block.size	67108864	Block size to use when reading files using the native S3 filesystem (s3n: URIs).
fs.s3n.multipart.uploads.enabled	false	Setting this property to true enables multiple uploads to native S3 filesystem. When uploading a file, it is split into blocks if the size is larger than fs.s3n.multipart.uploads.block.size.
fs.s3n.multipart.uploads.block.size	67108864	The block size for multipart uploads to native S3 filesystem. Default size is 64MB.
fs.s3n.multipart.copy.block.size	5368709120	The block size for multipart copy in native S3 filesystem. Default size is 5GB.
fs.s3n.server-side-encryption-algorithm		Specify a server-side encryption algorithm for S3. The default is NULL, and the only other currently allowable value is AES256.
fs.s3a.awsAccessKeyId		AWS access key ID. Omit for Role-based authentication.
fs.s3a.awsSecretAccessKey		AWS secret key. Omit for Role-based authentication.
fs.s3a.connection.maximum	15	Controls the maximum number of simultaneous connections to S3.
fs.s3a.connection.ssl.enabled	true	Enables or disables SSL connections to S3.
fs.s3a.endpoint		AWS S3 endpoint to connect to. An up-to-date list is provided in the AWS Documentation: regions and endpoints. Without this property, the standard region (s3.amazonaws.com) is assumed.
fs.s3a.proxy.host		Hostname of the (optional) proxy server for S3 connections.
fs.s3a.proxy.port		Proxy server port. If this property is not set but fs.s3a.proxy.host is, port 80 or 443 is assumed (consistent with the value of fs.s3a.connection.ssl.enabled).
fs.s3a.proxy.username		Username for authenticating with proxy server.
fs.s3a.proxy.password		Password for authenticating with proxy server.
fs.s3a.proxy.domain		Domain for authenticating with proxy server.
fs.s3a.proxy.workstation		Workstation for authenticating with proxy server.
fs.s3a.attempts.maximum	10	How many times we should retry commands on transient errors.
fs.s3a.connection.establish.timeout	5000	Socket connection setup timeout in milliseconds.
fs.s3a.connection.timeout	50000	Socket connection timeout in milliseconds.
fs.s3a.paging.maximum	5000	How many keys to request from S3 when doing directory listings at a time.
fs.s3a.threads.max	256	Maximum number of concurrent active (part)uploads, which each use a thread from the threadpool.
fs.s3a.threads.core	15	Number of core threads in the threadpool.
fs.s3a.threads.keepalivetime	60	Number of seconds a thread can be idle before being terminated.
fs.s3a.max.total.tasks	1000	Number of (part)uploads allowed to the queue before blocking additional uploads.
fs.s3a.multipart.size	104857600	How big (in bytes) to split upload or copy operations up into.
fs.s3a.multipart.threshold	2147483647	Threshold before uploads or copies use parallel multipart operations.
fs.s3a.acl.default		Set a canned ACL for newly created and copied objects. Value may be private, public-read, public-read-write, authenticated-read, log-delivery-write, bucket-owner-read, or bucket-owner-full-control.
fs.s3a.multipart.purge	false	True if you want to purge existing multipart uploads that may not have been completed/aborted correctly
fs.s3a.multipart.purge.age	86400	Minimum age in seconds of multipart uploads to purge
fs.s3a.buffer.dir	${hadoop.tmp.dir}/s3a	Comma separated list of directories that will be used to buffer file uploads to.
fs.s3a.fast.upload	false	Upload directly from memory instead of buffering to disk first. Memory usage and parallelism can be controlled as up to fs.s3a.multipart.size memory is consumed for each (part)upload actively uploading (fs.s3a.threads.max) or queueing (fs.s3a.max.total.tasks)
fs.s3a.fast.buffer.size	1048576	Size of initial memory buffer in bytes allocated for an upload. No effect if fs.s3a.fast.upload is false.
fs.s3a.impl	org.apache.hadoop.fs.s3a.S3AFileSystem	The implementation class of the S3A Filesystem
io.seqfile.compress.blocksize	1000000	The minimum block size for compression in block compressed SequenceFiles.
io.seqfile.lazydecompress	true	Should values of block-compressed SequenceFiles be decompressed only when necessary.
io.seqfile.sorter.recordlimit	1000000	The limit on number of records to be kept in memory in a spill in SequenceFiles.Sorter
io.mapfile.bloom.size	1048576	The size of BloomFilter-s used in BloomMapFile. Each time this many keys is appended the next BloomFilter will be created (inside a DynamicBloomFilter). Larger values minimize the number of filters, which slightly increases the performance, but may waste too much space if the total number of keys is usually much smaller than this number.
io.mapfile.bloom.error.rate	0.005	The rate of false positives in BloomFilter-s used in BloomMapFile. As this value decreases, the size of BloomFilter-s increases exponentially. This value is the probability of encountering false positives (default is 0.5%).
hadoop.util.hash.type	murmur	The default implementation of Hash. Currently this can take one of the two values: 'murmur' to select MurmurHash and 'jenkins' to select JenkinsHash.
ipc.client.idlethreshold	4000	Defines the threshold number of connections after which connections will be inspected for idleness.
ipc.client.kill.max	10	Defines the maximum number of clients to disconnect in one go.
ipc.client.connection.maxidletime	10000	The maximum time in msec after which a client will bring down the connection to the server.
ipc.client.connect.max.retries	10	Indicates the number of retries a client will make to establish a server connection.
ipc.client.connect.retry.interval	1000	Indicates the number of milliseconds a client will wait for before retrying to establish a server connection.
ipc.client.connect.timeout	20000	Indicates the number of milliseconds a client will wait for the socket to establish a server connection.
ipc.client.connect.max.retries.on.timeouts	45	Indicates the number of retries a client will make on socket timeout to establish a server connection.
ipc.server.listen.queue.size	128	Indicates the length of the listen queue for servers accepting client connections.
hadoop.security.impersonation.provider.class		A class which implements ImpersonationProvider interface, used to authorize whether one user can impersonate a specific user. If not specified, the DefaultImpersonationProvider will be used. If a class is specified, then that class will be used to determine the impersonation capability.
hadoop.rpc.socket.factory.class.default	org.apache.hadoop.net.StandardSocketFactory	Default SocketFactory to use. This parameter is expected to be formatted as "package.FactoryClassName".
hadoop.rpc.socket.factory.class.ClientProtocol		SocketFactory to use to connect to a DFS. If null or empty, use hadoop.rpc.socket.class.default. This socket factory is also used by DFSClient to create sockets to DataNodes.
hadoop.socks.server		Address (host:port) of the SOCKS server to be used by the SocksSocketFactory.
net.topology.node.switch.mapping.impl	org.apache.hadoop.net.ScriptBasedMapping	The default implementation of the DNSToSwitchMapping. It invokes a script specified in net.topology.script.file.name to resolve node names. If the value for net.topology.script.file.name is not set, the default value of DEFAULT_RACK is returned for all node names.
net.topology.impl	org.apache.hadoop.net.NetworkTopology	The default implementation of NetworkTopology which is classic three layer one.
net.topology.script.file.name		The script name that should be invoked to resolve DNS names to NetworkTopology names. Example: the script would take host.foo.bar as an argument, and return /rack1 as the output.
net.topology.script.number.args	100	The max number of args that the script configured with net.topology.script.file.name should be run with. Each arg is an IP address.
net.topology.table.file.name		The file name for a topology file, which is used when the net.topology.node.switch.mapping.impl property is set to org.apache.hadoop.net.TableMapping. The file format is a two column text file, with columns separated by whitespace. The first column is a DNS or IP address and the second column specifies the rack where the address maps. If no entry corresponding to a host in the cluster is found, then /default-rack is assumed.
file.stream-buffer-size	4096	The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.
file.bytes-per-checksum	512	The number of bytes per checksum. Must not be larger than file.stream-buffer-size
file.client-write-packet-size	65536	Packet size for clients to write
file.blocksize	67108864	Block size
file.replication	1	Replication factor
s3.stream-buffer-size	4096	The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.
s3.bytes-per-checksum	512	The number of bytes per checksum. Must not be larger than s3.stream-buffer-size
s3.client-write-packet-size	65536	Packet size for clients to write
s3.blocksize	67108864	Block size
s3.replication	3	Replication factor
s3native.stream-buffer-size	4096	The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.
s3native.bytes-per-checksum	512	The number of bytes per checksum. Must not be larger than s3native.stream-buffer-size
s3native.client-write-packet-size	65536	Packet size for clients to write
s3native.blocksize	67108864	Block size
s3native.replication	3	Replication factor
ftp.stream-buffer-size	4096	The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.
ftp.bytes-per-checksum	512	The number of bytes per checksum. Must not be larger than ftp.stream-buffer-size
ftp.client-write-packet-size	65536	Packet size for clients to write
ftp.blocksize	67108864	Block size
ftp.replication	3	Replication factor
tfile.io.chunk.size	1048576	Value chunk size in bytes. Default to 1MB. Values of the length less than the chunk size is guaranteed to have known value length in read time (See also TFile.Reader.Scanner.Entry.isValueLengthKnown()).
tfile.fs.output.buffer.size	262144	Buffer size used for FSDataOutputStream in bytes.
tfile.fs.input.buffer.size	262144	Buffer size used for FSDataInputStream in bytes.
hadoop.http.authentication.type	simple	Defines authentication used for Oozie HTTP endpoint. Supported values are: simple | kerberos | #AUTHENTICATION_HANDLER_CLASSNAME#
hadoop.http.authentication.token.validity	36000	Indicates how long (in seconds) an authentication token is valid before it has to be renewed.
hadoop.http.authentication.signature.secret.file	${user.home}/hadoop-http-auth-signature-secret	The signature secret for signing the authentication tokens. The same secret should be used for JT/NN/DN/TT configurations.
hadoop.http.authentication.cookie.domain		The domain to use for the HTTP cookie that stores the authentication token. In order to authentiation to work correctly across all Hadoop nodes web-consoles the domain must be correctly set. IMPORTANT: when using IP addresses, browsers ignore cookies with domain settings. For this setting to work properly all nodes in the cluster must be configured to generate URLs with hostname.domain names on it.
hadoop.http.authentication.simple.anonymous.allowed	true	Indicates if anonymous requests are allowed when using 'simple' authentication.
hadoop.http.authentication.kerberos.principal	HTTP/_HOST@LOCALHOST	Indicates the Kerberos principal to be used for HTTP endpoint. The principal MUST start with 'HTTP/' as per Kerberos HTTP SPNEGO specification.
hadoop.http.authentication.kerberos.keytab	${user.home}/hadoop.keytab	Location of the keytab file with the credentials for the principal. Referring to the same keytab file Oozie uses for its Kerberos credentials for Hadoop.
hadoop.http.cross-origin.enabled	false	Enable/disable the cross-origin (CORS) filter.
hadoop.http.cross-origin.allowed-origins	*	Comma separated list of origins that are allowed for web services needing cross-origin (CORS) support. Wildcards (*) and patterns allowed
hadoop.http.cross-origin.allowed-methods	GET,POST,HEAD	Comma separated list of methods that are allowed for web services needing cross-origin (CORS) support.
hadoop.http.cross-origin.allowed-headers	X-Requested-With,Content-Type,Accept,Origin	Comma separated list of headers that are allowed for web services needing cross-origin (CORS) support.
hadoop.http.cross-origin.max-age	1800	The number of seconds a pre-flighted request can be cached for web services needing cross-origin (CORS) support.
dfs.ha.fencing.methods		List of fencing methods to use for service fencing. May contain builtin methods (eg shell and sshfence) or user-defined method.
dfs.ha.fencing.ssh.connect-timeout	30000	SSH connection timeout, in milliseconds, to use with the builtin sshfence fencer.
dfs.ha.fencing.ssh.private-key-files		The SSH private key files to use with the builtin sshfence fencer.
hadoop.http.staticuser.user	dr.who	The user name to filter as, on static web filters while rendering content. An example use is the HDFS web UI (user to be used for browsing files).
ha.zookeeper.quorum		A list of ZooKeeper server addresses, separated by commas, that are to be used by the ZKFailoverController in automatic failover.
ha.zookeeper.session-timeout.ms	5000	The session timeout to use when the ZKFC connects to ZooKeeper. Setting this value to a lower value implies that server crashes will be detected more quickly, but risks triggering failover too aggressively in the case of a transient error or network blip.
ha.zookeeper.parent-znode	/hadoop-ha	The ZooKeeper znode under which the ZK failover controller stores its information. Note that the nameservice ID is automatically appended to this znode, so it is not normally necessary to configure this, even in a federated environment.
ha.zookeeper.acl	world:anyone:rwcda	A comma-separated list of ZooKeeper ACLs to apply to the znodes used by automatic failover. These ACLs are specified in the same format as used by the ZooKeeper CLI. If the ACL itself contains secrets, you may instead specify a path to a file, prefixed with the '@' symbol, and the value of this configuration will be loaded from within.
ha.zookeeper.auth		A comma-separated list of ZooKeeper authentications to add when connecting to ZooKeeper. These are specified in the same format as used by the "addauth" command in the ZK CLI. It is important that the authentications specified here are sufficient to access znodes with the ACL specified in ha.zookeeper.acl. If the auths contain secrets, you may instead specify a path to a file, prefixed with the '@' symbol, and the value of this configuration will be loaded from within.
hadoop.ssl.keystores.factory.class	org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory	The keystores factory to use for retrieving certificates.
hadoop.ssl.require.client.cert	false	Whether client certificates are required
hadoop.ssl.hostname.verifier	DEFAULT	The hostname verifier to provide for HttpsURLConnections. Valid values are: DEFAULT, STRICT, STRICT_I6, DEFAULT_AND_LOCALHOST and ALLOW_ALL
hadoop.ssl.server.conf	ssl-server.xml	Resource file from which ssl server keystore information will be extracted. This file is looked up in the classpath, typically it should be in Hadoop conf/ directory.
hadoop.ssl.client.conf	ssl-client.xml	Resource file from which ssl client keystore information will be extracted This file is looked up in the classpath, typically it should be in Hadoop conf/ directory.
hadoop.ssl.enabled	false	Deprecated. Use dfs.http.policy and yarn.http.policy instead.
hadoop.ssl.enabled.protocols	TLSv1	Protocols supported by the ssl.
hadoop.jetty.logs.serve.aliases	true	Enable/Disable aliases serving from jetty
fs.permissions.umask-mode	022	The umask used when creating files and directories. Can be in octal or in symbolic. Examples are: "022" (octal for u=rwx,g=r-x,o=r-x in symbolic), or "u=rwx,g=rwx,o=" (symbolic for 007 in octal).
ha.health-monitor.connect-retry-interval.ms	1000	How often to retry connecting to the service.
ha.health-monitor.check-interval.ms	1000	How often to check the service.
ha.health-monitor.sleep-after-disconnect.ms	1000	How long to sleep after an unexpected RPC error.
ha.health-monitor.rpc-timeout.ms	45000	Timeout for the actual monitorHealth() calls.
ha.failover-controller.new-active.rpc-timeout.ms	60000	Timeout that the FC waits for the new active to become active
ha.failover-controller.graceful-fence.rpc-timeout.ms	5000	Timeout that the FC waits for the old active to go to standby
ha.failover-controller.graceful-fence.connection.retries	1	FC connection retries for graceful fencing
ha.failover-controller.cli-check.rpc-timeout.ms	20000	Timeout that the CLI (manual) FC waits for monitorHealth, getServiceState
ipc.client.fallback-to-simple-auth-allowed	false	When a client is configured to attempt a secure connection, but attempts to connect to an insecure server, that server may instruct the client to switch to SASL SIMPLE (unsecure) authentication. This setting controls whether or not the client will accept this instruction from the server. When false (the default), the client will not allow the fallback to SIMPLE authentication, and will abort the connection.
fs.client.resolve.remote.symlinks	true	Whether to resolve symlinks when accessing a remote Hadoop filesystem. Setting this to false causes an exception to be thrown upon encountering a symlink. This setting does not apply to local filesystems, which automatically resolve local symlinks.
nfs.exports.allowed.hosts	* rw	By default, the export can be mounted by any client. The value string contains machine name and access privilege, separated by whitespace characters. The machine name format can be a single host, a Java regular expression, or an IPv4 address. The access privilege uses rw or ro to specify read/write or read-only access of the machines to exports. If the access privilege is not provided, the default is read-only. Entries are separated by ";". For example: "192.168.0.0/22 rw ; host.*\.example\.com ; host1.test.org ro;". Only the NFS gateway needs to restart after this property is updated.
hadoop.user.group.static.mapping.overrides	dr.who=;	Static mapping of user to groups. This will override the groups if available in the system for the specified user. In otherwords, groups look-up will not happen for these users, instead groups mapped in this configuration will be used. Mapping should be in this format. user1=group1,group2;user2=;user3=group2; Default, "dr.who=;" will consider "dr.who" as user without groups.
rpc.metrics.quantile.enable	false	Setting this property to true and rpc.metrics.percentiles.intervals to a comma-separated list of the granularity in seconds, the 50/75/90/95/99th percentile latency for rpc queue/processing time in milliseconds are added to rpc metrics.
rpc.metrics.percentiles.intervals		A comma-separated list of the granularity in seconds for the metrics which describe the 50/75/90/95/99th percentile latency for rpc queue/processing time. The metrics are outputted if rpc.metrics.quantile.enable is set to true.
hadoop.security.crypto.codec.classes.EXAMPLECIPHERSUITE		The prefix for a given crypto codec, contains a comma-separated list of implementation classes for a given crypto codec (eg EXAMPLECIPHERSUITE). The first implementation will be used if available, others are fallbacks.
hadoop.security.crypto.codec.classes.aes.ctr.nopadding	org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec,org.apache.hadoop.crypto.JceAesCtrCryptoCodec	Comma-separated list of crypto codec implementations for AES/CTR/NoPadding. The first implementation will be used if available, others are fallbacks.
hadoop.security.crypto.cipher.suite	AES/CTR/NoPadding	Cipher suite for crypto codec.
hadoop.security.crypto.jce.provider		The JCE provider name used in CryptoCodec.
hadoop.security.crypto.buffer.size	8192	The buffer size used by CryptoInputStream and CryptoOutputStream.
hadoop.security.java.secure.random.algorithm	SHA1PRNG	The java secure random algorithm.
hadoop.security.secure.random.impl		Implementation of secure random.
hadoop.security.random.device.file.path	/dev/urandom	OS security random device file path.
fs.har.impl.disable.cache	true	Don't cache 'har' filesystem instances.
hadoop.security.kms.client.authentication.retry-count	1	Number of time to retry connecting to KMS on authentication failure
hadoop.security.kms.client.encrypted.key.cache.size	500	Size of the EncryptedKeyVersion cache Queue for each key
hadoop.security.kms.client.encrypted.key.cache.low-watermark	0.3f	If size of the EncryptedKeyVersion cache Queue falls below the low watermark, this cache queue will be scheduled for a refill
hadoop.security.kms.client.encrypted.key.cache.num.refill.threads	2	Number of threads to use for refilling depleted EncryptedKeyVersion cache Queues
hadoop.security.kms.client.encrypted.key.cache.expiry	43200000	Cache expiry time for a Key, after which the cache Queue for this key will be dropped. Default = 12hrs
hadoop.htrace.spanreceiver.classes		A comma separated list of the fully-qualified class name of classes implementing SpanReceiver. The tracing system works by collecting information in structs called 'Spans'. It is up to you to choose how you want to receive this information by implementing the SpanReceiver interface.
ipc.server.max.connections	0	The maximum number of concurrent connections a server is allowed to accept. If this limit is exceeded, incoming connections will first fill the listen queue and then may go to an OS-specific listen overflow queue. The client may fail or timeout, but the server can avoid running out of file descriptors using this feature. 0 means no limit.
hadoop.registry.rm.enabled	false	Is the registry enabled in the YARN Resource Manager? If true, the YARN RM will, as needed. create the user and system paths, and purge service records when containers, application attempts and applications complete. If false, the paths must be created by other means, and no automatic cleanup of service records will take place.
hadoop.registry.zk.root	/registry	The root zookeeper node for the registry
hadoop.registry.zk.session.timeout.ms	60000	Zookeeper session timeout in milliseconds
hadoop.registry.zk.connection.timeout.ms	15000	Zookeeper connection timeout in milliseconds
hadoop.registry.zk.retry.times	5	Zookeeper connection retry count before failing
hadoop.registry.zk.retry.interval.ms	1000	
hadoop.registry.zk.retry.ceiling.ms	60000	Zookeeper retry limit in milliseconds, during exponential backoff. This places a limit even if the retry times and interval limit, combined with the backoff policy, result in a long retry period
hadoop.registry.zk.quorum	localhost:2181	List of hostname:port pairs defining the zookeeper quorum binding for the registry
hadoop.registry.secure	false	Key to set if the registry is secure. Turning it on changes the permissions policy from "open access" to restrictions on kerberos with the option of a user adding one or more auth key pairs down their own tree.
hadoop.registry.system.acls	sasl:yarn@, sasl:mapred@, sasl:hdfs@	A comma separated list of Zookeeper ACL identifiers with system access to the registry in a secure cluster. These are given full access to all entries. If there is an "@" at the end of a SASL entry it instructs the registry client to append the default kerberos domain.
hadoop.registry.kerberos.realm		The kerberos realm: used to set the realm of system principals which do not declare their realm, and any other accounts that need the value. If empty, the default realm of the running process is used. If neither are known and the realm is needed, then the registry service/client will fail.
hadoop.registry.jaas.context	Client	Key to define the JAAS context. Used in secure mode 
name	value	description
hadoop.hdfs.configuration.version	1	version of this configuration file
dfs.namenode.rpc-address		RPC address that handles all clients requests. In the case of HA/Federation where multiple namenodes exist, the name service id is added to the name e.g. dfs.namenode.rpc-address.ns1 dfs.namenode.rpc-address.EXAMPLENAMESERVICE The value of this property will take the form of nn-host1:rpc-port.
dfs.namenode.rpc-bind-host		The actual address the RPC server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.namenode.rpc-address. It can also be specified per name node or name service for HA/Federation. This is useful for making the name node listen on all interfaces by setting it to 0.0.0.0.
dfs.namenode.servicerpc-address		RPC address for HDFS Services communication. BackupNode, Datanodes and all other services should be connecting to this address if it is configured. In the case of HA/Federation where multiple namenodes exist, the name service id is added to the name e.g. dfs.namenode.servicerpc-address.ns1 dfs.namenode.rpc-address.EXAMPLENAMESERVICE The value of this property will take the form of nn-host1:rpc-port. If the value of this property is unset the value of dfs.namenode.rpc-address will be used as the default.
dfs.namenode.servicerpc-bind-host		The actual address the service RPC server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.namenode.servicerpc-address. It can also be specified per name node or name service for HA/Federation. This is useful for making the name node listen on all interfaces by setting it to 0.0.0.0.
dfs.namenode.secondary.http-address	0.0.0.0:50090	The secondary namenode http server address and port.
dfs.namenode.secondary.https-address	0.0.0.0:50091	The secondary namenode HTTPS server address and port.
dfs.datanode.address	0.0.0.0:50010	The datanode server address and port for data transfer.
dfs.datanode.http.address	0.0.0.0:50075	The datanode http server address and port.
dfs.datanode.ipc.address	0.0.0.0:50020	The datanode ipc server address and port.
dfs.datanode.handler.count	10	The number of server threads for the datanode.
dfs.namenode.http-address	0.0.0.0:50070	The address and the base port where the dfs namenode web ui will listen on.
dfs.namenode.http-bind-host		The actual adress the HTTP server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.namenode.http-address. It can also be specified per name node or name service for HA/Federation. This is useful for making the name node HTTP server listen on all interfaces by setting it to 0.0.0.0.
dfs.namenode.heartbeat.recheck-interval	300000	This time decides the interval to check for expired datanodes. With this value and dfs.heartbeat.interval, the interval of deciding the datanode is stale or not is also calculated. The unit of this configuration is millisecond.
dfs.http.policy	HTTP_ONLY	Decide if HTTPS(SSL) is supported on HDFS This configures the HTTP endpoint for HDFS daemons: The following values are supported: - HTTP_ONLY : Service is provided only on http - HTTPS_ONLY : Service is provided only on https - HTTP_AND_HTTPS : Service is provided both on http and https
dfs.client.https.need-auth	false	Whether SSL client certificate authentication is required
dfs.client.cached.conn.retry	3	The number of times the HDFS client will pull a socket from the cache. Once this number is exceeded, the client will try to create a new socket.
dfs.https.server.keystore.resource	ssl-server.xml	Resource file from which ssl server keystore information will be extracted
dfs.client.https.keystore.resource	ssl-client.xml	Resource file from which ssl client keystore information will be extracted
dfs.datanode.https.address	0.0.0.0:50475	The datanode secure http server address and port.
dfs.namenode.https-address	0.0.0.0:50470	The namenode secure http server address and port.
dfs.namenode.https-bind-host		The actual adress the HTTPS server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.namenode.https-address. It can also be specified per name node or name service for HA/Federation. This is useful for making the name node HTTPS server listen on all interfaces by setting it to 0.0.0.0.
dfs.datanode.dns.interface	default	The name of the Network Interface from which a data node should report its IP address.
dfs.datanode.dns.nameserver	default	The host name or IP address of the name server (DNS) which a DataNode should use to determine the host name used by the NameNode for communication and display purposes.
dfs.namenode.backup.address	0.0.0.0:50100	The backup node server address and port. If the port is 0 then the server will start on a free port.
dfs.namenode.backup.http-address	0.0.0.0:50105	The backup node http server address and port. If the port is 0 then the server will start on a free port.
dfs.namenode.replication.considerLoad	true	Decide if chooseTarget considers the target's load or not
dfs.default.chunk.view.size	32768	The number of bytes to view for a file on the browser.
dfs.datanode.du.reserved	0	Reserved space in bytes per volume. Always leave this much space free for non dfs use.
dfs.namenode.name.dir	file://${hadoop.tmp.dir}/dfs/name	Determines where on the local filesystem the DFS name node should store the name table(fsimage). If this is a comma-delimited list of directories then the name table is replicated in all of the directories, for redundancy.
dfs.namenode.name.dir.restore	false	Set to true to enable NameNode to attempt recovering a previously failed dfs.namenode.name.dir. When enabled, a recovery of any failed directory is attempted during checkpoint.
dfs.namenode.fs-limits.max-component-length	255	Defines the maximum number of bytes in UTF-8 encoding in each component of a path. A value of 0 will disable the check.
dfs.namenode.fs-limits.max-directory-items	1048576	Defines the maximum number of items that a directory may contain. Cannot set the property to a value less than 1 or more than 6400000.
dfs.namenode.fs-limits.min-block-size	1048576	Minimum block size in bytes, enforced by the Namenode at create time. This prevents the accidental creation of files with tiny block sizes (and thus many blocks), which can degrade performance.
dfs.namenode.fs-limits.max-blocks-per-file	1048576	Maximum number of blocks per file, enforced by the Namenode on write. This prevents the creation of extremely large files which can degrade performance.
dfs.namenode.edits.dir	${dfs.namenode.name.dir}	Determines where on the local filesystem the DFS name node should store the transaction (edits) file. If this is a comma-delimited list of directories then the transaction file is replicated in all of the directories, for redundancy. Default value is same as dfs.namenode.name.dir
dfs.namenode.shared.edits.dir		A directory on shared storage between the multiple namenodes in an HA cluster. This directory will be written by the active and read by the standby in order to keep the namespaces synchronized. This directory does not need to be listed in dfs.namenode.edits.dir above. It should be left empty in a non-HA cluster.
dfs.namenode.edits.journal-plugin.qjournal	org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager	
dfs.permissions.enabled	true	If "true", enable permission checking in HDFS. If "false", permission checking is turned off, but all other behavior is unchanged. Switching from one parameter value to the other does not change the mode, owner or group of files or directories.
dfs.permissions.superusergroup	supergroup	The name of the group of super-users.
dfs.namenode.acls.enabled	false	Set to true to enable support for HDFS ACLs (Access Control Lists). By default, ACLs are disabled. When ACLs are disabled, the NameNode rejects all RPCs related to setting or getting ACLs.
dfs.namenode.lazypersist.file.scrub.interval.sec	300	The NameNode periodically scans the namespace for LazyPersist files with missing blocks and unlinks them from the namespace. This configuration key controls the interval between successive scans. Set it to a negative value to disable this behavior.
dfs.block.access.token.enable	false	If "true", access tokens are used as capabilities for accessing datanodes. If "false", no access tokens are checked on accessing datanodes.
dfs.block.access.key.update.interval	600	Interval in minutes at which namenode updates its access keys.
dfs.block.access.token.lifetime	600	The lifetime of access tokens in minutes.
dfs.datanode.data.dir	file://${hadoop.tmp.dir}/dfs/data	Determines where on the local filesystem an DFS data node should store its blocks. If this is a comma-delimited list of directories, then data will be stored in all named directories, typically on different devices. The directories should be tagged with corresponding storage types ([SSD]/[DISK]/[ARCHIVE]/[RAM_DISK]) for HDFS storage policies. The default storage type will be DISK if the directory does not have a storage type tagged explicitly. Directories that do not exist will be created if local filesystem permission allows.
dfs.datanode.data.dir.perm	700	Permissions for the directories on on the local filesystem where the DFS data node store its blocks. The permissions can either be octal or symbolic.
dfs.replication	3	Default block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time.
dfs.replication.max	512	Maximal block replication.
dfs.namenode.replication.min	1	Minimal block replication.
dfs.blocksize	134217728	The default block size for new files, in bytes. You can use the following suffix (case insensitive): k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.), Or provide complete size in bytes (such as 134217728 for 128 MB).
dfs.client.block.write.retries	3	The number of retries for writing blocks to the data nodes, before we signal failure to the application.
dfs.client.block.write.replace-datanode-on-failure.enable	true	If there is a datanode/network failure in the write pipeline, DFSClient will try to remove the failed datanode from the pipeline and then continue writing with the remaining datanodes. As a result, the number of datanodes in the pipeline is decreased. The feature is to add new datanodes to the pipeline. This is a site-wide property to enable/disable the feature. When the cluster size is extremely small, e.g. 3 nodes or less, cluster administrators may want to set the policy to NEVER in the default configuration file or disable this feature. Otherwise, users may experience an unusually high rate of pipeline failures since it is impossible to find new datanodes for replacement. See also dfs.client.block.write.replace-datanode-on-failure.policy
dfs.client.block.write.replace-datanode-on-failure.policy	DEFAULT	This property is used only if the value of dfs.client.block.write.replace-datanode-on-failure.enable is true. ALWAYS: always add a new datanode when an existing datanode is removed. NEVER: never add a new datanode. DEFAULT: Let r be the replication number. Let n be the number of existing datanodes. Add a new datanode only if r is greater than or equal to 3 and either (1) floor(r/2) is greater than or equal to n; or (2) r is greater than n and the block is hflushed/appended.
dfs.client.block.write.replace-datanode-on-failure.best-effort	false	This property is used only if the value of dfs.client.block.write.replace-datanode-on-failure.enable is true. Best effort means that the client will try to replace a failed datanode in write pipeline (provided that the policy is satisfied), however, it continues the write operation in case that the datanode replacement also fails. Suppose the datanode replacement fails. false: An exception should be thrown so that the write will fail. true : The write should be resumed with the remaining datandoes. Note that setting this property to true allows writing to a pipeline with a smaller number of datanodes. As a result, it increases the probability of data loss.
dfs.blockreport.intervalMsec	21600000	Determines block reporting interval in milliseconds.
dfs.blockreport.initialDelay	0	Delay for first block report in seconds.
dfs.blockreport.split.threshold	1000000	If the number of blocks on the DataNode is below this threshold then it will send block reports for all Storage Directories in a single message. If the number of blocks exceeds this threshold then the DataNode will send block reports for each Storage Directory in separate messages. Set to zero to always split.
dfs.datanode.directoryscan.interval	21600	Interval in seconds for Datanode to scan data directories and reconcile the difference between blocks in memory and on the disk.
dfs.datanode.directoryscan.threads	1	How many threads should the threadpool used to compile reports for volumes in parallel have.
dfs.heartbeat.interval	3	Determines datanode heartbeat interval in seconds.
dfs.namenode.handler.count	10	The number of server threads for the namenode.
dfs.namenode.safemode.threshold-pct	0.999f	Specifies the percentage of blocks that should satisfy the minimal replication requirement defined by dfs.namenode.replication.min. Values less than or equal to 0 mean not to wait for any particular percentage of blocks before exiting safemode. Values greater than 1 will make safe mode permanent.
dfs.namenode.safemode.min.datanodes	0	Specifies the number of datanodes that must be considered alive before the name node exits safemode. Values less than or equal to 0 mean not to take the number of live datanodes into account when deciding whether to remain in safe mode during startup. Values greater than the number of datanodes in the cluster will make safe mode permanent.
dfs.namenode.safemode.extension	30000	Determines extension of safe mode in milliseconds after the threshold level is reached.
dfs.namenode.resource.check.interval	5000	The interval in milliseconds at which the NameNode resource checker runs. The checker calculates the number of the NameNode storage volumes whose available spaces are more than dfs.namenode.resource.du.reserved, and enters safemode if the number becomes lower than the minimum value specified by dfs.namenode.resource.checked.volumes.minimum.
dfs.namenode.resource.du.reserved	104857600	The amount of space to reserve/require for a NameNode storage directory in bytes. The default is 100MB.
dfs.namenode.resource.checked.volumes		A list of local directories for the NameNode resource checker to check in addition to the local edits directories.
dfs.namenode.resource.checked.volumes.minimum	1	The minimum number of redundant NameNode storage volumes required.
dfs.datanode.balance.bandwidthPerSec	1048576	Specifies the maximum amount of bandwidth that each datanode can utilize for the balancing purpose in term of the number of bytes per second.
dfs.hosts		Names a file that contains a list of hosts that are permitted to connect to the namenode. The full pathname of the file must be specified. If the value is empty, all hosts are permitted.
dfs.hosts.exclude		Names a file that contains a list of hosts that are not permitted to connect to the namenode. The full pathname of the file must be specified. If the value is empty, no hosts are excluded.
dfs.namenode.max.objects	0	The maximum number of files, directories and blocks dfs supports. A value of zero indicates no limit to the number of objects that dfs supports.
dfs.namenode.datanode.registration.ip-hostname-check	true	If true (the default), then the namenode requires that a connecting datanode's address must be resolved to a hostname. If necessary, a reverse DNS lookup is performed. All attempts to register a datanode from an unresolvable address are rejected. It is recommended that this setting be left on to prevent accidental registration of datanodes listed by hostname in the excludes file during a DNS outage. Only set this to false in environments where there is no infrastructure to support reverse DNS lookup.
dfs.namenode.decommission.interval	30	Namenode periodicity in seconds to check if decommission is complete.
dfs.namenode.decommission.blocks.per.interval	500000	The approximate number of blocks to process per decommission interval, as defined in dfs.namenode.decommission.interval.
dfs.namenode.decommission.max.concurrent.tracked.nodes	100	The maximum number of decommission-in-progress datanodes nodes that will be tracked at one time by the namenode. Tracking a decommission-in-progress datanode consumes additional NN memory proportional to the number of blocks on the datnode. Having a conservative limit reduces the potential impact of decomissioning a large number of nodes at once. A value of 0 means no limit will be enforced.
dfs.namenode.replication.interval	3	The periodicity in seconds with which the namenode computes replication work for datanodes.
dfs.namenode.accesstime.precision	3600000	The access time for HDFS file is precise upto this value. The default value is 1 hour. Setting a value of 0 disables access times for HDFS.
dfs.datanode.plugins		Comma-separated list of datanode plug-ins to be activated.
dfs.namenode.plugins		Comma-separated list of namenode plug-ins to be activated.
dfs.stream-buffer-size	4096	The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.
dfs.bytes-per-checksum	512	The number of bytes per checksum. Must not be larger than dfs.stream-buffer-size
dfs.client-write-packet-size	65536	Packet size for clients to write
dfs.client.write.exclude.nodes.cache.expiry.interval.millis	600000	The maximum period to keep a DN in the excluded nodes list at a client. After this period, in milliseconds, the previously excluded node(s) will be removed automatically from the cache and will be considered good for block allocations again. Useful to lower or raise in situations where you keep a file open for very long periods (such as a Write-Ahead-Log (WAL) file) to make the writer tolerant to cluster maintenance restarts. Defaults to 10 minutes.
dfs.namenode.checkpoint.dir	file://${hadoop.tmp.dir}/dfs/namesecondary	Determines where on the local filesystem the DFS secondary name node should store the temporary images to merge. If this is a comma-delimited list of directories then the image is replicated in all of the directories for redundancy.
dfs.namenode.checkpoint.edits.dir	${dfs.namenode.checkpoint.dir}	Determines where on the local filesystem the DFS secondary name node should store the temporary edits to merge. If this is a comma-delimited list of directories then the edits is replicated in all of the directories for redundancy. Default value is same as dfs.namenode.checkpoint.dir
dfs.namenode.checkpoint.period	3600	The number of seconds between two periodic checkpoints.
dfs.namenode.checkpoint.txns	1000000	The Secondary NameNode or CheckpointNode will create a checkpoint of the namespace every 'dfs.namenode.checkpoint.txns' transactions, regardless of whether 'dfs.namenode.checkpoint.period' has expired.
dfs.namenode.checkpoint.check.period	60	The SecondaryNameNode and CheckpointNode will poll the NameNode every 'dfs.namenode.checkpoint.check.period' seconds to query the number of uncheckpointed transactions.
dfs.namenode.checkpoint.max-retries	3	The SecondaryNameNode retries failed checkpointing. If the failure occurs while loading fsimage or replaying edits, the number of retries is limited by this variable.
dfs.namenode.num.checkpoints.retained	2	The number of image checkpoint files (fsimage_*) that will be retained by the NameNode and Secondary NameNode in their storage directories. All edit logs (stored on edits_* files) necessary to recover an up-to-date namespace from the oldest retained checkpoint will also be retained.
dfs.namenode.num.extra.edits.retained	1000000	The number of extra transactions which should be retained beyond what is minimally necessary for a NN restart. It does not translate directly to file's age, or the number of files kept, but to the number of transactions (here "edits" means transactions). One edit file may contain several transactions (edits). During checkpoint, NameNode will identify the total number of edits to retain as extra by checking the latest checkpoint transaction value, subtracted by the value of this property. Then, it scans edits files to identify the older ones that don't include the computed range of retained transactions that are to be kept around, and purges them subsequently. The retainment can be useful for audit purposes or for an HA setup where a remote Standby Node may have been offline for some time and need to have a longer backlog of retained edits in order to start again. Typically each edit is on the order of a few hundred bytes, so the default of 1 million edits should be on the order of hundreds of MBs or low GBs. NOTE: Fewer extra edits may be retained than value specified for this setting if doing so would mean that more segments would be retained than the number configured by dfs.namenode.max.extra.edits.segments.retained.
dfs.namenode.max.extra.edits.segments.retained	10000	The maximum number of extra edit log segments which should be retained beyond what is minimally necessary for a NN restart. When used in conjunction with dfs.namenode.num.extra.edits.retained, this configuration property serves to cap the number of extra edits files to a reasonable value.
dfs.namenode.delegation.key.update-interval	86400000	The update interval for master key for delegation tokens in the namenode in milliseconds.
dfs.namenode.delegation.token.max-lifetime	604800000	The maximum lifetime in milliseconds for which a delegation token is valid.
dfs.namenode.delegation.token.renew-interval	86400000	The renewal interval for delegation token in milliseconds.
dfs.datanode.failed.volumes.tolerated	0	The number of volumes that are allowed to fail before a datanode stops offering service. By default any volume failure will cause a datanode to shutdown.
dfs.image.compress	false	Should the dfs image be compressed?
dfs.image.compression.codec	org.apache.hadoop.io.compress.DefaultCodec	If the dfs image is compressed, how should they be compressed? This has to be a codec defined in io.compression.codecs.
dfs.image.transfer.timeout	60000	Socket timeout for image transfer in milliseconds. This timeout and the related dfs.image.transfer.bandwidthPerSec parameter should be configured such that normal image transfer can complete successfully. This timeout prevents client hangs when the sender fails during image transfer. This is socket timeout during image transfer.
dfs.image.transfer.bandwidthPerSec	0	Maximum bandwidth used for image transfer in bytes per second. This can help keep normal namenode operations responsive during checkpointing. The maximum bandwidth and timeout in dfs.image.transfer.timeout should be set such that normal image transfers can complete successfully. A default value of 0 indicates that throttling is disabled.
dfs.image.transfer.chunksize	65536	Chunksize in bytes to upload the checkpoint. Chunked streaming is used to avoid internal buffering of contents of image file of huge size.
dfs.namenode.support.allow.format	true	Does HDFS namenode allow itself to be formatted? You may consider setting this to false for any production cluster, to avoid any possibility of formatting a running DFS.
dfs.datanode.max.transfer.threads	4096	Specifies the maximum number of threads to use for transferring data in and out of the DN.
dfs.datanode.scan.period.hours	504	If this is positive, the DataNode will not scan any individual block more than once in the specified scan period. If this is negative, the block scanner is disabled. If this is set to zero, then the default value of 504 hours or 3 weeks is used. Prior versions of HDFS incorrectly documented that setting this key to zero will disable the block scanner.
dfs.block.scanner.volume.bytes.per.second	1048576	If this is 0, the DataNode's block scanner will be disabled. If this is positive, this is the number of bytes per second that the DataNode's block scanner will try to scan from each volume.
dfs.datanode.readahead.bytes	4194304	While reading block files, if the Hadoop native libraries are available, the datanode can use the posix_fadvise system call to explicitly page data into the operating system buffer cache ahead of the current reader's position. This can improve performance especially when disks are highly contended. This configuration specifies the number of bytes ahead of the current read position which the datanode will attempt to read ahead. This feature may be disabled by configuring this property to 0. If the native libraries are not available, this configuration has no effect.
dfs.datanode.drop.cache.behind.reads	false	In some workloads, the data read from HDFS is known to be significantly large enough that it is unlikely to be useful to cache it in the operating system buffer cache. In this case, the DataNode may be configured to automatically purge all data from the buffer cache after it is delivered to the client. This behavior is automatically disabled for workloads which read only short sections of a block (e.g HBase random-IO workloads). This may improve performance for some workloads by freeing buffer cache space usage for more cacheable data. If the Hadoop native libraries are not available, this configuration has no effect.
dfs.datanode.drop.cache.behind.writes	false	In some workloads, the data written to HDFS is known to be significantly large enough that it is unlikely to be useful to cache it in the operating system buffer cache. In this case, the DataNode may be configured to automatically purge all data from the buffer cache after it is written to disk. This may improve performance for some workloads by freeing buffer cache space usage for more cacheable data. If the Hadoop native libraries are not available, this configuration has no effect.
dfs.datanode.sync.behind.writes	false	If this configuration is enabled, the datanode will instruct the operating system to enqueue all written data to the disk immediately after it is written. This differs from the usual OS policy which may wait for up to 30 seconds before triggering writeback. This may improve performance for some workloads by smoothing the IO profile for data written to disk. If the Hadoop native libraries are not available, this configuration has no effect.
dfs.client.failover.max.attempts	15	Expert only. The number of client failover attempts that should be made before the failover is considered failed.
dfs.client.failover.sleep.base.millis	500	Expert only. The time to wait, in milliseconds, between failover attempts increases exponentially as a function of the number of attempts made so far, with a random factor of +/- 50%. This option specifies the base value used in the failover calculation. The first failover will retry immediately. The 2nd failover attempt will delay at least dfs.client.failover.sleep.base.millis milliseconds. And so on.
dfs.client.failover.sleep.max.millis	15000	Expert only. The time to wait, in milliseconds, between failover attempts increases exponentially as a function of the number of attempts made so far, with a random factor of +/- 50%. This option specifies the maximum value to wait between failovers. Specifically, the time between two failover attempts will not exceed +/- 50% of dfs.client.failover.sleep.max.millis milliseconds.
dfs.client.failover.connection.retries	0	Expert only. Indicates the number of retries a failover IPC client will make to establish a server connection.
dfs.client.failover.connection.retries.on.timeouts	0	Expert only. The number of retry attempts a failover IPC client will make on socket timeout when establishing a server connection.
dfs.client.datanode-restart.timeout	30	Expert only. The time to wait, in seconds, from reception of an datanode shutdown notification for quick restart, until declaring the datanode dead and invoking the normal recovery mechanisms. The notification is sent by a datanode when it is being shutdown using the shutdownDatanode admin command with the upgrade option.
dfs.nameservices		Comma-separated list of nameservices.
dfs.nameservice.id		The ID of this nameservice. If the nameservice ID is not configured or more than one nameservice is configured for dfs.nameservices it is determined automatically by matching the local node's address with the configured address.
dfs.internal.nameservices		Comma-separated list of nameservices that belong to this cluster. Datanode will report to all the nameservices in this list. By default this is set to the value of dfs.nameservices.
dfs.ha.namenodes.EXAMPLENAMESERVICE		The prefix for a given nameservice, contains a comma-separated list of namenodes for a given nameservice (eg EXAMPLENAMESERVICE).
dfs.ha.namenode.id		The ID of this namenode. If the namenode ID is not configured it is determined automatically by matching the local node's address with the configured address.
dfs.ha.log-roll.period	120	How often, in seconds, the StandbyNode should ask the active to roll edit logs. Since the StandbyNode only reads from finalized log segments, the StandbyNode will only be as up-to-date as how often the logs are rolled. Note that failover triggers a log roll so the StandbyNode will be up to date before it becomes active.
dfs.ha.tail-edits.period	60	How often, in seconds, the StandbyNode should check for new finalized log segments in the shared edits log.
dfs.ha.automatic-failover.enabled	false	Whether automatic failover is enabled. See the HDFS High Availability documentation for details on automatic HA configuration.
dfs.client.use.datanode.hostname	false	Whether clients should use datanode hostnames when connecting to datanodes.
dfs.datanode.use.datanode.hostname	false	Whether datanodes should use datanode hostnames when connecting to other datanodes for data transfer.
dfs.client.local.interfaces		A comma separated list of network interface names to use for data transfer between the client and datanodes. When creating a connection to read from or write to a datanode, the client chooses one of the specified interfaces at random and binds its socket to the IP of that interface. Individual names may be specified as either an interface name (eg "eth0"), a subinterface name (eg "eth0:0"), or an IP address (which may be specified using CIDR notation to match a range of IPs).
dfs.datanode.shared.file.descriptor.paths	/dev/shm,/tmp	A comma-separated list of paths to use when creating file descriptors that will be shared between the DataNode and the DFSClient. Typically we use /dev/shm, so that the file descriptors will not be written to disk. Systems that don't have /dev/shm will fall back to /tmp by default.
dfs.short.circuit.shared.memory.watcher.interrupt.check.ms	60000	The length of time in milliseconds that the short-circuit shared memory watcher will go between checking for java interruptions sent from other threads. This is provided mainly for unit tests.
dfs.namenode.kerberos.internal.spnego.principal	${dfs.web.authentication.kerberos.principal}	
dfs.secondary.namenode.kerberos.internal.spnego.principal	${dfs.web.authentication.kerberos.principal}	
dfs.namenode.kerberos.principal.pattern	*	A client-side RegEx that can be configured to control allowed realms to authenticate with (useful in cross-realm env.)
dfs.namenode.avoid.read.stale.datanode	false	Indicate whether or not to avoid reading from "stale" datanodes whose heartbeat messages have not been received by the namenode for more than a specified time interval. Stale datanodes will be moved to the end of the node list returned for reading. See dfs.namenode.avoid.write.stale.datanode for a similar setting for writes.
dfs.namenode.avoid.write.stale.datanode	false	Indicate whether or not to avoid writing to "stale" datanodes whose heartbeat messages have not been received by the namenode for more than a specified time interval. Writes will avoid using stale datanodes unless more than a configured ratio (dfs.namenode.write.stale.datanode.ratio) of datanodes are marked as stale. See dfs.namenode.avoid.read.stale.datanode for a similar setting for reads.
dfs.namenode.stale.datanode.interval	30000	Default time interval for marking a datanode as "stale", i.e., if the namenode has not received heartbeat msg from a datanode for more than this time interval, the datanode will be marked and treated as "stale" by default. The stale interval cannot be too small since otherwise this may cause too frequent change of stale states. We thus set a minimum stale interval value (the default value is 3 times of heartbeat interval) and guarantee that the stale interval cannot be less than the minimum value. A stale data node is avoided during lease/block recovery. It can be conditionally avoided for reads (see dfs.namenode.avoid.read.stale.datanode) and for writes (see dfs.namenode.avoid.write.stale.datanode).
dfs.namenode.write.stale.datanode.ratio	0.5f	When the ratio of number stale datanodes to total datanodes marked is greater than this ratio, stop avoiding writing to stale nodes so as to prevent causing hotspots.
dfs.namenode.invalidate.work.pct.per.iteration	0.32f	*Note*: Advanced property. Change with caution. This determines the percentage amount of block invalidations (deletes) to do over a single DN heartbeat deletion command. The final deletion count is determined by applying this percentage to the number of live nodes in the system. The resultant number is the number of blocks from the deletion list chosen for proper invalidation over a single heartbeat of a single DN. Value should be a positive, non-zero percentage in float notation (X.Yf), with 1.0f meaning 100%.
dfs.namenode.replication.work.multiplier.per.iteration	2	*Note*: Advanced property. Change with caution. This determines the total amount of block transfers to begin in parallel at a DN, for replication, when such a command list is being sent over a DN heartbeat by the NN. The actual number is obtained by multiplying this multiplier with the total number of live nodes in the cluster. The result number is the number of blocks to begin transfers immediately for, per DN heartbeat. This number can be any positive, non-zero integer.
nfs.server.port	2049	Specify the port number used by Hadoop NFS.
nfs.mountd.port	4242	Specify the port number used by Hadoop mount daemon.
nfs.dump.dir	/tmp/.hdfs-nfs	This directory is used to temporarily save out-of-order writes before writing to HDFS. For each file, the out-of-order writes are dumped after they are accumulated to exceed certain threshold (e.g., 1MB) in memory. One needs to make sure the directory has enough space.
nfs.rtmax	1048576	This is the maximum size in bytes of a READ request supported by the NFS gateway. If you change this, make sure you also update the nfs mount's rsize(add rsize= # of bytes to the mount directive).
nfs.wtmax	1048576	This is the maximum size in bytes of a WRITE request supported by the NFS gateway. If you change this, make sure you also update the nfs mount's wsize(add wsize= # of bytes to the mount directive).
nfs.keytab.file		*Note*: Advanced property. Change with caution. This is the path to the keytab file for the hdfs-nfs gateway. This is required when the cluster is kerberized.
nfs.kerberos.principal		*Note*: Advanced property. Change with caution. This is the name of the kerberos principal. This is required when the cluster is kerberized.It must be of this format: nfs-gateway-user/nfs-gateway-host@kerberos-realm
nfs.allow.insecure.ports	true	When set to false, client connections originating from unprivileged ports (those above 1023) will be rejected. This is to ensure that clients connecting to this NFS Gateway must have had root privilege on the machine where they're connecting from.
dfs.webhdfs.enabled	true	Enable WebHDFS (REST API) in Namenodes and Datanodes.
hadoop.fuse.connection.timeout	300	The minimum number of seconds that we'll cache libhdfs connection objects in fuse_dfs. Lower values will result in lower memory consumption; higher values may speed up access by avoiding the overhead of creating new connection objects.
hadoop.fuse.timer.period	5	The number of seconds between cache expiry checks in fuse_dfs. Lower values will result in fuse_dfs noticing changes to Kerberos ticket caches more quickly.
dfs.metrics.percentiles.intervals		Comma-delimited set of integers denoting the desired rollover intervals (in seconds) for percentile latency metrics on the Namenode and Datanode. By default, percentile latency metrics are disabled.
hadoop.user.group.metrics.percentiles.intervals		A comma-separated list of the granularity in seconds for the metrics which describe the 50/75/90/95/99th percentile latency for group resolution in milliseconds. By default, percentile latency metrics are disabled.
dfs.encrypt.data.transfer	false	Whether or not actual block data that is read/written from/to HDFS should be encrypted on the wire. This only needs to be set on the NN and DNs, clients will deduce this automatically. It is possible to override this setting per connection by specifying custom logic via dfs.trustedchannel.resolver.class.
dfs.encrypt.data.transfer.algorithm		This value may be set to either "3des" or "rc4". If nothing is set, then the configured JCE default on the system is used (usually 3DES.) It is widely believed that 3DES is more cryptographically secure, but RC4 is substantially faster. Note that if AES is supported by both the client and server then this encryption algorithm will only be used to initially transfer keys for AES. (See dfs.encrypt.data.transfer.cipher.suites.)
dfs.encrypt.data.transfer.cipher.suites		This value may be either undefined or AES/CTR/NoPadding. If defined, then dfs.encrypt.data.transfer uses the specified cipher suite for data encryption. If not defined, then only the algorithm specified in dfs.encrypt.data.transfer.algorithm is used. By default, the property is not defined.
dfs.encrypt.data.transfer.cipher.key.bitlength	128	The key bitlength negotiated by dfsclient and datanode for encryption. This value may be set to either 128, 192 or 256.
dfs.trustedchannel.resolver.class		TrustedChannelResolver is used to determine whether a channel is trusted for plain data transfer. The TrustedChannelResolver is invoked on both client and server side. If the resolver indicates that the channel is trusted, then the data transfer will not be encrypted even if dfs.encrypt.data.transfer is set to true. The default implementation returns false indicating that the channel is not trusted.
dfs.data.transfer.protection		A comma-separated list of SASL protection values used for secured connections to the DataNode when reading or writing block data. Possible values are authentication, integrity and privacy. authentication means authentication only and no integrity or privacy; integrity implies authentication and integrity are enabled; and privacy implies all of authentication, integrity and privacy are enabled. If dfs.encrypt.data.transfer is set to true, then it supersedes the setting for dfs.data.transfer.protection and enforces that all connections must use a specialized encrypted SASL handshake. This property is ignored for connections to a DataNode listening on a privileged port. In this case, it is assumed that the use of a privileged port establishes sufficient trust.
dfs.data.transfer.saslproperties.resolver.class		SaslPropertiesResolver used to resolve the QOP used for a connection to the DataNode when reading or writing block data. If not specified, the value of hadoop.security.saslproperties.resolver.class is used as the default value.
dfs.datanode.hdfs-blocks-metadata.enabled	false	Boolean which enables backend datanode-side support for the experimental DistributedFileSystem#getFileVBlockStorageLocations API.
dfs.client.file-block-storage-locations.num-threads	10	Number of threads used for making parallel RPCs in DistributedFileSystem#getFileBlockStorageLocations().
dfs.client.file-block-storage-locations.timeout.millis	1000	Timeout (in milliseconds) for the parallel RPCs made in DistributedFileSystem#getFileBlockStorageLocations().
dfs.journalnode.rpc-address	0.0.0.0:8485	The JournalNode RPC server address and port.
dfs.journalnode.http-address	0.0.0.0:8480	The address and port the JournalNode HTTP server listens on. If the port is 0 then the server will start on a free port.
dfs.journalnode.https-address	0.0.0.0:8481	The address and port the JournalNode HTTPS server listens on. If the port is 0 then the server will start on a free port.
dfs.namenode.audit.loggers	default	List of classes implementing audit loggers that will receive audit events. These should be implementations of org.apache.hadoop.hdfs.server.namenode.AuditLogger. The special value "default" can be used to reference the default audit logger, which uses the configured log system. Installing custom audit loggers may affect the performance and stability of the NameNode. Refer to the custom logger's documentation for more details.
dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold	10737418240	Only used when the dfs.datanode.fsdataset.volume.choosing.policy is set to org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy. This setting controls how much DN volumes are allowed to differ in terms of bytes of free disk space before they are considered imbalanced. If the free space of all the volumes are within this range of each other, the volumes will be considered balanced and block assignments will be done on a pure round robin basis.
dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction	0.75f	Only used when the dfs.datanode.fsdataset.volume.choosing.policy is set to org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy. This setting controls what percentage of new block allocations will be sent to volumes with more available disk space than others. This setting should be in the range 0.0 - 1.0, though in practice 0.5 - 1.0, since there should be no reason to prefer that volumes with less available disk space receive more block allocations.
dfs.namenode.edits.noeditlogchannelflush	false	Specifies whether to flush edit log file channel. When set, expensive FileChannel#force calls are skipped and synchronous disk writes are enabled instead by opening the edit log file with RandomAccessFile("rws") flags. This can significantly improve the performance of edit log writes on the Windows platform. Note that the behavior of the "rws" flags is platform and hardware specific and might not provide the same level of guarantees as FileChannel#force. For example, the write will skip the disk-cache on SAS and SCSI devices while it might not on SATA devices. This is an expert level setting, change with caution.
dfs.client.cache.drop.behind.writes		Just like dfs.datanode.drop.cache.behind.writes, this setting causes the page cache to be dropped behind HDFS writes, potentially freeing up more memory for other uses. Unlike dfs.datanode.drop.cache.behind.writes, this is a client-side setting rather than a setting for the entire datanode. If present, this setting will override the DataNode default. If the native libraries are not available to the DataNode, this configuration has no effect.
dfs.client.cache.drop.behind.reads		Just like dfs.datanode.drop.cache.behind.reads, this setting causes the page cache to be dropped behind HDFS reads, potentially freeing up more memory for other uses. Unlike dfs.datanode.drop.cache.behind.reads, this is a client-side setting rather than a setting for the entire datanode. If present, this setting will override the DataNode default. If the native libraries are not available to the DataNode, this configuration has no effect.
dfs.client.cache.readahead		When using remote reads, this setting causes the datanode to read ahead in the block file using posix_fadvise, potentially decreasing I/O wait times. Unlike dfs.datanode.readahead.bytes, this is a client-side setting rather than a setting for the entire datanode. If present, this setting will override the DataNode default. When using local reads, this setting determines how much readahead we do in BlockReaderLocal. If the native libraries are not available to the DataNode, this configuration has no effect.
dfs.namenode.enable.retrycache	true	This enables the retry cache on the namenode. Namenode tracks for non-idempotent requests the corresponding response. If a client retries the request, the response from the retry cache is sent. Such operations are tagged with annotation @AtMostOnce in namenode protocols. It is recommended that this flag be set to true. Setting it to false, will result in clients getting failure responses to retried request. This flag must be enabled in HA setup for transparent fail-overs. The entries in the cache have expiration time configurable using dfs.namenode.retrycache.expirytime.millis.
dfs.namenode.retrycache.expirytime.millis	600000	The time for which retry cache entries are retained.
dfs.namenode.retrycache.heap.percent	0.03f	This parameter configures the heap size allocated for retry cache (excluding the response cached). This corresponds to approximately 4096 entries for every 64MB of namenode process java heap size. Assuming retry cache entry expiration time (configured using dfs.namenode.retrycache.expirytime.millis) of 10 minutes, this enables retry cache to support 7 operations per second sustained for 10 minutes. As the heap size is increased, the operation rate linearly increases.
dfs.client.mmap.enabled	true	If this is set to false, the client won't attempt to perform memory-mapped reads.
dfs.client.mmap.cache.size	256	When zero-copy reads are used, the DFSClient keeps a cache of recently used memory mapped regions. This parameter controls the maximum number of entries that we will keep in that cache. The larger this number is, the more file descriptors we will potentially use for memory-mapped files. mmaped files also use virtual address space. You may need to increase your ulimit virtual address space limits before increasing the client mmap cache size. Note that you can still do zero-copy reads when this size is set to 0.
dfs.client.mmap.cache.timeout.ms	3600000	The minimum length of time that we will keep an mmap entry in the cache between uses. If an entry is in the cache longer than this, and nobody uses it, it will be removed by a background thread.
dfs.client.mmap.retry.timeout.ms	300000	The minimum amount of time that we will wait before retrying a failed mmap operation.
dfs.client.short.circuit.replica.stale.threshold.ms	1800000	The maximum amount of time that we will consider a short-circuit replica to be valid, if there is no communication from the DataNode. After this time has elapsed, we will re-fetch the short-circuit replica even if it is in the cache.
dfs.namenode.path.based.cache.block.map.allocation.percent	0.25	The percentage of the Java heap which we will allocate to the cached blocks map. The cached blocks map is a hash map which uses chained hashing. Smaller maps may be accessed more slowly if the number of cached blocks is large; larger maps will consume more memory.
dfs.datanode.max.locked.memory	0	The amount of memory in bytes to use for caching of block replicas in memory on the datanode. The datanode's maximum locked memory soft ulimit (RLIMIT_MEMLOCK) must be set to at least this value, else the datanode will abort on startup. By default, this parameter is set to 0, which disables in-memory caching. If the native libraries are not available to the DataNode, this configuration has no effect.
dfs.namenode.list.cache.directives.num.responses	100	This value controls the number of cache directives that the NameNode will send over the wire in response to a listDirectives RPC.
dfs.namenode.list.cache.pools.num.responses	100	This value controls the number of cache pools that the NameNode will send over the wire in response to a listPools RPC.
dfs.namenode.path.based.cache.refresh.interval.ms	30000	The amount of milliseconds between subsequent path cache rescans. Path cache rescans are when we calculate which blocks should be cached, and on what datanodes. By default, this parameter is set to 30 seconds.
dfs.namenode.path.based.cache.retry.interval.ms	30000	When the NameNode needs to uncache something that is cached, or cache something that is not cached, it must direct the DataNodes to do so by sending a DNA_CACHE or DNA_UNCACHE command in response to a DataNode heartbeat. This parameter controls how frequently the NameNode will resend these commands.
dfs.datanode.fsdatasetcache.max.threads.per.volume	4	The maximum number of threads per volume to use for caching new data on the datanode. These threads consume both I/O and CPU. This can affect normal datanode operations.
dfs.cachereport.intervalMsec	10000	Determines cache reporting interval in milliseconds. After this amount of time, the DataNode sends a full report of its cache state to the NameNode. The NameNode uses the cache report to update its map of cached blocks to DataNode locations. This configuration has no effect if in-memory caching has been disabled by setting dfs.datanode.max.locked.memory to 0 (which is the default). If the native libraries are not available to the DataNode, this configuration has no effect.
dfs.namenode.edit.log.autoroll.multiplier.threshold	2.0	Determines when an active namenode will roll its own edit log. The actual threshold (in number of edits) is determined by multiplying this value by dfs.namenode.checkpoint.txns. This prevents extremely large edit files from accumulating on the active namenode, which can cause timeouts during namenode startup and pose an administrative hassle. This behavior is intended as a failsafe for when the standby or secondary namenode fail to roll the edit log by the normal checkpoint threshold.
dfs.namenode.edit.log.autoroll.check.interval.ms	300000	How often an active namenode will check if it needs to roll its edit log, in milliseconds.
dfs.webhdfs.user.provider.user.pattern	^[A-Za-z_][A-Za-z0-9._-]*[$]?$	Valid pattern for user and group names for webhdfs, it must be a valid java regex.
dfs.client.context	default	The name of the DFSClient context that we should use. Clients that share a context share a socket cache and short-circuit cache, among other things. You should only change this if you don't want to share with another set of threads.
dfs.client.read.shortcircuit	false	This configuration parameter turns on short-circuit local reads.
dfs.domain.socket.path		Optional. This is a path to a UNIX domain socket that will be used for communication between the DataNode and local HDFS clients. If the string "_PORT" is present in this path, it will be replaced by the TCP port of the DataNode.
dfs.client.read.shortcircuit.skip.checksum	false	If this configuration parameter is set, short-circuit local reads will skip checksums. This is normally not recommended, but it may be useful for special setups. You might consider using this if you are doing your own checksumming outside of HDFS.
dfs.client.read.shortcircuit.streams.cache.size	256	The DFSClient maintains a cache of recently opened file descriptors. This parameter controls the size of that cache. Setting this higher will use more file descriptors, but potentially provide better performance on workloads involving lots of seeks.
dfs.client.read.shortcircuit.streams.cache.expiry.ms	300000	This controls the minimum amount of time file descriptors need to sit in the client cache context before they can be closed for being inactive for too long.
dfs.datanode.shared.file.descriptor.paths	/dev/shm,/tmp	Comma separated paths to the directory on which shared memory segments are created. The client and the DataNode exchange information via this shared memory segment. It tries paths in order until creation of shared memory segment succeeds.
dfs.client.use.legacy.blockreader.local	false	Legacy short-circuit reader implementation based on HDFS-2246 is used if this configuration parameter is true. This is for the platforms other than Linux where the new implementation based on HDFS-347 is not available.
dfs.block.local-path-access.user		Comma separated list of the users allowd to open block files on legacy short-circuit local read.
dfs.client.domain.socket.data.traffic	false	This control whether we will try to pass normal data traffic over UNIX domain socket rather than over TCP socket on node-local data transfer. This is currently experimental and turned off by default.
dfs.namenode.reject-unresolved-dn-topology-mapping	false	If the value is set to true, then namenode will reject datanode registration if the topology mapping for a datanode is not resolved and NULL is returned (script defined by net.topology.script.file.name fails to execute). Otherwise, datanode will be registered and the default rack will be assigned as the topology path. Topology paths are important for data resiliency, since they define fault domains. Thus it may be unwanted behavior to allow datanode registration with the default rack if the resolving topology failed.
dfs.client.slow.io.warning.threshold.ms	30000	The threshold in milliseconds at which we will log a slow io warning in a dfsclient. By default, this parameter is set to 30000 milliseconds (30 seconds).
dfs.datanode.slow.io.warning.threshold.ms	300	The threshold in milliseconds at which we will log a slow io warning in a datanode. By default, this parameter is set to 300 milliseconds.
dfs.namenode.xattrs.enabled	true	Whether support for extended attributes is enabled on the NameNode.
dfs.namenode.fs-limits.max-xattrs-per-inode	32	Maximum number of extended attributes per inode.
dfs.namenode.fs-limits.max-xattr-size	16384	The maximum combined size of the name and value of an extended attribute in bytes.
dfs.namenode.startup.delay.block.deletion.sec	0	The delay in seconds at which we will pause the blocks deletion after Namenode startup. By default it's disabled. In the case a directory has large number of directories and files are deleted, suggested delay is one hour to give the administrator enough time to notice large number of pending deletion blocks and take corrective action.
dfs.namenode.list.encryption.zones.num.responses	100	When listing encryption zones, the maximum number of zones that will be returned in a batch. Fetching the list incrementally in batches improves namenode performance.
dfs.namenode.inotify.max.events.per.rpc	1000	Maximum number of events that will be sent to an inotify client in a single RPC response. The default value attempts to amortize away the overhead for this RPC while avoiding huge memory requirements for the client and NameNode (1000 events should consume no more than 1 MB.)
dfs.user.home.dir.prefix	/user	The directory to prepend to user name to get the user's home direcotry.
dfs.datanode.cache.revocation.timeout.ms	900000	When the DFSClient reads from a block file which the DataNode is caching, the DFSClient can skip verifying checksums. The DataNode will keep the block file in cache until the client is done. If the client takes an unusually long time, though, the DataNode may need to evict the block file from the cache anyway. This value controls how long the DataNode will wait for the client to release a replica that it is reading without checksums.
dfs.datanode.cache.revocation.polling.ms	500	How often the DataNode should poll to see if the clients have stopped using a replica that the DataNode wants to uncache.
dfs.datanode.block.id.layout.upgrade.threads	12	The number of threads to use when creating hard links from current to previous blocks during upgrade of a DataNode to block ID-based block layout (see HDFS-6482 for details on the layout).
dfs.encryption.key.provider.uri		The KeyProvider to use when interacting with encryption keys used when reading and writing to an encryption zone.
dfs.storage.policy.enabled	true	Allow users to change the storage policy on files and directories.
dfs.namenode.legacy-oiv-image.dir		Determines where to save the namespace in the old fsimage format during checkpointing by standby NameNode or SecondaryNameNode. Users can dump the contents of the old format fsimage by oiv_legacy command. If the value is not specified, old format fsimage will not be saved in checkpoint.
dfs.namenode.top.enabled	true	Enable nntop: reporting top users on namenode
dfs.namenode.top.window.num.buckets	10	Number of buckets in the rolling window implementation of nntop
dfs.namenode.top.num.users	10	Number of top users returned by the top tool
dfs.namenode.top.windows.minutes	1,5,25	comma separated list of nntop reporting periods in minutes
dfs.namenode.blocks.per.postponedblocks.rescan	10000	Number of blocks to rescan for each iteration of postponedMisreplicatedBlocks.
dfs.datanode.block-pinning.enabled	false	Whether pin blocks on favored DataNode.
dfs.datanode.bp-ready.timeout	20	The maximum wait time for datanode to be ready before failing the received request. Setting this to 0 fails requests right away if the datanode is not yet registered with the namenode. This wait time reduces initial request failures after datanode restart. 
name	value	description
mapreduce.jobtracker.jobhistory.location		If job tracker is static the history files are stored in this single well known place. If No value is set here, by default, it is in the local file system at ${hadoop.log.dir}/history.
mapreduce.jobtracker.jobhistory.task.numberprogresssplits	12	Every task attempt progresses from 0.0 to 1.0 [unless it fails or is killed]. We record, for each task attempt, certain statistics over each twelfth of the progress range. You can change the number of intervals we divide the entire range of progress into by setting this property. Higher values give more precision to the recorded data, but costs more memory in the job tracker at runtime. Each increment in this attribute costs 16 bytes per running task.
mapreduce.job.userhistorylocation		User can specify a location to store the history files of a particular job. If nothing is specified, the logs are stored in output directory. The files are stored in "_logs/history/" in the directory. User can stop logging by giving the value "none".
mapreduce.jobtracker.jobhistory.completed.location		The completed job history files are stored at this single well known location. If nothing is specified, the files are stored at ${mapreduce.jobtracker.jobhistory.location}/done.
mapreduce.job.committer.setup.cleanup.needed	true	true, if job needs job-setup and job-cleanup. false, otherwise
mapreduce.task.io.sort.factor	10	The number of streams to merge at once while sorting files. This determines the number of open file handles.
mapreduce.task.io.sort.mb	100	The total amount of buffer memory to use while sorting files, in megabytes. By default, gives each merge stream 1MB, which should minimize seeks.
mapreduce.map.sort.spill.percent	0.80	The soft limit in the serialization buffer. Once reached, a thread will begin to spill the contents to disk in the background. Note that collection will not block if this threshold is exceeded while a spill is already in progress, so spills may be larger than this threshold when it is set to less than .5
mapreduce.jobtracker.address	local	The host and port that the MapReduce job tracker runs at. If "local", then jobs are run in-process as a single map and reduce task.
mapreduce.local.clientfactory.class.name	org.apache.hadoop.mapred.LocalClientFactory	This the client factory that is responsible for creating local job runner client
mapreduce.jobtracker.http.address	0.0.0.0:50030	The job tracker http server address and port the server will listen on. If the port is 0 then the server will start on a free port.
mapreduce.jobtracker.handler.count	10	The number of server threads for the JobTracker. This should be roughly 4% of the number of tasktracker nodes.
mapreduce.tasktracker.report.address	127.0.0.1:0	The interface and port that task tracker server listens on. Since it is only connected to by the tasks, it uses the local interface. EXPERT ONLY. Should only be changed if your host does not have the loopback interface.
mapreduce.cluster.local.dir	${hadoop.tmp.dir}/mapred/local	The local directory where MapReduce stores intermediate data files. May be a comma-separated list of directories on different devices in order to spread disk i/o. Directories that do not exist are ignored.
mapreduce.jobtracker.system.dir	${hadoop.tmp.dir}/mapred/system	The directory where MapReduce stores control files.
mapreduce.jobtracker.staging.root.dir	${hadoop.tmp.dir}/mapred/staging	The root of the staging area for users' job files In practice, this should be the directory where users' home directories are located (usually /user)
mapreduce.cluster.temp.dir	${hadoop.tmp.dir}/mapred/temp	A shared directory for temporary files.
mapreduce.tasktracker.local.dir.minspacestart	0	If the space in mapreduce.cluster.local.dir drops under this, do not ask for more tasks. Value in bytes.
mapreduce.tasktracker.local.dir.minspacekill	0	If the space in mapreduce.cluster.local.dir drops under this, do not ask more tasks until all the current ones have finished and cleaned up. Also, to save the rest of the tasks we have running, kill one of them, to clean up some space. Start with the reduce tasks, then go with the ones that have finished the least. Value in bytes.
mapreduce.jobtracker.expire.trackers.interval	600000	Expert: The time-interval, in miliseconds, after which a tasktracker is declared 'lost' if it doesn't send heartbeats.
mapreduce.tasktracker.instrumentation	org.apache.hadoop.mapred.TaskTrackerMetricsInst	Expert: The instrumentation class to associate with each TaskTracker.
mapreduce.tasktracker.resourcecalculatorplugin		Name of the class whose instance will be used to query resource information on the tasktracker. The class must be an instance of org.apache.hadoop.util.ResourceCalculatorPlugin. If the value is null, the tasktracker attempts to use a class appropriate to the platform. Currently, the only platform supported is Linux.
mapreduce.tasktracker.taskmemorymanager.monitoringinterval	5000	The interval, in milliseconds, for which the tasktracker waits between two cycles of monitoring its tasks' memory usage. Used only if tasks' memory management is enabled via mapred.tasktracker.tasks.maxmemory.
mapreduce.tasktracker.tasks.sleeptimebeforesigkill	5000	The time, in milliseconds, the tasktracker waits for sending a SIGKILL to a task, after it has been sent a SIGTERM. This is currently not used on WINDOWS where tasks are just sent a SIGTERM.
mapreduce.job.maps	2	The default number of map tasks per job. Ignored when mapreduce.jobtracker.address is "local".
mapreduce.job.reduces	1	The default number of reduce tasks per job. Typically set to 99% of the cluster's reduce capacity, so that if a node fails the reduces can still be executed in a single wave. Ignored when mapreduce.jobtracker.address is "local".
mapreduce.jobtracker.restart.recover	false	"true" to enable (job) recovery upon restart, "false" to start afresh
mapreduce.jobtracker.jobhistory.block.size	3145728	The block size of the job history file. Since the job recovery uses job history, its important to dump job history to disk as soon as possible. Note that this is an expert level parameter. The default value is set to 3 MB.
mapreduce.jobtracker.taskscheduler	org.apache.hadoop.mapred.JobQueueTaskScheduler	The class responsible for scheduling the tasks.
mapreduce.job.running.map.limit	0	The maximum number of simultaneous map tasks per job. There is no limit if this value is 0 or negative.
mapreduce.job.running.reduce.limit	0	The maximum number of simultaneous reduce tasks per job. There is no limit if this value is 0 or negative.
mapreduce.job.reducer.preempt.delay.sec	0	The threshold in terms of seconds after which an unsatisfied mapper request triggers reducer preemption to free space. Default 0 implies that the reduces should be preempted immediately after allocation if there is currently no room for newly allocated mappers.
mapreduce.job.max.split.locations	10	The max number of block locations to store for each split for locality calculation.
mapreduce.job.split.metainfo.maxsize	10000000	The maximum permissible size of the split metainfo file. The JobTracker won't attempt to read split metainfo files bigger than the configured value. No limits if set to -1.
mapreduce.jobtracker.taskscheduler.maxrunningtasks.perjob		The maximum number of running tasks for a job before it gets preempted. No limits if undefined.
mapreduce.map.maxattempts	4	Expert: The maximum number of attempts per map task. In other words, framework will try to execute a map task these many number of times before giving up on it.
mapreduce.reduce.maxattempts	4	Expert: The maximum number of attempts per reduce task. In other words, framework will try to execute a reduce task these many number of times before giving up on it.
mapreduce.reduce.shuffle.fetch.retry.enabled	${yarn.nodemanager.recovery.enabled}	Set to enable fetch retry during host restart.
mapreduce.reduce.shuffle.fetch.retry.interval-ms	1000	Time of interval that fetcher retry to fetch again when some non-fatal failure happens because of some events like NM restart.
mapreduce.reduce.shuffle.fetch.retry.timeout-ms	30000	Timeout value for fetcher to retry to fetch again when some non-fatal failure happens because of some events like NM restart.
mapreduce.reduce.shuffle.retry-delay.max.ms	60000	The maximum number of ms the reducer will delay before retrying to download map data.
mapreduce.reduce.shuffle.parallelcopies	5	The default number of parallel transfers run by reduce during the copy(shuffle) phase.
mapreduce.reduce.shuffle.connect.timeout	180000	Expert: The maximum amount of time (in milli seconds) reduce task spends in trying to connect to a tasktracker for getting map output.
mapreduce.reduce.shuffle.read.timeout	180000	Expert: The maximum amount of time (in milli seconds) reduce task waits for map output data to be available for reading after obtaining connection.
mapreduce.shuffle.connection-keep-alive.enable	false	set to true to support keep-alive connections.
mapreduce.shuffle.connection-keep-alive.timeout	5	The number of seconds a shuffle client attempts to retain http connection. Refer "Keep-Alive: timeout=" header in Http specification
mapreduce.task.timeout	600000	The number of milliseconds before a task will be terminated if it neither reads an input, writes an output, nor updates its status string. A value of 0 disables the timeout.
mapreduce.tasktracker.map.tasks.maximum	2	The maximum number of map tasks that will be run simultaneously by a task tracker.
mapreduce.tasktracker.reduce.tasks.maximum	2	The maximum number of reduce tasks that will be run simultaneously by a task tracker.
mapreduce.map.memory.mb	1024	The amount of memory to request from the scheduler for each map task.
mapreduce.map.cpu.vcores	1	The number of virtual cores to request from the scheduler for each map task.
mapreduce.reduce.memory.mb	1024	The amount of memory to request from the scheduler for each reduce task.
mapreduce.reduce.cpu.vcores	1	The number of virtual cores to request from the scheduler for each reduce task.
mapreduce.jobtracker.retiredjobs.cache.size	1000	The number of retired job status to keep in the cache.
mapreduce.tasktracker.outofband.heartbeat	false	Expert: Set this to true to let the tasktracker send an out-of-band heartbeat on task-completion for better latency.
mapreduce.jobtracker.jobhistory.lru.cache.size	5	The number of job history files loaded in memory. The jobs are loaded when they are first accessed. The cache is cleared based on LRU.
mapreduce.jobtracker.instrumentation	org.apache.hadoop.mapred.JobTrackerMetricsInst	Expert: The instrumentation class to associate with each JobTracker.
mapred.child.java.opts	-Xmx200m	Java opts for the task processes. The following symbol, if present, will be interpolated: @taskid@ is replaced by current TaskID. Any other occurrences of '@' will go unchanged. For example, to enable verbose gc logging to a file named for the taskid in /tmp and to set the heap maximum to be a gigabyte, pass a 'value' of: -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc Usage of -Djava.library.path can cause programs to no longer function if hadoop native libraries are used. These values should instead be set as part of LD_LIBRARY_PATH in the map / reduce JVM env using the mapreduce.map.env and mapreduce.reduce.env config settings.
mapred.child.env		User added environment variables for the task processes. Example : 1) A=foo This will set the env variable A to foo 2) B=$B:c This is inherit nodemanager's B env variable on Unix. 3) B=%B%;c This is inherit nodemanager's B env variable on Windows.
mapreduce.admin.user.env		Expert: Additional execution environment entries for map and reduce task processes. This is not an additive property. You must preserve the original value if you want your map and reduce tasks to have access to native libraries (compression, etc). When this value is empty, the command to set execution envrionment will be OS dependent: For linux, use LD_LIBRARY_PATH=$HADOOP_COMMON_HOME/lib/native. For windows, use PATH = %PATH%;%HADOOP_COMMON_HOME%\\bin.
mapreduce.map.log.level	INFO	The logging level for the map task. The allowed levels are: OFF, FATAL, ERROR, WARN, INFO, DEBUG, TRACE and ALL. The setting here could be overridden if "mapreduce.job.log4j-properties-file" is set.
mapreduce.reduce.log.level	INFO	The logging level for the reduce task. The allowed levels are: OFF, FATAL, ERROR, WARN, INFO, DEBUG, TRACE and ALL. The setting here could be overridden if "mapreduce.job.log4j-properties-file" is set.
mapreduce.map.cpu.vcores	1	The number of virtual cores required for each map task.
mapreduce.reduce.cpu.vcores	1	The number of virtual cores required for each reduce task.
mapreduce.reduce.merge.inmem.threshold	1000	The threshold, in terms of the number of files for the in-memory merge process. When we accumulate threshold number of files we initiate the in-memory merge and spill to disk. A value of 0 or less than 0 indicates we want to DON'T have any threshold and instead depend only on the ramfs's memory consumption to trigger the merge.
mapreduce.reduce.shuffle.merge.percent	0.66	The usage threshold at which an in-memory merge will be initiated, expressed as a percentage of the total memory allocated to storing in-memory map outputs, as defined by mapreduce.reduce.shuffle.input.buffer.percent.
mapreduce.reduce.shuffle.input.buffer.percent	0.70	The percentage of memory to be allocated from the maximum heap size to storing map outputs during the shuffle.
mapreduce.reduce.input.buffer.percent	0.0	The percentage of memory- relative to the maximum heap size- to retain map outputs during the reduce. When the shuffle is concluded, any remaining map outputs in memory must consume less than this threshold before the reduce can begin.
mapreduce.reduce.shuffle.memory.limit.percent	0.25	Expert: Maximum percentage of the in-memory limit that a single shuffle can consume
mapreduce.shuffle.ssl.enabled	false	Whether to use SSL for for the Shuffle HTTP endpoints.
mapreduce.shuffle.ssl.file.buffer.size	65536	Buffer size for reading spills from file when using SSL.
mapreduce.shuffle.max.connections	0	Max allowed connections for the shuffle. Set to 0 (zero) to indicate no limit on the number of connections.
mapreduce.shuffle.max.threads	0	Max allowed threads for serving shuffle connections. Set to zero to indicate the default of 2 times the number of available processors (as reported by Runtime.availableProcessors()). Netty is used to serve requests, so a thread is not needed for each connection.
mapreduce.shuffle.transferTo.allowed		This option can enable/disable using nio transferTo method in the shuffle phase. NIO transferTo does not perform well on windows in the shuffle phase. Thus, with this configuration property it is possible to disable it, in which case custom transfer method will be used. Recommended value is false when running Hadoop on Windows. For Linux, it is recommended to set it to true. If nothing is set then the default value is false for Windows, and true for Linux.
mapreduce.shuffle.transfer.buffer.size	131072	This property is used only if mapreduce.shuffle.transferTo.allowed is set to false. In that case, this property defines the size of the buffer used in the buffer copy code for the shuffle phase. The size of this buffer determines the size of the IO requests.
mapreduce.reduce.markreset.buffer.percent	0.0	The percentage of memory -relative to the maximum heap size- to be used for caching values when using the mark-reset functionality.
mapreduce.map.speculative	true	If true, then multiple instances of some map tasks may be executed in parallel.
mapreduce.reduce.speculative	true	If true, then multiple instances of some reduce tasks may be executed in parallel.
mapreduce.job.speculative.speculative-cap-running-tasks	0.1	The max percent (0-1) of running tasks that can be speculatively re-executed at any time.
mapreduce.job.speculative.speculative-cap-total-tasks	0.01	The max percent (0-1) of all tasks that can be speculatively re-executed at any time.
mapreduce.job.speculative.minimum-allowed-tasks	10	The minimum allowed tasks that can be speculatively re-executed at any time.
mapreduce.job.speculative.retry-after-no-speculate	1000	The waiting time(ms) to do next round of speculation if there is no task speculated in this round.
mapreduce.job.speculative.retry-after-speculate	15000	The waiting time(ms) to do next round of speculation if there are tasks speculated in this round.
mapreduce.job.map.output.collector.class	org.apache.hadoop.mapred.MapTask$MapOutputBuffer	The MapOutputCollector implementation(s) to use. This may be a comma-separated list of class names, in which case the map task will try to initialize each of the collectors in turn. The first to successfully initialize will be used.
mapreduce.job.speculative.slowtaskthreshold	1.0	The number of standard deviations by which a task's ave progress-rates must be lower than the average of all running tasks' for the task to be considered too slow.
mapreduce.job.jvm.numtasks	1	How many tasks to run per jvm. If set to -1, there is no limit.
mapreduce.job.ubertask.enable	false	Whether to enable the small-jobs "ubertask" optimization, which runs "sufficiently small" jobs sequentially within a single JVM. "Small" is defined by the following maxmaps, maxreduces, and maxbytes settings. Note that configurations for application masters also affect the "Small" definition - yarn.app.mapreduce.am.resource.mb must be larger than both mapreduce.map.memory.mb and mapreduce.reduce.memory.mb, and yarn.app.mapreduce.am.resource.cpu-vcores must be larger than both mapreduce.map.cpu.vcores and mapreduce.reduce.cpu.vcores to enable ubertask. Users may override this value.
mapreduce.job.ubertask.maxmaps	9	Threshold for number of maps, beyond which job is considered too big for the ubertasking optimization. Users may override this value, but only downward.
mapreduce.job.ubertask.maxreduces	1	Threshold for number of reduces, beyond which job is considered too big for the ubertasking optimization. CURRENTLY THE CODE CANNOT SUPPORT MORE THAN ONE REDUCE and will ignore larger values. (Zero is a valid max, however.) Users may override this value, but only downward.
mapreduce.job.ubertask.maxbytes		Threshold for number of input bytes, beyond which job is considered too big for the ubertasking optimization. If no value is specified, dfs.block.size is used as a default. Be sure to specify a default value in mapred-site.xml if the underlying filesystem is not HDFS. Users may override this value, but only downward.
mapreduce.job.emit-timeline-data	false	Specifies if the Application Master should emit timeline data to the timeline server. Individual jobs can override this value.
mapreduce.input.fileinputformat.split.minsize	0	The minimum size chunk that map input should be split into. Note that some file formats may have minimum split sizes that take priority over this setting.
mapreduce.input.fileinputformat.list-status.num-threads	1	The number of threads to use to list and fetch block locations for the specified input paths. Note: multiple threads should not be used if a custom non thread-safe path filter is used.
mapreduce.jobtracker.maxtasks.perjob	-1	The maximum number of tasks for a single job. A value of -1 indicates that there is no maximum.
mapreduce.input.lineinputformat.linespermap	1	When using NLineInputFormat, the number of lines of input data to include in each split.
mapreduce.client.submit.file.replication	10	The replication level for submitted job files. This should be around the square root of the number of nodes.
mapreduce.tasktracker.dns.interface	default	The name of the Network Interface from which a task tracker should report its IP address.
mapreduce.tasktracker.dns.nameserver	default	The host name or IP address of the name server (DNS) which a TaskTracker should use to determine the host name used by the JobTracker for communication and display purposes.
mapreduce.tasktracker.http.threads	40	The number of worker threads that for the http server. This is used for map output fetching
mapreduce.tasktracker.http.address	0.0.0.0:50060	The task tracker http server address and port. If the port is 0 then the server will start on a free port.
mapreduce.task.files.preserve.failedtasks	false	Should the files for failed tasks be kept. This should only be used on jobs that are failing, because the storage is never reclaimed. It also prevents the map outputs from being erased from the reduce directory as they are consumed.
mapreduce.output.fileoutputformat.compress	false	Should the job outputs be compressed?
mapreduce.output.fileoutputformat.compress.type	RECORD	If the job outputs are to compressed as SequenceFiles, how should they be compressed? Should be one of NONE, RECORD or BLOCK.
mapreduce.output.fileoutputformat.compress.codec	org.apache.hadoop.io.compress.DefaultCodec	If the job outputs are compressed, how should they be compressed?
mapreduce.map.output.compress	false	Should the outputs of the maps be compressed before being sent across the network. Uses SequenceFile compression.
mapreduce.map.output.compress.codec	org.apache.hadoop.io.compress.DefaultCodec	If the map outputs are compressed, how should they be compressed?
map.sort.class	org.apache.hadoop.util.QuickSort	The default sort class for sorting keys.
mapreduce.task.userlog.limit.kb	0	The maximum size of user-logs of each task in KB. 0 disables the cap.
yarn.app.mapreduce.am.container.log.limit.kb	0	The maximum size of the MRAppMaster attempt container logs in KB. 0 disables the cap.
yarn.app.mapreduce.task.container.log.backups	0	Number of backup files for task logs when using ContainerRollingLogAppender (CRLA). See org.apache.log4j.RollingFileAppender.maxBackupIndex. By default, ContainerLogAppender (CLA) is used, and container logs are not rolled. CRLA is enabled for tasks when both mapreduce.task.userlog.limit.kb and yarn.app.mapreduce.task.container.log.backups are greater than zero.
yarn.app.mapreduce.am.container.log.backups	0	Number of backup files for the ApplicationMaster logs when using ContainerRollingLogAppender (CRLA). See org.apache.log4j.RollingFileAppender.maxBackupIndex. By default, ContainerLogAppender (CLA) is used, and container logs are not rolled. CRLA is enabled for the ApplicationMaster when both mapreduce.task.userlog.limit.kb and yarn.app.mapreduce.am.container.log.backups are greater than zero.
yarn.app.mapreduce.shuffle.log.separate	true	If enabled ('true') logging generated by the client-side shuffle classes in a reducer will be written in a dedicated log file 'syslog.shuffle' instead of 'syslog'.
yarn.app.mapreduce.shuffle.log.limit.kb	0	Maximum size of the syslog.shuffle file in kilobytes (0 for no limit).
yarn.app.mapreduce.shuffle.log.backups	0	If yarn.app.mapreduce.shuffle.log.limit.kb and yarn.app.mapreduce.shuffle.log.backups are greater than zero then a ContainerRollngLogAppender is used instead of ContainerLogAppender for syslog.shuffle. See org.apache.log4j.RollingFileAppender.maxBackupIndex
mapreduce.job.userlog.retain.hours	24	The maximum time, in hours, for which the user-logs are to be retained after the job completion.
mapreduce.jobtracker.hosts.filename		Names a file that contains the list of nodes that may connect to the jobtracker. If the value is empty, all hosts are permitted.
mapreduce.jobtracker.hosts.exclude.filename		Names a file that contains the list of hosts that should be excluded by the jobtracker. If the value is empty, no hosts are excluded.
mapreduce.jobtracker.heartbeats.in.second	100	Expert: Approximate number of heart-beats that could arrive at JobTracker in a second. Assuming each RPC can be processed in 10msec, the default value is made 100 RPCs in a second.
mapreduce.jobtracker.tasktracker.maxblacklists	4	The number of blacklists for a taskTracker by various jobs after which the task tracker could be blacklisted across all jobs. The tracker will be given a tasks later (after a day). The tracker will become a healthy tracker after a restart.
mapreduce.job.maxtaskfailures.per.tracker	3	The number of task-failures on a tasktracker of a given job after which new tasks of that job aren't assigned to it. It MUST be less than mapreduce.map.maxattempts and mapreduce.reduce.maxattempts otherwise the failed task will never be tried on a different node.
mapreduce.client.output.filter	FAILED	The filter for controlling the output of the task's userlogs sent to the console of the JobClient. The permissible options are: NONE, KILLED, FAILED, SUCCEEDED and ALL.
mapreduce.client.completion.pollinterval	5000	The interval (in milliseconds) between which the JobClient polls the JobTracker for updates about job status. You may want to set this to a lower value to make tests run faster on a single node system. Adjusting this value in production may lead to unwanted client-server traffic.
mapreduce.client.progressmonitor.pollinterval	1000	The interval (in milliseconds) between which the JobClient reports status to the console and checks for job completion. You may want to set this to a lower value to make tests run faster on a single node system. Adjusting this value in production may lead to unwanted client-server traffic.
mapreduce.jobtracker.persist.jobstatus.active	true	Indicates if persistency of job status information is active or not.
mapreduce.jobtracker.persist.jobstatus.hours	1	The number of hours job status information is persisted in DFS. The job status information will be available after it drops of the memory queue and between jobtracker restarts. With a zero value the job status information is not persisted at all in DFS.
mapreduce.jobtracker.persist.jobstatus.dir	/jobtracker/jobsInfo	The directory where the job status information is persisted in a file system to be available after it drops of the memory queue and between jobtracker restarts.
mapreduce.task.profile	false	To set whether the system should collect profiler information for some of the tasks in this job? The information is stored in the user log directory. The value is "true" if task profiling is enabled.
mapreduce.task.profile.maps	0-2	To set the ranges of map tasks to profile. mapreduce.task.profile has to be set to true for the value to be accounted.
mapreduce.task.profile.reduces	0-2	To set the ranges of reduce tasks to profile. mapreduce.task.profile has to be set to true for the value to be accounted.
mapreduce.task.profile.params	-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s	JVM profiler parameters used to profile map and reduce task attempts. This string may contain a single format specifier %s that will be replaced by the path to profile.out in the task attempt log directory. To specify different profiling options for map tasks and reduce tasks, more specific parameters mapreduce.task.profile.map.params and mapreduce.task.profile.reduce.params should be used.
mapreduce.task.profile.map.params	${mapreduce.task.profile.params}	Map-task-specific JVM profiler parameters. See mapreduce.task.profile.params
mapreduce.task.profile.reduce.params	${mapreduce.task.profile.params}	Reduce-task-specific JVM profiler parameters. See mapreduce.task.profile.params
mapreduce.task.skip.start.attempts	2	The number of Task attempts AFTER which skip mode will be kicked off. When skip mode is kicked off, the tasks reports the range of records which it will process next, to the TaskTracker. So that on failures, TT knows which ones are possibly the bad records. On further executions, those are skipped.
mapreduce.map.skip.proc.count.autoincr	true	The flag which if set to true, SkipBadRecords.COUNTER_MAP_PROCESSED_RECORDS is incremented by MapRunner after invoking the map function. This value must be set to false for applications which process the records asynchronously or buffer the input records. For example streaming. In such cases applications should increment this counter on their own.
mapreduce.reduce.skip.proc.count.autoincr	true	The flag which if set to true, SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS is incremented by framework after invoking the reduce function. This value must be set to false for applications which process the records asynchronously or buffer the input records. For example streaming. In such cases applications should increment this counter on their own.
mapreduce.job.skip.outdir		If no value is specified here, the skipped records are written to the output directory at _logs/skip. User can stop writing skipped records by giving the value "none".
mapreduce.map.skip.maxrecords	0	The number of acceptable skip records surrounding the bad record PER bad record in mapper. The number includes the bad record as well. To turn the feature of detection/skipping of bad records off, set the value to 0. The framework tries to narrow down the skipped range by retrying until this threshold is met OR all attempts get exhausted for this task. Set the value to Long.MAX_VALUE to indicate that framework need not try to narrow down. Whatever records(depends on application) get skipped are acceptable.
mapreduce.reduce.skip.maxgroups	0	The number of acceptable skip groups surrounding the bad group PER bad group in reducer. The number includes the bad group as well. To turn the feature of detection/skipping of bad groups off, set the value to 0. The framework tries to narrow down the skipped range by retrying until this threshold is met OR all attempts get exhausted for this task. Set the value to Long.MAX_VALUE to indicate that framework need not try to narrow down. Whatever groups(depends on application) get skipped are acceptable.
mapreduce.ifile.readahead	true	Configuration key to enable/disable IFile readahead.
mapreduce.ifile.readahead.bytes	4194304	Configuration key to set the IFile readahead length in bytes.
mapreduce.jobtracker.taskcache.levels	2	This is the max level of the task cache. For example, if the level is 2, the tasks cached are at the host level and at the rack level.
mapreduce.job.queuename	default	Queue to which a job is submitted. This must match one of the queues defined in mapred-queues.xml for the system. Also, the ACL setup for the queue must allow the current user to submit a job to the queue. Before specifying a queue, ensure that the system is configured with the queue, and access is allowed for submitting jobs to the queue.
mapreduce.job.tags		Tags for the job that will be passed to YARN at submission time. Queries to YARN for applications can filter on these tags.
mapreduce.cluster.acls.enabled	false	Specifies whether ACLs should be checked for authorization of users for doing various queue and job level operations. ACLs are disabled by default. If enabled, access control checks are made by JobTracker and TaskTracker when requests are made by users for queue operations like submit job to a queue and kill a job in the queue and job operations like viewing the job-details (See mapreduce.job.acl-view-job) or for modifying the job (See mapreduce.job.acl-modify-job) using Map/Reduce APIs, RPCs or via the console and web user interfaces. For enabling this flag(mapreduce.cluster.acls.enabled), this is to be set to true in mapred-site.xml on JobTracker node and on all TaskTracker nodes.
mapreduce.job.acl-modify-job		Job specific access-control list for 'modifying' the job. It is only used if authorization is enabled in Map/Reduce by setting the configuration property mapreduce.cluster.acls.enabled to true. This specifies the list of users and/or groups who can do modification operations on the job. For specifying a list of users and groups the format to use is "user1,user2 group1,group". If set to '*', it allows all users/groups to modify this job. If set to ' '(i.e. space), it allows none. This configuration is used to guard all the modifications with respect to this job and takes care of all the following operations: o killing this job o killing a task of this job, failing a task of this job o setting the priority of this job Each of these operations are also protected by the per-queue level ACL "acl-administer-jobs" configured via mapred-queues.xml. So a caller should have the authorization to satisfy either the queue-level ACL or the job-level ACL. Irrespective of this ACL configuration, (a) job-owner, (b) the user who started the cluster, (c) members of an admin configured supergroup configured via mapreduce.cluster.permissions.supergroup and (d) queue administrators of the queue to which this job was submitted to configured via acl-administer-jobs for the specific queue in mapred-queues.xml can do all the modification operations on a job. By default, nobody else besides job-owner, the user who started the cluster, members of supergroup and queue administrators can perform modification operations on a job.
mapreduce.job.acl-view-job		Job specific access-control list for 'viewing' the job. It is only used if authorization is enabled in Map/Reduce by setting the configuration property mapreduce.cluster.acls.enabled to true. This specifies the list of users and/or groups who can view private details about the job. For specifying a list of users and groups the format to use is "user1,user2 group1,group". If set to '*', it allows all users/groups to modify this job. If set to ' '(i.e. space), it allows none. This configuration is used to guard some of the job-views and at present only protects APIs that can return possibly sensitive information of the job-owner like o job-level counters o task-level counters o tasks' diagnostic information o task-logs displayed on the TaskTracker web-UI and o job.xml showed by the JobTracker's web-UI Every other piece of information of jobs is still accessible by any other user, for e.g., JobStatus, JobProfile, list of jobs in the queue, etc. Irrespective of this ACL configuration, (a) job-owner, (b) the user who started the cluster, (c) members of an admin configured supergroup configured via mapreduce.cluster.permissions.supergroup and (d) queue administrators of the queue to which this job was submitted to configured via acl-administer-jobs for the specific queue in mapred-queues.xml can do all the view operations on a job. By default, nobody else besides job-owner, the user who started the cluster, memebers of supergroup and queue administrators can perform view operations on a job.
mapreduce.tasktracker.indexcache.mb	10	The maximum memory that a task tracker allows for the index cache that is used when serving map outputs to reducers.
mapreduce.job.token.tracking.ids.enabled	false	Whether to write tracking ids of tokens to job-conf. When true, the configuration property "mapreduce.job.token.tracking.ids" is set to the token-tracking-ids of the job
mapreduce.job.token.tracking.ids		When mapreduce.job.token.tracking.ids.enabled is set to true, this is set by the framework to the token-tracking-ids used by the job.
mapreduce.task.merge.progress.records	10000	The number of records to process during merge before sending a progress notification to the TaskTracker.
mapreduce.task.combine.progress.records	10000	The number of records to process during combine output collection before sending a progress notification.
mapreduce.job.reduce.slowstart.completedmaps	0.05	Fraction of the number of maps in the job which should be complete before reduces are scheduled for the job.
mapreduce.job.complete.cancel.delegation.tokens	true	if false - do not unregister/cancel delegation tokens from renewal, because same tokens may be used by spawned jobs
mapreduce.tasktracker.taskcontroller	org.apache.hadoop.mapred.DefaultTaskController	TaskController which is used to launch and manage task execution
mapreduce.tasktracker.group		Expert: Group to which TaskTracker belongs. If LinuxTaskController is configured via mapreduce.tasktracker.taskcontroller, the group owner of the task-controller binary should be same as this group.
mapreduce.shuffle.port	13562	Default port that the ShuffleHandler will run on. ShuffleHandler is a service run at the NodeManager to facilitate transfers of intermediate Map outputs to requesting Reducers.
mapreduce.job.reduce.shuffle.consumer.plugin.class	org.apache.hadoop.mapreduce.task.reduce.Shuffle	Name of the class whose instance will be used to send shuffle requests by reducetasks of this job. The class must be an instance of org.apache.hadoop.mapred.ShuffleConsumerPlugin.
mapreduce.tasktracker.healthchecker.script.path		Absolute path to the script which is periodicallyrun by the node health monitoring service to determine if the node is healthy or not. If the value of this key is empty or the file does not exist in the location configured here, the node health monitoring service is not started.
mapreduce.tasktracker.healthchecker.interval	60000	Frequency of the node health script to be run, in milliseconds
mapreduce.tasktracker.healthchecker.script.timeout	600000	Time after node health script should be killed if unresponsive and considered that the script has failed.
mapreduce.tasktracker.healthchecker.script.args		List of arguments which are to be passed to node health script when it is being launched comma seperated.
mapreduce.job.counters.limit	120	Limit on the number of user counters allowed per job.
mapreduce.framework.name	local	The runtime framework for executing MapReduce jobs. Can be one of local, classic or yarn.
yarn.app.mapreduce.am.staging-dir	/tmp/hadoop-yarn/staging	The staging dir used while submitting jobs.
mapreduce.am.max-attempts	2	The maximum number of application attempts. It is a application-specific setting. It should not be larger than the global number set by resourcemanager. Otherwise, it will be override. The default number is set to 2, to allow at least one retry for AM.
mapreduce.job.end-notification.url		Indicates url which will be called on completion of job to inform end status of job. User can give at most 2 variables with URI : $jobId and $jobStatus. If they are present in URI, then they will be replaced by their respective values.
mapreduce.job.end-notification.retry.attempts	0	The number of times the submitter of the job wants to retry job end notification if it fails. This is capped by mapreduce.job.end-notification.max.attempts
mapreduce.job.end-notification.retry.interval	1000	The number of milliseconds the submitter of the job wants to wait before job end notification is retried if it fails. This is capped by mapreduce.job.end-notification.max.retry.interval
mapreduce.job.end-notification.max.attempts	5	The maximum number of times a URL will be read for providing job end notification. Cluster administrators can set this to limit how long after end of a job, the Application Master waits before exiting. Must be marked as final to prevent users from overriding this.
mapreduce.job.log4j-properties-file		Used to override the default settings of log4j in container-log4j.properties for NodeManager. Like container-log4j.properties, it requires certain framework appenders properly defined in this overriden file. The file on the path will be added to distributed cache and classpath. If no-scheme is given in the path, it defaults to point to a log4j file on the local FS.
mapreduce.job.end-notification.max.retry.interval	5000	The maximum amount of time (in milliseconds) to wait before retrying job end notification. Cluster administrators can set this to limit how long the Application Master waits before exiting. Must be marked as final to prevent users from overriding this.
yarn.app.mapreduce.am.env		User added environment variables for the MR App Master processes. Example : 1) A=foo This will set the env variable A to foo 2) B=$B:c This is inherit tasktracker's B env variable.
yarn.app.mapreduce.am.admin.user.env		Environment variables for the MR App Master processes for admin purposes. These values are set first and can be overridden by the user env (yarn.app.mapreduce.am.env) Example : 1) A=foo This will set the env variable A to foo 2) B=$B:c This is inherit app master's B env variable.
yarn.app.mapreduce.am.command-opts	-Xmx1024m	Java opts for the MR App Master processes. The following symbol, if present, will be interpolated: @taskid@ is replaced by current TaskID. Any other occurrences of '@' will go unchanged. For example, to enable verbose gc logging to a file named for the taskid in /tmp and to set the heap maximum to be a gigabyte, pass a 'value' of: -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc Usage of -Djava.library.path can cause programs to no longer function if hadoop native libraries are used. These values should instead be set as part of LD_LIBRARY_PATH in the map / reduce JVM env using the mapreduce.map.env and mapreduce.reduce.env config settings.
yarn.app.mapreduce.am.admin-command-opts		Java opts for the MR App Master processes for admin purposes. It will appears before the opts set by yarn.app.mapreduce.am.command-opts and thus its options can be overridden user. Usage of -Djava.library.path can cause programs to no longer function if hadoop native libraries are used. These values should instead be set as part of LD_LIBRARY_PATH in the map / reduce JVM env using the mapreduce.map.env and mapreduce.reduce.env config settings.
yarn.app.mapreduce.am.job.task.listener.thread-count	30	The number of threads used to handle RPC calls in the MR AppMaster from remote tasks
yarn.app.mapreduce.am.job.client.port-range		Range of ports that the MapReduce AM can use when binding. Leave blank if you want all possible ports. For example 50000-50050,50100-50200
yarn.app.mapreduce.am.job.committer.cancel-timeout	60000	The amount of time in milliseconds to wait for the output committer to cancel an operation if the job is killed
yarn.app.mapreduce.am.job.committer.commit-window	10000	Defines a time window in milliseconds for output commit operations. If contact with the RM has occurred within this window then commits are allowed, otherwise the AM will not allow output commits until contact with the RM has been re-established.
mapreduce.fileoutputcommitter.algorithm.version	1	The file output committer algorithm version valid algorithm version number: 1 or 2 default to 1, which is the original algorithm In algorithm version 1, 1. commitTask will rename directory $joboutput/_temporary/$appAttemptID/_temporary/$taskAttemptID/ to $joboutput/_temporary/$appAttemptID/$taskID/ 2. recoverTask will also do a rename $joboutput/_temporary/$appAttemptID/$taskID/ to $joboutput/_temporary/($appAttemptID + 1)/$taskID/ 3. commitJob will merge every task output file in $joboutput/_temporary/$appAttemptID/$taskID/ to $joboutput/, then it will delete $joboutput/_temporary/ and write $joboutput/_SUCCESS It has a performance regression, which is discussed in MAPREDUCE-4815. If a job generates many files to commit then the commitJob method call at the end of the job can take minutes. the commit is single-threaded and waits until all tasks have completed before commencing. algorithm version 2 will change the behavior of commitTask, recoverTask, and commitJob. 1. commitTask will rename all files in $joboutput/_temporary/$appAttemptID/_temporary/$taskAttemptID/ to $joboutput/ 2. recoverTask actually doesn't require to do anything, but for upgrade from version 1 to version 2 case, it will check if there are any files in $joboutput/_temporary/($appAttemptID - 1)/$taskID/ and rename them to $joboutput/ 3. commitJob can simply delete $joboutput/_temporary and write $joboutput/_SUCCESS This algorithm will reduce the output commit time for large jobs by having the tasks commit directly to the final output directory as they were completing and commitJob had very little to do.
yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms	1000	The interval in ms at which the MR AppMaster should send heartbeats to the ResourceManager
yarn.app.mapreduce.client-am.ipc.max-retries	3	The number of client retries to the AM - before reconnecting to the RM to fetch Application Status.
yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts	3	The number of client retries on socket timeouts to the AM - before reconnecting to the RM to fetch Application Status.
yarn.app.mapreduce.client.max-retries	3	The number of client retries to the RM/HS before throwing exception. This is a layer above the ipc.
yarn.app.mapreduce.am.resource.mb	1536	The amount of memory the MR AppMaster needs.
yarn.app.mapreduce.am.resource.cpu-vcores	1	The number of virtual CPU cores the MR AppMaster needs.
yarn.app.mapreduce.am.hard-kill-timeout-ms	10000	Number of milliseconds to wait before the job client kills the application.
yarn.app.mapreduce.client.job.max-retries	0	The number of retries the client will make for getJob and dependent calls. The default is 0 as this is generally only needed for non-HDFS DFS where additional, high level retries are required to avoid spurious failures during the getJob call. 30 is a good value for WASB
yarn.app.mapreduce.client.job.retry-interval	2000	The delay between getJob retries in ms for retries configured with yarn.app.mapreduce.client.job.max-retries.
mapreduce.application.classpath		CLASSPATH for MR applications. A comma-separated list of CLASSPATH entries. If mapreduce.application.framework is set then this must specify the appropriate classpath for that archive, and the name of the archive must be present in the classpath. If mapreduce.app-submission.cross-platform is false, platform-specific environment vairable expansion syntax would be used to construct the default CLASSPATH entries. For Linux: $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*, $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*. For Windows: %HADOOP_MAPRED_HOME%/share/hadoop/mapreduce/*, %HADOOP_MAPRED_HOME%/share/hadoop/mapreduce/lib/*. If mapreduce.app-submission.cross-platform is true, platform-agnostic default CLASSPATH for MR applications would be used: {{HADOOP_MAPRED_HOME}}/share/hadoop/mapreduce/*, {{HADOOP_MAPRED_HOME}}/share/hadoop/mapreduce/lib/* Parameter expansion marker will be replaced by NodeManager on container launch based on the underlying OS accordingly.
mapreduce.app-submission.cross-platform	false	If enabled, user can submit an application cross-platform i.e. submit an application from a Windows client to a Linux/Unix server or vice versa.
mapreduce.application.framework.path		Path to the MapReduce framework archive. If set, the framework archive will automatically be distributed along with the job, and this path would normally reside in a public location in an HDFS filesystem. As with distributed cache files, this can be a URL with a fragment specifying the alias to use for the archive name. For example, hdfs:/mapred/framework/hadoop-mapreduce-2.1.1.tar.gz#mrframework would alias the localized archive as "mrframework". Note that mapreduce.application.classpath must include the appropriate classpath for the specified framework. The base name of the archive, or alias of the archive if an alias is used, must appear in the specified classpath.
mapreduce.job.classloader	false	Whether to use a separate (isolated) classloader for user classes in the task JVM.
mapreduce.job.classloader.system.classes		Used to override the default definition of the system classes for the job classloader. The system classes are a comma-separated list of patterns that indicate whether to load a class from the system classpath, instead from the user-supplied JARs, when mapreduce.job.classloader is enabled. A positive pattern is defined as: 1. A single class name 'C' that matches 'C' and transitively all nested classes 'C$*' defined in C; 2. A package name ending with a '.' (e.g., "com.example.") that matches all classes from that package. A negative pattern is defined by a '-' in front of a positive pattern (e.g., "-com.example."). A class is considered a system class if and only if it matches one of the positive patterns and none of the negative ones. More formally: A class is a member of the inclusion set I if it matches one of the positive patterns. A class is a member of the exclusion set E if it matches one of the negative patterns. The set of system classes S = I \ E.
mapreduce.jobhistory.address	0.0.0.0:10020	MapReduce JobHistory Server IPC host:port
mapreduce.jobhistory.webapp.address	0.0.0.0:19888	MapReduce JobHistory Server Web UI host:port
mapreduce.jobhistory.keytab	/etc/security/keytab/jhs.service.keytab	Location of the kerberos keytab file for the MapReduce JobHistory Server.
mapreduce.jobhistory.principal	jhs/_HOST@REALM.TLD	Kerberos principal name for the MapReduce JobHistory Server.
mapreduce.jobhistory.intermediate-done-dir	${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate	
mapreduce.jobhistory.done-dir	${yarn.app.mapreduce.am.staging-dir}/history/done	
mapreduce.jobhistory.cleaner.enable	true	
mapreduce.jobhistory.cleaner.interval-ms	86400000	How often the job history cleaner checks for files to delete, in milliseconds. Defaults to 86400000 (one day). Files are only deleted if they are older than mapreduce.jobhistory.max-age-ms.
mapreduce.jobhistory.max-age-ms	604800000	Job history files older than this many milliseconds will be deleted when the history cleaner runs. Defaults to 604800000 (1 week).
mapreduce.jobhistory.client.thread-count	10	The number of threads to handle client API requests
mapreduce.jobhistory.datestring.cache.size	200000	Size of the date string cache. Effects the number of directories which will be scanned to find a job.
mapreduce.jobhistory.joblist.cache.size	20000	Size of the job list cache
mapreduce.jobhistory.loadedjobs.cache.size	5	Size of the loaded job cache
mapreduce.jobhistory.move.interval-ms	180000	Scan for history files to more from intermediate done dir to done dir at this frequency.
mapreduce.jobhistory.move.thread-count	3	The number of threads used to move files.
mapreduce.jobhistory.store.class		The HistoryStorage class to use to cache history data.
mapreduce.jobhistory.minicluster.fixed.ports	false	Whether to use fixed ports with the minicluster
mapreduce.jobhistory.admin.address	0.0.0.0:10033	The address of the History server admin interface.
mapreduce.jobhistory.admin.acl	*	ACL of who can be admin of the History server.
mapreduce.jobhistory.recovery.enable	false	Enable the history server to store server state and recover server state upon startup. If enabled then mapreduce.jobhistory.recovery.store.class must be specified.
mapreduce.jobhistory.recovery.store.class	org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService	The HistoryServerStateStoreService class to store history server state for recovery.
mapreduce.jobhistory.recovery.store.fs.uri	${hadoop.tmp.dir}/mapred/history/recoverystore	The URI where history server state will be stored if HistoryServerFileSystemStateStoreService is configured as the recovery storage class.
mapreduce.jobhistory.recovery.store.leveldb.path	${hadoop.tmp.dir}/mapred/history/recoverystore	The URI where history server state will be stored if HistoryServerLeveldbSystemStateStoreService is configured as the recovery storage class.
mapreduce.jobhistory.http.policy	HTTP_ONLY	This configures the HTTP endpoint for JobHistoryServer web UI. The following values are supported: - HTTP_ONLY : Service is provided only on http - HTTPS_ONLY : Service is provided only on https
yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size	10	The initial size of thread pool to launch containers in the app master. 
name	value	description
yarn.ipc.client.factory.class		Factory to create client IPC classes.
yarn.ipc.server.factory.class		Factory to create server IPC classes.
yarn.ipc.record.factory.class		Factory to create serializeable records.
yarn.ipc.rpc.class	org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC	RPC class implementation
yarn.resourcemanager.hostname	0.0.0.0	The hostname of the RM.
yarn.resourcemanager.address	${yarn.resourcemanager.hostname}:8032	The address of the applications manager interface in the RM.
yarn.resourcemanager.bind-host		The actual address the server will bind to. If this optional address is set, the RPC and webapp servers will bind to this address and the port specified in yarn.resourcemanager.address and yarn.resourcemanager.webapp.address, respectively. This is most useful for making RM listen to all interfaces by setting to 0.0.0.0.
yarn.resourcemanager.client.thread-count	50	The number of threads used to handle applications manager requests.
yarn.resourcemanager.amlauncher.thread-count	50	Number of threads used to launch/cleanup AM.
yarn.resourcemanager.nodemanager-connect-retries	10	Retry times to connect with NM.
yarn.dispatcher.drain-events.timeout	300000	Timeout in milliseconds when YARN dispatcher tries to drain the events. Typically, this happens when service is stopping. e.g. RM drains the ATS events dispatcher when stopping.
yarn.am.liveness-monitor.expiry-interval-ms	600000	The expiry interval for application master reporting.
yarn.resourcemanager.principal		The Kerberos principal for the resource manager.
yarn.resourcemanager.scheduler.address	${yarn.resourcemanager.hostname}:8030	The address of the scheduler interface.
yarn.resourcemanager.scheduler.client.thread-count	50	Number of threads to handle scheduler interface.
yarn.http.policy	HTTP_ONLY	This configures the HTTP endpoint for Yarn Daemons.The following values are supported: - HTTP_ONLY : Service is provided only on http - HTTPS_ONLY : Service is provided only on https
yarn.resourcemanager.webapp.address	${yarn.resourcemanager.hostname}:8088	The http address of the RM web application.
yarn.resourcemanager.webapp.https.address	${yarn.resourcemanager.hostname}:8090	The https adddress of the RM web application.
yarn.resourcemanager.resource-tracker.address	${yarn.resourcemanager.hostname}:8031	
yarn.acl.enable	false	Are acls enabled.
yarn.admin.acl	*	ACL of who can be admin of the YARN cluster.
yarn.resourcemanager.admin.address	${yarn.resourcemanager.hostname}:8033	The address of the RM admin interface.
yarn.resourcemanager.admin.client.thread-count	1	Number of threads used to handle RM admin interface.
yarn.resourcemanager.connect.max-wait.ms	900000	Maximum time to wait to establish connection to ResourceManager.
yarn.resourcemanager.connect.retry-interval.ms	30000	How often to try connecting to the ResourceManager.
yarn.resourcemanager.am.max-attempts	2	The maximum number of application attempts. It's a global setting for all application masters. Each application master can specify its individual maximum number of application attempts via the API, but the individual number cannot be more than the global upper bound. If it is, the resourcemanager will override it. The default number is set to 2, to allow at least one retry for AM.
yarn.resourcemanager.container.liveness-monitor.interval-ms	600000	How often to check that containers are still alive.
yarn.resourcemanager.keytab	/etc/krb5.keytab	The keytab for the resource manager.
yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled	true	Flag to enable override of the default kerberos authentication filter with the RM authentication filter to allow authentication using delegation tokens(fallback to kerberos if the tokens are missing). Only applicable when the http authentication type is kerberos.
yarn.resourcemanager.webapp.cross-origin.enabled	false	Flag to enable cross-origin (CORS) support in the RM. This flag requires the CORS filter initializer to be added to the filter initializers list in core-site.xml.
yarn.nm.liveness-monitor.expiry-interval-ms	600000	How long to wait until a node manager is considered dead.
yarn.resourcemanager.nodes.include-path		Path to file with nodes to include.
yarn.resourcemanager.nodes.exclude-path		Path to file with nodes to exclude.
yarn.resourcemanager.resource-tracker.client.thread-count	50	Number of threads to handle resource tracker calls.
yarn.resourcemanager.scheduler.class	org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler	The class to use as the resource scheduler.
yarn.scheduler.minimum-allocation-mb	1024	The minimum allocation for every container request at the RM, in MBs. Memory requests lower than this will throw a InvalidResourceRequestException.
yarn.scheduler.maximum-allocation-mb	8192	The maximum allocation for every container request at the RM, in MBs. Memory requests higher than this will throw a InvalidResourceRequestException.
yarn.scheduler.minimum-allocation-vcores	1	The minimum allocation for every container request at the RM, in terms of virtual CPU cores. Requests lower than this will throw a InvalidResourceRequestException.
yarn.scheduler.maximum-allocation-vcores	32	The maximum allocation for every container request at the RM, in terms of virtual CPU cores. Requests higher than this will throw a InvalidResourceRequestException.
yarn.resourcemanager.recovery.enabled	false	Enable RM to recover state after starting. If true, then yarn.resourcemanager.store.class must be specified.
yarn.resourcemanager.fail-fast	${yarn.fail-fast}	Should RM fail fast if it encounters any errors. By defalt, it points to ${yarn.fail-fast}. Errors include: 1) exceptions when state-store write/read operations fails.
yarn.fail-fast	false	Should YARN fail fast if it encounters any errors. This is a global config for all other components including RM,NM etc. If no value is set for component-specific config (e.g yarn.resourcemanager.fail-fast), this value will be the default.
yarn.resourcemanager.work-preserving-recovery.enabled	true	Enable RM work preserving recovery. This configuration is private to YARN for experimenting the feature.
yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms	10000	Set the amount of time RM waits before allocating new containers on work-preserving-recovery. Such wait period gives RM a chance to settle down resyncing with NMs in the cluster on recovery, before assigning new containers to applications.
yarn.resourcemanager.store.class	org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore	The class to use as the persistent store. If org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore is used, the store is implicitly fenced; meaning a single ResourceManager is able to use the store at any point in time. More details on this implicit fencing, along with setting up appropriate ACLs is discussed under yarn.resourcemanager.zk-state-store.root-node.acl.
yarn.resourcemanager.state-store.max-completed-applications	${yarn.resourcemanager.max-completed-applications}	The maximum number of completed applications RM state store keeps, less than or equals to ${yarn.resourcemanager.max-completed-applications}. By default, it equals to ${yarn.resourcemanager.max-completed-applications}. This ensures that the applications kept in the state store are consistent with the applications remembered in RM memory. Any values larger than ${yarn.resourcemanager.max-completed-applications} will be reset to ${yarn.resourcemanager.max-completed-applications}. Note that this value impacts the RM recovery performance.Typically, a smaller value indicates better performance on RM recovery.
yarn.resourcemanager.zk-address		Host:Port of the ZooKeeper server to be used by the RM. This must be supplied when using the ZooKeeper based implementation of the RM state store and/or embedded automatic failover in a HA setting.
yarn.resourcemanager.zk-num-retries	1000	Number of times RM tries to connect to ZooKeeper.
yarn.resourcemanager.zk-retry-interval-ms	1000	Retry interval in milliseconds when connecting to ZooKeeper. When HA is enabled, the value here is NOT used. It is generated automatically from yarn.resourcemanager.zk-timeout-ms and yarn.resourcemanager.zk-num-retries.
yarn.resourcemanager.zk-state-store.parent-path	/rmstore	Full path of the ZooKeeper znode where RM state will be stored. This must be supplied when using org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore as the value for yarn.resourcemanager.store.class
yarn.resourcemanager.zk-timeout-ms	10000	ZooKeeper session timeout in milliseconds. Session expiration is managed by the ZooKeeper cluster itself, not by the client. This value is used by the cluster to determine when the client's session expires. Expirations happens when the cluster does not hear from the client within the specified session timeout period (i.e. no heartbeat).
yarn.resourcemanager.zk-acl	world:anyone:rwcda	ACL's to be used for ZooKeeper znodes.
yarn.resourcemanager.zk-state-store.root-node.acl		ACLs to be used for the root znode when using ZKRMStateStore in a HA scenario for fencing. ZKRMStateStore supports implicit fencing to allow a single ResourceManager write-access to the store. For fencing, the ResourceManagers in the cluster share read-write-admin privileges on the root node, but the Active ResourceManager claims exclusive create-delete permissions. By default, when this property is not set, we use the ACLs from yarn.resourcemanager.zk-acl for shared admin access and rm-address:random-number for username-based exclusive create-delete access. This property allows users to set ACLs of their choice instead of using the default mechanism. For fencing to work, the ACLs should be carefully set differently on each ResourceManger such that all the ResourceManagers have shared admin access and the Active ResourceManger takes over (exclusively) the create-delete access.
yarn.resourcemanager.zk-auth		Specify the auths to be used for the ACL's specified in both the yarn.resourcemanager.zk-acl and yarn.resourcemanager.zk-state-store.root-node.acl properties. This takes a comma-separated list of authentication mechanisms, each of the form 'scheme:auth' (the same syntax used for the 'addAuth' command in the ZK CLI).
yarn.resourcemanager.fs.state-store.uri	${hadoop.tmp.dir}/yarn/system/rmstore	URI pointing to the location of the FileSystem path where RM state will be stored. This must be supplied when using org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore as the value for yarn.resourcemanager.store.class
yarn.resourcemanager.fs.state-store.retry-policy-spec	2000, 500	hdfs client retry policy specification. hdfs client retry is always enabled. Specified in pairs of sleep-time and number-of-retries and (t0, n0), (t1, n1), ..., the first n0 retries sleep t0 milliseconds on average, the following n1 retries sleep t1 milliseconds on average, and so on.
yarn.resourcemanager.fs.state-store.num-retries	0	the number of retries to recover from IOException in FileSystemRMStateStore.
yarn.resourcemanager.fs.state-store.retry-interval-ms	1000	Retry interval in milliseconds in FileSystemRMStateStore.
yarn.resourcemanager.leveldb-state-store.path	${hadoop.tmp.dir}/yarn/system/rmstore	Local path where the RM state will be stored when using org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore as the value for yarn.resourcemanager.store.class
yarn.resourcemanager.ha.enabled	false	Enable RM high-availability. When enabled, (1) The RM starts in the Standby mode by default, and transitions to the Active mode when prompted to. (2) The nodes in the RM ensemble are listed in yarn.resourcemanager.ha.rm-ids (3) The id of each RM either comes from yarn.resourcemanager.ha.id if yarn.resourcemanager.ha.id is explicitly specified or can be figured out by matching yarn.resourcemanager.address.{id} with local address (4) The actual physical addresses come from the configs of the pattern - {rpc-config}.{id}
yarn.resourcemanager.ha.automatic-failover.enabled	true	Enable automatic failover. By default, it is enabled only when HA is enabled
yarn.resourcemanager.ha.automatic-failover.embedded	true	Enable embedded automatic failover. By default, it is enabled only when HA is enabled. The embedded elector relies on the RM state store to handle fencing, and is primarily intended to be used in conjunction with ZKRMStateStore.
yarn.resourcemanager.ha.automatic-failover.zk-base-path	/yarn-leader-election	The base znode path to use for storing leader information, when using ZooKeeper based leader election.
yarn.resourcemanager.cluster-id		Name of the cluster. In a HA setting, this is used to ensure the RM participates in leader election for this cluster and ensures it does not affect other clusters
yarn.resourcemanager.ha.rm-ids		The list of RM nodes in the cluster when HA is enabled. See description of yarn.resourcemanager.ha .enabled for full details on how this is used.
yarn.resourcemanager.ha.id		The id (string) of the current RM. When HA is enabled, this is an optional config. The id of current RM can be set by explicitly specifying yarn.resourcemanager.ha.id or figured out by matching yarn.resourcemanager.address.{id} with local address See description of yarn.resourcemanager.ha.enabled for full details on how this is used.
yarn.client.failover-proxy-provider	org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider	When HA is enabled, the class to be used by Clients, AMs and NMs to failover to the Active RM. It should extend org.apache.hadoop.yarn.client.RMFailoverProxyProvider
yarn.client.failover-max-attempts		When HA is enabled, the max number of times FailoverProxyProvider should attempt failover. When set, this overrides the yarn.resourcemanager.connect.max-wait.ms. When not set, this is inferred from yarn.resourcemanager.connect.max-wait.ms.
yarn.client.failover-sleep-base-ms		When HA is enabled, the sleep base (in milliseconds) to be used for calculating the exponential delay between failovers. When set, this overrides the yarn.resourcemanager.connect.* settings. When not set, yarn.resourcemanager.connect.retry-interval.ms is used instead.
yarn.client.failover-sleep-max-ms		When HA is enabled, the maximum sleep time (in milliseconds) between failovers. When set, this overrides the yarn.resourcemanager.connect.* settings. When not set, yarn.resourcemanager.connect.retry-interval.ms is used instead.
yarn.client.failover-retries	0	When HA is enabled, the number of retries per attempt to connect to a ResourceManager. In other words, it is the ipc.client.connect.max.retries to be used during failover attempts
yarn.client.failover-retries-on-socket-timeouts	0	When HA is enabled, the number of retries per attempt to connect to a ResourceManager on socket timeouts. In other words, it is the ipc.client.connect.max.retries.on.timeouts to be used during failover attempts
yarn.resourcemanager.max-completed-applications	10000	The maximum number of completed applications RM keeps.
yarn.resourcemanager.delayed.delegation-token.removal-interval-ms	30000	Interval at which the delayed token removal thread runs
yarn.resourcemanager.proxy-user-privileges.enabled	false	If true, ResourceManager will have proxy-user privileges. Use case: In a secure cluster, YARN requires the user hdfs delegation-tokens to do localization and log-aggregation on behalf of the user. If this is set to true, ResourceManager is able to request new hdfs delegation tokens on behalf of the user. This is needed by long-running-service, because the hdfs tokens will eventually expire and YARN requires new valid tokens to do localization and log-aggregation. Note that to enable this use case, the corresponding HDFS NameNode has to configure ResourceManager as the proxy-user so that ResourceManager can itself ask for new tokens on behalf of the user when tokens are past their max-life-time.
yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs	86400	Interval for the roll over for the master key used to generate application tokens
yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs	86400	Interval for the roll over for the master key used to generate container tokens. It is expected to be much greater than yarn.nm.liveness-monitor.expiry-interval-ms and yarn.resourcemanager.rm.container-allocation.expiry-interval-ms. Otherwise the behavior is undefined.
yarn.resourcemanager.nodemanagers.heartbeat-interval-ms	1000	The heart-beat interval in milliseconds for every NodeManager in the cluster.
yarn.resourcemanager.nodemanager.minimum.version	NONE	The minimum allowed version of a connecting nodemanager. The valid values are NONE (no version checking), EqualToRM (the nodemanager's version is equal to or greater than the RM version), or a Version String.
yarn.resourcemanager.scheduler.monitor.enable	false	Enable a set of periodic monitors (specified in yarn.resourcemanager.scheduler.monitor.policies) that affect the scheduler.
yarn.resourcemanager.scheduler.monitor.policies	org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy	The list of SchedulingEditPolicy classes that interact with the scheduler. A particular module may be incompatible with the scheduler, other policies, or a configuration of either.
yarn.resourcemanager.configuration.provider-class	org.apache.hadoop.yarn.LocalConfigurationProvider	The class to use as the configuration provider. If org.apache.hadoop.yarn.LocalConfigurationProvider is used, the local configuration will be loaded. If org.apache.hadoop.yarn.FileSystemBasedConfigurationProvider is used, the configuration which will be loaded should be uploaded to remote File system first.
yarn.resourcemanager.system-metrics-publisher.enabled	false	The setting that controls whether yarn system metrics is published on the timeline server or not by RM.
yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size	10	Number of worker threads that send the yarn system metrics data.
yarn.nodemanager.hostname	0.0.0.0	The hostname of the NM.
yarn.nodemanager.address	${yarn.nodemanager.hostname}:0	The address of the container manager in the NM.
yarn.nodemanager.bind-host		The actual address the server will bind to. If this optional address is set, the RPC and webapp servers will bind to this address and the port specified in yarn.nodemanager.address and yarn.nodemanager.webapp.address, respectively. This is most useful for making NM listen to all interfaces by setting to 0.0.0.0.
yarn.nodemanager.admin-env	MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX	Environment variables that should be forwarded from the NodeManager's environment to the container's.
yarn.nodemanager.env-whitelist	JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,HADOOP_YARN_HOME	Environment variables that containers may override rather than use NodeManager's default.
yarn.nodemanager.container-executor.class	org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor	who will execute(launch) the containers.
yarn.nodemanager.container-manager.thread-count	20	Number of threads container manager uses.
yarn.nodemanager.delete.thread-count	4	Number of threads used in cleanup.
yarn.nodemanager.delete.debug-delay-sec	0	Number of seconds after an application finishes before the nodemanager's DeletionService will delete the application's localized file directory and log directory. To diagnose Yarn application problems, set this property's value large enough (for example, to 600 = 10 minutes) to permit examination of these directories. After changing the property's value, you must restart the nodemanager in order for it to have an effect. The roots of Yarn applications' work directories is configurable with the yarn.nodemanager.local-dirs property (see below), and the roots of the Yarn applications' log directories is configurable with the yarn.nodemanager.log-dirs property (see also below).
yarn.nodemanager.keytab	/etc/krb5.keytab	Keytab for NM.
yarn.nodemanager.local-dirs	${hadoop.tmp.dir}/nm-local-dir	List of directories to store localized files in. An application's localized file directory will be found in: ${yarn.nodemanager.local-dirs}/usercache/${user}/appcache/application_${appid}. Individual containers' work directories, called container_${contid}, will be subdirectories of this.
yarn.nodemanager.local-cache.max-files-per-directory	8192	It limits the maximum number of files which will be localized in a single local directory. If the limit is reached then sub-directories will be created and new files will be localized in them. If it is set to a value less than or equal to 36 [which are sub-directories (0-9 and then a-z)] then NodeManager will fail to start. For example; [for public cache] if this is configured with a value of 40 ( 4 files + 36 sub-directories) and the local-dir is "/tmp/local-dir1" then it will allow 4 files to be created directly inside "/tmp/local-dir1/filecache". For files that are localized further it will create a sub-directory "0" inside "/tmp/local-dir1/filecache" and will localize files inside it until it becomes full. If a file is removed from a sub-directory that is marked full, then that sub-directory will be used back again to localize files.
yarn.nodemanager.localizer.address	${yarn.nodemanager.hostname}:8040	Address where the localizer IPC is.
yarn.nodemanager.localizer.cache.cleanup.interval-ms	600000	Interval in between cache cleanups.
yarn.nodemanager.localizer.cache.target-size-mb	10240	Target size of localizer cache in MB, per nodemanager. It is a target retention size that only includes resources with PUBLIC and PRIVATE visibility and excludes resources with APPLICATION visibility
yarn.nodemanager.localizer.client.thread-count	5	Number of threads to handle localization requests.
yarn.nodemanager.localizer.fetch.thread-count	4	Number of threads to use for localization fetching.
yarn.nodemanager.log-dirs	${yarn.log.dir}/userlogs	Where to store container logs. An application's localized log directory will be found in ${yarn.nodemanager.log-dirs}/application_${appid}. Individual containers' log directories will be below this, in directories named container_{$contid}. Each container directory will contain the files stderr, stdin, and syslog generated by that container.
yarn.log-aggregation-enable	false	Whether to enable log aggregation. Log aggregation collects each container's logs and moves these logs onto a file-system, for e.g. HDFS, after the application completes. Users can configure the "yarn.nodemanager.remote-app-log-dir" and "yarn.nodemanager.remote-app-log-dir-suffix" properties to determine where these logs are moved to. Users can access the logs via the Application Timeline Server.
yarn.log-aggregation.retain-seconds	-1	How long to keep aggregation logs before deleting them. -1 disables. Be careful set this too small and you will spam the name node.
yarn.log-aggregation.retain-check-interval-seconds	-1	How long to wait between aggregated log retention checks. If set to 0 or a negative value then the value is computed as one-tenth of the aggregated log retention time. Be careful set this too small and you will spam the name node.
yarn.nodemanager.log.retain-seconds	10800	Time in seconds to retain user logs. Only applicable if log aggregation is disabled
yarn.nodemanager.remote-app-log-dir	/tmp/logs	Where to aggregate logs to.
yarn.nodemanager.remote-app-log-dir-suffix	logs	The remote log dir will be created at {yarn.nodemanager.remote-app-log-dir}/${user}/{thisParam}
yarn.nodemanager.resource.memory-mb	8192	Amount of physical memory, in MB, that can be allocated for containers.
yarn.nodemanager.pmem-check-enabled	true	Whether physical memory limits will be enforced for containers.
yarn.nodemanager.vmem-check-enabled	true	Whether virtual memory limits will be enforced for containers.
yarn.nodemanager.vmem-pmem-ratio	2.1	Ratio between virtual memory to physical memory when setting memory limits for containers. Container allocations are expressed in terms of physical memory, and virtual memory usage is allowed to exceed this allocation by this ratio.
yarn.nodemanager.resource.cpu-vcores	8	Number of vcores that can be allocated for containers. This is used by the RM scheduler when allocating resources for containers. This is not used to limit the number of physical cores used by YARN containers.
yarn.nodemanager.resource.percentage-physical-cpu-limit	100	Percentage of CPU that can be allocated for containers. This setting allows users to limit the amount of CPU that YARN containers use. Currently functional only on Linux using cgroups. The default is to use 100% of CPU.
yarn.nodemanager.webapp.address	${yarn.nodemanager.hostname}:8042	NM Webapp address.
yarn.nodemanager.container-monitor.interval-ms	3000	How often to monitor containers.
yarn.nodemanager.container-monitor.resource-calculator.class		Class that calculates containers current resource utilization.
yarn.nodemanager.health-checker.interval-ms	600000	Frequency of running node health script.
yarn.nodemanager.health-checker.script.timeout-ms	1200000	Script time out period.
yarn.nodemanager.health-checker.script.path		The health check script to run.
yarn.nodemanager.health-checker.script.opts		The arguments to pass to the health check script.
yarn.nodemanager.disk-health-checker.interval-ms	120000	Frequency of running disk health checker code.
yarn.nodemanager.disk-health-checker.min-healthy-disks	0.25	The minimum fraction of number of disks to be healthy for the nodemanager to launch new containers. This correspond to both yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs. i.e. If there are less number of healthy local-dirs (or log-dirs) available, then new containers will not be launched on this node.
yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage	90.0	The maximum percentage of disk space utilization allowed after which a disk is marked as bad. Values can range from 0.0 to 100.0. If the value is greater than or equal to 100, the nodemanager will check for full disk. This applies to yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs.
yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb	0	The minimum space that must be available on a disk for it to be used. This applies to yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs.
yarn.nodemanager.linux-container-executor.path		The path to the Linux container executor.
yarn.nodemanager.linux-container-executor.resources-handler.class	org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler	The class which should help the LCE handle resources.
yarn.nodemanager.linux-container-executor.cgroups.hierarchy	/hadoop-yarn	The cgroups hierarchy under which to place YARN proccesses (cannot contain commas). If yarn.nodemanager.linux-container-executor.cgroups.mount is false (that is, if cgroups have been pre-configured), then this cgroups hierarchy must already exist and be writable by the NodeManager user, otherwise the NodeManager may fail. Only used when the LCE resources handler is set to the CgroupsLCEResourcesHandler.
yarn.nodemanager.linux-container-executor.cgroups.mount	false	Whether the LCE should attempt to mount cgroups if not found. Only used when the LCE resources handler is set to the CgroupsLCEResourcesHandler.
yarn.nodemanager.linux-container-executor.cgroups.mount-path		Where the LCE should attempt to mount cgroups if not found. Common locations include /sys/fs/cgroup and /cgroup; the default location can vary depending on the Linux distribution in use. This path must exist before the NodeManager is launched. Only used when the LCE resources handler is set to the CgroupsLCEResourcesHandler, and yarn.nodemanager.linux-container-executor.cgroups.mount is true.
yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users	true	This determines which of the two modes that LCE should use on a non-secure cluster. If this value is set to true, then all containers will be launched as the user specified in yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user. If this value is set to false, then containers will run as the user who submitted the application.
yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user	nobody	The UNIX user that containers will run as when Linux-container-executor is used in nonsecure mode (a use case for this is using cgroups) if the yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users is set to true.
yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern	^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[$]?$	The allowed pattern for UNIX user names enforced by Linux-container-executor when used in nonsecure mode (use case for this is using cgroups). The default value is taken from /usr/sbin/adduser
yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage	false	This flag determines whether apps should run with strict resource limits or be allowed to consume spare resources if they need them. For example, turning the flag on will restrict apps to use only their share of CPU, even if the node has spare CPU cycles. The default value is false i.e. use available resources. Please note that turning this flag on may reduce job throughput on the cluster.
yarn.nodemanager.windows-container.memory-limit.enabled	false	This flag determines whether memory limit will be set for the Windows Job Object of the containers launched by the default container executor.
yarn.nodemanager.windows-container.cpu-limit.enabled	false	This flag determines whether CPU limit will be set for the Windows Job Object of the containers launched by the default container executor.
yarn.nodemanager.log-aggregation.compression-type	none	T-file compression types used to compress aggregated logs.
yarn.nodemanager.principal		The kerberos principal for the node manager.
yarn.nodemanager.aux-services		A comma separated list of services where service name should only contain a-zA-Z0-9_ and can not start with numbers
yarn.nodemanager.sleep-delay-before-sigkill.ms	250	No. of ms to wait between sending a SIGTERM and SIGKILL to a container
yarn.nodemanager.process-kill-wait.ms	2000	Max time to wait for a process to come up when trying to cleanup a container
yarn.nodemanager.resourcemanager.minimum.version	NONE	The minimum allowed version of a resourcemanager that a nodemanager will connect to. The valid values are NONE (no version checking), EqualToNM (the resourcemanager's version is equal to or greater than the NM version), or a Version String.
yarn.client.nodemanager-client-async.thread-pool-max-size	500	Max number of threads in NMClientAsync to process container management events
yarn.client.nodemanager-connect.max-wait-ms	180000	Max time to wait to establish a connection to NM
yarn.client.nodemanager-connect.retry-interval-ms	10000	Time interval between each attempt to connect to NM
yarn.client.max-cached-nodemanagers-proxies	0	Maximum number of proxy connections to cache for node managers. If set to a value greater than zero then the cache is enabled and the NMClient and MRAppMaster will cache the specified number of node manager proxies. There will be at max one proxy per node manager. Ex. configuring it to a value of 5 will make sure that client will at max have 5 proxies cached with 5 different node managers. These connections for these proxies will be timed out if idle for more than the system wide idle timeout period. Note that this could cause issues on large clusters as many connections could linger simultaneously and lead to a large number of connection threads. The token used for authentication will be used only at connection creation time. If a new token is received then the earlier connection should be closed in order to use the new token. This and (yarn.client.nodemanager-client-async.thread-pool-max-size) are related and should be in sync (no need for them to be equal). If the value of this property is zero then the connection cache is disabled and connections will use a zero idle timeout to prevent too many connection threads on large clusters.
yarn.nodemanager.recovery.enabled	false	Enable the node manager to recover after starting
yarn.nodemanager.recovery.dir	${hadoop.tmp.dir}/yarn-nm-recovery	The local filesystem directory in which the node manager will store state when recovery is enabled.
yarn.nodemanager.container-metrics.unregister-delay-ms	10000	The delay time ms to unregister container metrics after completion.
yarn.nodemanager.docker-container-executor.exec-name	/usr/bin/docker	Name or path to the Docker client.
yarn.nodemanager.aux-services.mapreduce_shuffle.class	org.apache.hadoop.mapred.ShuffleHandler	
mapreduce.job.jar		
mapreduce.job.hdfs-servers	${fs.defaultFS}	
yarn.web-proxy.principal		The kerberos principal for the proxy, if the proxy is not running as part of the RM.
yarn.web-proxy.keytab		Keytab for WebAppProxy, if the proxy is not running as part of the RM.
yarn.web-proxy.address		The address for the web proxy as HOST:PORT, if this is not given then the proxy will run as part of the RM
yarn.application.classpath		CLASSPATH for YARN applications. A comma-separated list of CLASSPATH entries. When this value is empty, the following default CLASSPATH for YARN applications would be used. For Linux: $HADOOP_CONF_DIR, $HADOOP_COMMON_HOME/share/hadoop/common/*, $HADOOP_COMMON_HOME/share/hadoop/common/lib/*, $HADOOP_HDFS_HOME/share/hadoop/hdfs/*, $HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*, $HADOOP_YARN_HOME/share/hadoop/yarn/*, $HADOOP_YARN_HOME/share/hadoop/yarn/lib/* For Windows: %HADOOP_CONF_DIR%, %HADOOP_COMMON_HOME%/share/hadoop/common/*, %HADOOP_COMMON_HOME%/share/hadoop/common/lib/*, %HADOOP_HDFS_HOME%/share/hadoop/hdfs/*, %HADOOP_HDFS_HOME%/share/hadoop/hdfs/lib/*, %HADOOP_YARN_HOME%/share/hadoop/yarn/*, %HADOOP_YARN_HOME%/share/hadoop/yarn/lib/*
yarn.timeline-service.enabled	false	Indicate to clients whether timeline service is enabled or not. If enabled, clients will put entities and events to the timeline server.
yarn.timeline-service.hostname	0.0.0.0	The hostname of the timeline service web application.
yarn.timeline-service.address	${yarn.timeline-service.hostname}:10200	This is default address for the timeline server to start the RPC server.
yarn.timeline-service.webapp.address	${yarn.timeline-service.hostname}:8188	The http address of the timeline service web application.
yarn.timeline-service.webapp.https.address	${yarn.timeline-service.hostname}:8190	The https address of the timeline service web application.
yarn.timeline-service.bind-host		The actual address the server will bind to. If this optional address is set, the RPC and webapp servers will bind to this address and the port specified in yarn.timeline-service.address and yarn.timeline-service.webapp.address, respectively. This is most useful for making the service listen to all interfaces by setting to 0.0.0.0.
yarn.timeline-service.generic-application-history.max-applications	10000	Defines the max number of applications could be fetched using REST API or application history protocol and shown in timeline server web ui.
yarn.timeline-service.store-class	org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore	Store class name for timeline store.
yarn.timeline-service.ttl-enable	true	Enable age off of timeline store data.
yarn.timeline-service.ttl-ms	604800000	Time to live for timeline store data in milliseconds.
yarn.timeline-service.leveldb-timeline-store.path	${hadoop.tmp.dir}/yarn/timeline	Store file name for leveldb timeline store.
yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms	300000	Length of time to wait between deletion cycles of leveldb timeline store in milliseconds.
yarn.timeline-service.leveldb-timeline-store.read-cache-size	104857600	Size of read cache for uncompressed blocks for leveldb timeline store in bytes.
yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size	10000	Size of cache for recently read entity start times for leveldb timeline store in number of entities.
yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size	10000	Size of cache for recently written entity start times for leveldb timeline store in number of entities.
yarn.timeline-service.handler-thread-count	10	Handler thread count to serve the client RPC requests.
yarn.timeline-service.http-authentication.type	simple	Defines authentication used for the timeline server HTTP endpoint. Supported values are: simple | kerberos | #AUTHENTICATION_HANDLER_CLASSNAME#
yarn.timeline-service.http-authentication.simple.anonymous.allowed	true	Indicates if anonymous requests are allowed by the timeline server when using 'simple' authentication.
yarn.timeline-service.principal		The Kerberos principal for the timeline server.
yarn.timeline-service.keytab	/etc/krb5.keytab	The Kerberos keytab for the timeline server.
yarn.timeline-service.ui-names		Comma separated list of UIs that will be hosted
yarn.timeline-service.client.max-retries	30	Default maximum number of retires for timeline servive client and value -1 means no limit.
yarn.timeline-service.client.best-effort	false	Client policy for whether timeline operations are non-fatal
yarn.timeline-service.client.retry-interval-ms	1000	Default retry time interval for timeline servive client.
yarn.timeline-service.recovery.enabled	false	Enable timeline server to recover state after starting. If true, then yarn.timeline-service.state-store-class must be specified.
yarn.timeline-service.state-store-class	org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore	Store class name for timeline state store.
yarn.timeline-service.leveldb-state-store.path	${hadoop.tmp.dir}/yarn/timeline	Store file name for leveldb state store.
yarn.sharedcache.enabled	false	Whether the shared cache is enabled
yarn.sharedcache.root-dir	/sharedcache	The root directory for the shared cache
yarn.sharedcache.nested-level	3	The level of nested directories before getting to the checksum directories. It must be non-negative.
yarn.sharedcache.store.class	org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore	The implementation to be used for the SCM store
yarn.sharedcache.app-checker.class	org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker	The implementation to be used for the SCM app-checker
yarn.sharedcache.store.in-memory.staleness-period-mins	10080	A resource in the in-memory store is considered stale if the time since the last reference exceeds the staleness period. This value is specified in minutes.
yarn.sharedcache.store.in-memory.initial-delay-mins	10	Initial delay before the in-memory store runs its first check to remove dead initial applications. Specified in minutes.
yarn.sharedcache.store.in-memory.check-period-mins	720	The frequency at which the in-memory store checks to remove dead initial applications. Specified in minutes.
yarn.sharedcache.admin.address	0.0.0.0:8047	The address of the admin interface in the SCM (shared cache manager)
yarn.sharedcache.admin.thread-count	1	The number of threads used to handle SCM admin interface (1 by default)
yarn.sharedcache.webapp.address	0.0.0.0:8788	The address of the web application in the SCM (shared cache manager)
yarn.sharedcache.cleaner.period-mins	1440	The frequency at which a cleaner task runs. Specified in minutes.
yarn.sharedcache.cleaner.initial-delay-mins	10	Initial delay before the first cleaner task is scheduled. Specified in minutes.
yarn.sharedcache.cleaner.resource-sleep-ms	0	The time to sleep between processing each shared cache resource. Specified in milliseconds.
yarn.sharedcache.uploader.server.address	0.0.0.0:8046	The address of the node manager interface in the SCM (shared cache manager)
yarn.sharedcache.uploader.server.thread-count	50	The number of threads used to handle shared cache manager requests from the node manager (50 by default)
yarn.sharedcache.client-server.address	0.0.0.0:8045	The address of the client interface in the SCM (shared cache manager)
yarn.sharedcache.client-server.thread-count	50	The number of threads used to handle shared cache manager requests from clients (50 by default)
yarn.sharedcache.checksum.algo.impl	org.apache.hadoop.yarn.sharedcache.ChecksumSHA256Impl	The algorithm used to compute checksums of files (SHA-256 by default)
yarn.sharedcache.nm.uploader.replication.factor	10	The replication factor for the node manager uploader for the shared cache (10 by default)
yarn.sharedcache.nm.uploader.thread-count	20	The number of threads used to upload files from a node manager instance (20 by default)
yarn.client.application-client-protocol.poll-interval-ms	200	The interval that the yarn client library uses to poll the completion status of the asynchronous API of application client protocol.
yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled	false	RSS usage of a process computed via /proc/pid/stat is not very accurate as it includes shared pages of a process. /proc/pid/smaps provides useful information like Private_Dirty, Private_Clean, Shared_Dirty, Shared_Clean which can be used for computing more accurate RSS. When this flag is enabled, RSS is computed as Min(Shared_Dirty, Pss) + Private_Clean + Private_Dirty. It excludes read-only shared mappings in RSS computation.
yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds	-1	Defines how often NMs wake up to upload log files. The default value is -1. By default, the logs will be uploaded when the application is finished. By setting this configure, logs can be uploaded periodically when the application is running. The minimum rolling-interval-seconds can be set is 3600.
yarn.nodemanager.webapp.cross-origin.enabled	false	Flag to enable cross-origin (CORS) support in the NM. This flag requires the CORS filter initializer to be added to the filter initializers list in core-site.xml.
